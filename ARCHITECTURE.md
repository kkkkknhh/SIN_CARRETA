# System Architecture Documentation
*Generated: 2025-10-05 11:01:35*

## Overview

This document describes the architecture of the MINIMINIMOON system, including 66 dependency flows, 5 critical paths, and detailed component interactions.

> **üìù Note:** For information on the canonical P-D-Q notation system used throughout the codebase, see [`docs/PDQ_CANONICAL_NOTATION.md`](docs/PDQ_CANONICAL_NOTATION.md).

## System Components

### Core Components

- **causal_pattern_detector**
  - Incoming dependencies: 4
  - Outgoing dependencies: 0
  - Type: Critical

- **cli**
  - Incoming dependencies: 0
  - Outgoing dependencies: 4
  - Type: Critical

- **contradiction_detector**
  - Incoming dependencies: 2
  - Outgoing dependencies: 0
  - Type: Critical

- **dag_validation**
  - Incoming dependencies: 4
  - Outgoing dependencies: 2
  - Type: Critical

- **data_flow_contract**
  - Incoming dependencies: 3
  - Outgoing dependencies: 0
  - Type: Critical

- **decalogo_pipeline_orchestrator**
  - Incoming dependencies: 0
  - Outgoing dependencies: 7
  - Type: Critical

- **document_segmenter**
  - Incoming dependencies: 2
  - Outgoing dependencies: 0
  - Type: Critical

- **embedding_model**
  - Incoming dependencies: 4
  - Outgoing dependencies: 0
  - Type: Critical

- **evidence_registry**
  - Incoming dependencies: 3
  - Outgoing dependencies: 0
  - Type: Critical

- **example_usage**
  - Incoming dependencies: 0
  - Outgoing dependencies: 1
  - Type: Critical

- **feasibility_scorer**
  - Incoming dependencies: 6
  - Outgoing dependencies: 0
  - Type: Critical

- **integrated_evaluation_system**
  - Incoming dependencies: 0
  - Outgoing dependencies: 2
  - Type: Critical

- **integration_example**
  - Incoming dependencies: 0
  - Outgoing dependencies: 1
  - Type: Critical

- **json_utils**
  - Incoming dependencies: 1
  - Outgoing dependencies: 0
  - Type: Critical

- **log_config**
  - Incoming dependencies: 5
  - Outgoing dependencies: 0
  - Type: Critical

- **miniminimoon_immutability**
  - Incoming dependencies: 1
  - Outgoing dependencies: 0
  - Type: Critical

- **miniminimoon_orchestrator**
  - Incoming dependencies: 2
  - Outgoing dependencies: 16
  - Type: Critical

- **monetary_detector**
  - Incoming dependencies: 4
  - Outgoing dependencies: 0
  - Type: Critical

- **plan_processor**
  - Incoming dependencies: 2
  - Outgoing dependencies: 1
  - Type: Critical

- **plan_sanitizer**
  - Incoming dependencies: 2
  - Outgoing dependencies: 1
  - Type: Critical

- **questionnaire_engine**
  - Incoming dependencies: 4
  - Outgoing dependencies: 0
  - Type: Critical

- **responsibility_detector**
  - Incoming dependencies: 3
  - Outgoing dependencies: 0
  - Type: Critical

- **spacy_loader**
  - Incoming dependencies: 1
  - Outgoing dependencies: 0
  - Type: Critical

- **system_validators**
  - Incoming dependencies: 1
  - Outgoing dependencies: 0
  - Type: Critical

- **teoria_cambio**
  - Incoming dependencies: 4
  - Outgoing dependencies: 1
  - Type: Critical

- **unified_evaluation_pipeline**
  - Incoming dependencies: 0
  - Outgoing dependencies: 4
  - Type: Critical

- **verify_coverage_metric**
  - Incoming dependencies: 0
  - Outgoing dependencies: 10
  - Type: Critical

- **verify_reproducibility**
  - Incoming dependencies: 0
  - Outgoing dependencies: 1
  - Type: Critical


## Dependency Statistics

- Total dependency flows: 66
- Critical flows: 39
- Unique modules: 45
- Critical paths identified: 5
- Canonical flow order: defined in `tools/flow_doc.json`

## Flow Types Distribution

- Configuration: 5 flows
- Control: 2 flows
- Data: 58 flows
- Utility: 1 flows

---

## Artifacts Directory

The `artifacts/` directory serves as the **canonical location for all pipeline outputs** that provide evidence for system correctness, reproducibility, and auditability. All validation tools, CI/CD gates, and auditing processes consume these artifacts to verify system behavior.

### Directory Purpose

- **Canonical Output Location**: Single source of truth for pipeline execution results
- **Evidence for Correctness**: Cryptographically verifiable proof of deterministic execution
- **Reproducibility Guarantee**: Complete execution trace enabling bit-for-bit reproduction
- **Audit Trail**: Immutable record of all decisions, evidence, and evaluations
- **Validation Input**: Primary data source for pre/post execution gates and compliance checks

### Artifact Specifications

#### 1. flow_runtime.json

**Purpose**: Execution trace capturing the exact order and timing of all pipeline stages for deterministic flow validation (Gate #2).

**Generated By**: `miniminimoon_orchestrator.py` `process_plan_deterministic()` method via `_generate_flow_runtime_metadata()`, exported during `export_artifacts()` call

**Consumed By**: 
- `system_validators.py` (post-execution validation)
- `CanonicalFlowValidator` (canonical flow order validation, Gate #2)
- CI/CD audit gates
- `results_bundle.json` (bundled for audit trail)

**JSON Schema**:
```json
{
  "evidence_hash": "sha256_hex_64_chars",
  "duration_seconds": 45.2,
  "end_time": "ISO-8601",
  "errors": {},
  "flow_hash": "sha256_hex_64_chars",
  "orchestrator_version": "2.1.0",
  "plan_path": "path/to/plan.txt",
  "stage_count": 15,
  "stage_timestamps": {
    "sanitization": 1696680000.123,
    "plan_processing": 1696680001.456,
    ...
  },
  "stages": [
    "sanitization",
    "plan_processing",
    "document_segmentation",
    "embedding",
    "responsibility_detection",
    "contradiction_detection",
    "monetary_detection",
    "feasibility_scoring",
    "causal_detection",
    "teoria_cambio",
    "dag_validation",
    "evidence_registry_build",
    "decalogo_evaluation",
    "questionnaire_evaluation",
    "answers_assembly"
  ],
  "start_time": "ISO-8601",
  "validation": {
    "flow_valid": true,
    "expected_stages": [...],
    "actual_stages": [...],
    "missing_stages": [],
    "extra_stages": [],
    "flow_hash": "sha256",
    "doc_hash": "sha256",
    "hashes_match": true
  }
}
```

**Validation Requirements**:
- **Schema Constraints**:
  - `flow_hash` must be SHA-256 hex digest (64 chars)
  - `evidence_hash` must be SHA-256 hex digest (64 chars) matching `evidence_registry.json`
  - `stages` must contain exactly 15 canonical stages
  - `stage_count` must equal length of `stages` (15)
  - `stage_timestamps` keys must match `stages` exactly
  - `duration_seconds` must be non-negative float
  - `start_time` and `end_time` must be ISO-8601 format
  - `orchestrator_version` must match semantic version format (e.g., "2.1.0")
- **Completeness Checks**:
  - All 15 canonical stages must be present in `stages` array
  - No duplicate stages in execution order
  - Timestamps must be monotonically increasing
  - `errors` must be empty dict for successful executions
- **Relationships**:
  - `stages` must exactly match canonical order defined in `PipelineStage` enum
  - `flow_hash` must match SHA-256 hash computed from concatenated stage names
  - `evidence_hash` must reference hash from `evidence_registry.json`
  - `validation.flow_valid` must be true for Gate #2 to pass
  - Referenced in `results_bundle.json` `pipeline_results` section
  - Exported to `artifacts/flow_runtime.json` after all stages complete

#### 2. evidence_registry.json

**Purpose**: Cryptographically-hashed registry of all evidence extracted during pipeline execution, providing traceability and non-repudiation (Gate #3).

**Generated By**: `miniminimoon_orchestrator.py` `process_plan_deterministic()` method via `_build_evidence_registry()` during REGISTRY_BUILD stage, exported via `EvidenceRegistry.export()` call in `export_artifacts()`

**Consumed By**:
- `system_validators.py` (evidence presence validation, hash verification)
- `questionnaire_engine.py` (evidence consumption for question evaluation)
- All detector modules (responsibility, monetary, causal, etc.) register evidence entries
- `answers_report.json` (evidence_ids linkage for provenance)
- Audit and compliance tools

**JSON Schema**:
```json
{
  "evidence_count": 25,
  "deterministic_hash": "sha256_hex_64_chars",
  "evidence": {
    "resp_a3f9cd012e": {
      "evidence_id": "resp_a3f9cd012e",
      "stage": "responsibility_detection",
      "content": {
        "entity": "Ministerio de Educaci√≥n",
        "type": "ORG",
        "text": "El Ministerio de Educaci√≥n implementar√°..."
      },
      "source_segment_ids": [],
      "confidence": 0.85,
      "timestamp": "2024-10-07T12:34:56.789012",
      "metadata": {}
    },
    "money_f2c1ab345d": {
      "evidence_id": "money_f2c1ab345d",
      "stage": "monetary_detection",
      "content": {
        "amount": 1000000,
        "currency": "MXN",
        "context": "presupuesto anual"
      },
      "source_segment_ids": [],
      "confidence": 0.90,
      "timestamp": "2024-10-07T12:34:57.123456",
      "metadata": {}
    },
    "answer_DE-1-Q1": {
      "evidence_id": "answer_DE-1-Q1",
      "stage": "answers_assembly",
      "content": {
        "question_id": "DE-1-Q1",
        "dimension": "DE-1",
        "raw_score": 0.72,
        "rationale": "..."
      },
      "source_segment_ids": ["resp_a3f9cd012e"],
      "confidence": 0.85,
      "timestamp": "2024-10-07T12:35:00.456789",
      "metadata": {
        "question_id": "DE-1-Q1",
        "dimension": "DE-1",
        "score": 0.72,
        "evidence_count": 1,
        "question_unique_id": "DE-1-Q1",
        "source_evidence_ids": ["resp_a3f9cd012e"],
        "rubric_weight": 0.00333,
        "provenance": {
          "stage": "answers_assembly",
          "linked_evidence_ids": ["resp_a3f9cd012e"],
          "question_engine_score": 0.72,
          "final_confidence": 0.85
        }
      }
    }
  }
}
```

**Validation Requirements**:
- **Schema Constraints**:
  - `evidence_count` must equal number of keys in `evidence` object
  - `deterministic_hash` must be SHA-256 hex digest (64 chars)
  - Each `evidence_id` must follow format `{stage_prefix}_{hash_10_chars}` or `answer_{question_id}`
  - `confidence` must be in range [0.0, 1.0]
  - `timestamp` must be valid ISO-8601 format
  - `stage` must match one of 15 canonical stage names from `PipelineStage` enum
- **Completeness Checks**:
  - All evidence_ids must be globally unique within registry
  - All referenced `source_segment_ids` must exist as evidence_ids in registry
  - All evidence entries must have non-empty `content` field
  - Hash must be reproducible from sorted evidence IDs
- **Relationships**:
  - Referenced by `answers_report.json` via `evidence_ids` arrays in each answer
  - `deterministic_hash` included in `flow_runtime.json` as `evidence_hash`
  - Consumed during QUESTIONNAIRE_EVAL stage for evidence-based scoring
  - Assembled answers registered back into registry during ANSWER_ASSEMBLY stage with provenance metadata
  - Exported to `artifacts/evidence_registry.json` after ANSWER_ASSEMBLY completes

#### 3. answers_report.json

**Purpose**: Complete report of all 300 questionnaire answers with evidence linkage, confidence scores, and rubric alignment (Gate #4, #5).

**Generated By**: `miniminimoon_orchestrator.py` `process_plan_deterministic()` method via `_assemble_answers()` during ANSWER_ASSEMBLY stage, using `answer_assembler.py` `ExternalAnswerAssembler.assemble()` method

**Consumed By**:
- `system_validators.py` (coverage validation ‚â•300 questions, Gate #4)
- `_generate_coverage_report()` (dimension breakdown extraction)
- Scoring and reporting tools
- `evidence_registry.json` (assembled answers registered back with provenance)

**JSON Schema**:
```json
{
  "metadata": {
    "execution_timestamp": "2024-10-07T12:35:00.123456",
    "plan_path": "data/plan.txt",
    "rubric_version": "1.0",
    "assembler_version": "4.0"
  },
  "global_summary": {
    "total_questions": 300,
    "answered_questions": 300,
    "avg_confidence": 0.78,
    "avg_score": 0.72,
    "dimensions": {
      "D1": {"questions": 50, "avg_score": 0.70},
      "D2": {"questions": 50, "avg_score": 0.75}
    }
  },
  "question_answers": [
    {
      "question_id": "DE-1-Q1",
      "dimension": "DE-1",
      "evidence_ids": ["resp_a3f9cd012e", "money_f2c1ab345d"],
      "evidence_count": 2,
      "confidence": 0.85,
      "raw_score": 0.72,
      "rationale": "Evidence-based rationale with confidence adjustment",
      "scoring_modality": "EVIDENCE_BASED",
      "rubric_weight": 0.00333,
      "supporting_quotes": ["El Ministerio de Educaci√≥n implementar√°..."]
    }
  ]
}
```

**Validation Requirements**:
- **Schema Constraints**:
  - `global_summary.total_questions` must equal 300 (DEC√ÅLOGO standard)
  - `global_summary.answered_questions` must be ‚â•300 (Gate #4)
  - Length of `question_answers` array must equal `answered_questions`
  - Each `question_id` must be unique across all answers
  - `confidence` and `raw_score` must be in [0.0, 1.0]
  - `rubric_weight` sum across all answers must equal 1.0 (Gate #5)
  - `evidence_count` must equal length of `evidence_ids` array
  - `scoring_modality` must be one of: EVIDENCE_BASED, HEURISTIC, FALLBACK
- **Completeness Checks**:
  - All 300 questions from `RUBRIC_SCORING.json` must be present
  - All `evidence_ids` must exist in `evidence_registry.json`
  - No missing required fields in any answer
  - Dimension breakdown must cover all 6 dimensions (D1-D6)
- **Relationships**:
  - `question_id` must match keys in `RUBRIC_SCORING.json` `questions` section
  - `rubric_weight` must match value from `RUBRIC_SCORING.json` `weights` section
  - `evidence_ids` must reference valid entries in `evidence_registry.json`
  - Each assembled answer registered back into `evidence_registry.json` with `stage="answers_assembly"`
  - `global_summary` consumed by `_generate_coverage_report()` to create `coverage_report.json`
  - Exported to `artifacts/answers_report.json` via deterministic JSON encoding (sorted keys)

#### 4. answers_sample.json

**Purpose**: Human-readable sample of first 10 answers for quick validation, debugging, and smoke testing.

**Generated By**: `miniminimoon_orchestrator.py` `export_artifacts()` method, extracted from `answers_report.json` during artifact export

**Consumed By**:
- Manual inspection and debugging workflows
- CI/CD smoke tests (quick validation before full test suite)
- Documentation and usage examples

**JSON Schema**:
```json
{
  "metadata": {
    "execution_timestamp": "2024-10-07T12:35:00.123456",
    "plan_path": "data/plan.txt",
    "rubric_version": "1.0",
    "assembler_version": "4.0"
  },
  "global_summary": {
    "total_questions": 300,
    "answered_questions": 300,
    "avg_confidence": 0.78,
    "avg_score": 0.72
  },
  "sample_question_answers": [
    {
      "question_id": "DE-1-Q1",
      "dimension": "DE-1",
      "evidence_ids": ["resp_a3f9cd012e"],
      "confidence": 0.85,
      "raw_score": 0.72,
      "rationale": "...",
      "rubric_weight": 0.00333
    }
  ]
}
```

**Validation Requirements**:
- **Schema Constraints**:
  - `sample_question_answers` must contain exactly 10 answers (or fewer if total < 10)
  - Each answer in sample must match schema from `answers_report.json` `question_answers`
  - `metadata` and `global_summary` must match `answers_report.json` exactly
- **Completeness Checks**:
  - Sample must be exact subset of first 10 entries from `answers_report.json`
  - Order must be deterministic (sorted by `question_id`)
  - All required fields from parent schema must be present
- **Relationships**:
  - Must be synchronized with `answers_report.json` (generated from same source)
  - `evidence_ids` must reference entries in `evidence_registry.json`
  - Exported to `artifacts/answers_sample.json` immediately after `answers_report.json`
  - Used for smoke testing before full validation runs

#### 5. module_to_questions_matrix.csv

**Purpose**: Provenance traceability matrix mapping each question to source detection modules via evidence IDs.

**Generated By**: `trace_matrix.py`

**Consumed By**:
- Audit and compliance verification
- Coverage analysis tools
- Provenance debugging
- External auditors

**Contents**:
```csv
module,question_id,evidence_id,confidence,score
responsibility_detector,DE-1-Q1,responsibility_detector::assignment::a3f9,0.85,0.72
monetary_detector,DE-1-Q2,monetary_detector::amount::f2c1,0.90,0.78
```

**Validation Requirements**:
- **Schema Constraints**:
  - Must be valid UTF-8 CSV with header row
  - Header: `module,question_id,evidence_id,confidence,score`
  - `confidence` and `score` must be parseable as floats
  - No empty cells allowed
- **Completeness Checks**:
  - All `question_id` from `answers_report.json` must be present
  - All `evidence_id` must exist in `evidence_registry.json`
  - Module names must match first segment of evidence_id
- **Relationships**:
  - Direct expansion of `answers_report.json` evidence linkage
  - Enables reverse lookup from question ‚Üí evidence ‚Üí detector
  - Used for coverage analysis across detection modules

#### 6. coverage_report.json

**Purpose**: Questionnaire coverage analysis report showing evaluation completeness across DEC√ÅLOGO dimensions.

**Generated By**: `miniminimoon_orchestrator.py` `process_plan_deterministic()` method, computed from `answers_report.json` during the ANSWER_ASSEMBLY stage

**Consumed By**:
- CI/CD quality gates (Gate #4: ‚â•300 questions answered)
- Coverage monitoring dashboards
- Post-execution validation (`system_validators.py`)

**JSON Schema**:
```json
{
  "total_questions": 300,
  "answered_questions": 300,
  "coverage_percentage": 100.0,
  "dimensions": {
    "D1": {
      "questions": 50,
      "answered": 50
    },
    "D2": {
      "questions": 50,
      "answered": 50
    },
    "D3": {
      "questions": 50,
      "answered": 50
    },
    "D4": {
      "questions": 50,
      "answered": 50
    },
    "D5": {
      "questions": 50,
      "answered": 50
    },
    "D6": {
      "questions": 50,
      "answered": 50
    }
  }
}
```

**Validation Requirements**:
- **Schema Constraints**:
  - `total_questions` must equal 300 (DEC√ÅLOGO standard)
  - `answered_questions` must be ‚â•300 (Gate #4)
  - `coverage_percentage` = (answered_questions / total_questions) * 100, must be in [0.0, 100.0]
  - Each dimension must have exactly 50 questions (6 dimensions √ó 50 = 300)
  - Each dimension's `answered` count must be ‚â§ `questions`
- **Completeness Checks**:
  - All 6 dimensions (D1-D6) must be present
  - Sum of dimension questions must equal `total_questions`
  - Sum of dimension answered must equal `answered_questions`
  - For CI/CD gate to pass: `answered_questions` ‚â• 300 and `coverage_percentage` = 100.0
- **Relationships**:
  - Derived from `answers_report.json` `global_summary` section
  - Exported to `artifacts/coverage_report.json` after ANSWER_ASSEMBLY stage
  - Referenced in `results_bundle.json` for audit trail

#### 7. results_bundle.json

**Purpose**: Comprehensive bundle of pre-validation, execution results, and post-validation for complete audit trail.

**Generated By**: `run_evaluation.py`

**Consumed By**:
- CI/CD audit gates
- Compliance reporting
- End-to-end validation tools

**Contents**:
```json
{
  "pre_validation": {
    "pre_validation_ok": true,
    "checks": [...]
  },
  "pipeline_results": {
    "plan_path": "data/plan.txt",
    "orchestrator_version": "2.0.0",
    "start_time": "ISO-8601",
    "stages_completed": [...],
    "evaluations": {...},
    "evidence_hash": "sha256",
    "validation": {"flow_valid": true, "flow_hash": "sha256"}
  },
  "post_validation": {
    "post_validation_ok": true,
    "checks": [...]
  },
  "bundle_timestamp": "ISO-8601"
}
```

**Validation Requirements**:
- **Schema Constraints**:
  - `pre_validation_ok` and `post_validation_ok` must be boolean
  - All hashes must be SHA-256 format
  - Timestamps must be ISO-8601
- **Completeness Checks**:
  - All validation checks must have status PASS/FAIL
  - `stages_completed` must match `flow_runtime.json`
  - Evidence hash must match `evidence_registry.json`
- **Relationships**:
  - Aggregates data from all other artifacts
  - Primary input for compliance verification
  - Must be immutable once generated

#### 8. final_results.json

**Purpose**: Final aggregated results with deterministic hash for non-repudiation.

**Generated By**: `run_evaluation.py`

**Consumed By**:
- Non-repudiation verification
- Result comparison across runs
- Final reporting and dashboards

**Contents**:
```json
{
  "result_hash": "sha256(evidence_hash + flow_hash + total_questions)",
  "evidence_hash": "sha256",
  "flow_hash": "sha256",
  "summary": {
    "total_questions": 300,
    "overall_score": 0.73,
    "execution_time": 45.2,
    "all_gates_passed": true
  }
}
```

**Validation Requirements**:
- **Schema Constraints**:
  - All hashes must be SHA-256 hex format (64 chars)
  - `overall_score` must be in [0.0, 1.0]
  - `execution_time` must be positive float
  - `all_gates_passed` must be boolean
- **Completeness Checks**:
  - `result_hash` must be verifiable from constituent hashes
  - `evidence_hash` must match `evidence_registry.json`
  - `flow_hash` must match `flow_runtime.json`
- **Relationships**:
  - Top-level summary referencing all other artifacts
  - Used for reproducibility verification across executions
  - Enables cryptographic proof of result integrity

#### 9. nonrepudiation_bundle.zip

**Purpose**: Compressed archive of all artifacts for archival and external audit.

**Generated By**: Archive scripts (external to main pipeline)

**Consumed By**:
- External auditors
- Long-term archival systems
- Compliance verification

**Contents**:
- All JSON artifacts listed above
- `module_to_questions_matrix.csv`
- Manifest file with checksums
- Timestamp and signature metadata

**Validation Requirements**:
- **Schema Constraints**:
  - Must be valid ZIP archive
  - Must include manifest with SHA-256 checksums
  - Must include timestamp of bundle creation
- **Completeness Checks**:
  - All 8 artifacts must be present in archive
  - Checksums in manifest must match actual file hashes
  - No extraneous files allowed
- **Relationships**:
  - Self-contained snapshot of complete execution
  - Can be extracted and validated independently
  - Used for forensic analysis and compliance audits

#### 10. run*.json (Timestamped Execution Records)

**Purpose**: Timestamped copies of execution results for historical comparison.

**Generated By**: `unified_evaluation_pipeline.py` (_export_results)

**Consumed By**:
- Historical analysis tools
- Regression testing
- Performance trend analysis

**Contents**:
```json
{
  "status": "success",
  "metadata": {
    "pdm_path": "path/to/plan.txt",
    "execution_start": "ISO-8601",
    "execution_end": "ISO-8601",
    "execution_time_seconds": 45.2
  },
  "pipeline": {...},
  "evidence_registry": {...},
  "evaluations": {...},
  "validation": {...}
}
```

**Naming Convention**: `{plan_name}_results_{timestamp}.json`

**Validation Requirements**:
- **Schema Constraints**:
  - Must follow unified_evaluation_pipeline result schema
  - Timestamps must be ISO-8601 format
  - Status must be one of: success, failure, partial
- **Completeness Checks**:
  - Must include all pipeline, evidence, and validation sections
  - Metadata must have all required fields
- **Relationships**:
  - Companion files: `{plan_name}_evidence_{timestamp}.json`
  - Enables comparison across multiple runs of same plan
  - Used for detecting regressions and performance changes

### Validation Tool Integration

| Artifact | Generator | Primary Validator | Secondary Validators |
|----------|-----------|-------------------|---------------------|
| flow_runtime.json | miniminimoon_orchestrator.py `_generate_flow_runtime_metadata()` | system_validators.py | CanonicalFlowValidator (Gate #2) |
| evidence_registry.json | miniminimoon_orchestrator.py `_build_evidence_registry()` + `EvidenceRegistry.export()` | system_validators.py | All evaluation modules |
| answers_report.json | miniminimoon_orchestrator.py `_assemble_answers()` via answer_assembler.py | system_validators.py | verify_coverage_metric.py (Gate #4) |
| answers_sample.json | miniminimoon_orchestrator.py `export_artifacts()` | Manual inspection | CI smoke tests |
| coverage_report.json | miniminimoon_orchestrator.py `_generate_coverage_report()` | CI/CD gates (Gate #4) | Quality dashboards |
| module_to_questions_matrix.csv | trace_matrix.py | Audit tools | Coverage analyzers |
| results_bundle.json | unified_evaluation_pipeline.py `evaluate()` | Compliance tools | Audit systems |
| final_results.json | run_evaluation.py | Non-repudiation tools | Comparison utilities |
| nonrepudiation_bundle.zip | Archive scripts | Audit systems | Forensic tools |
| run*.json | unified_evaluation_pipeline.py | Regression tools | Trend analyzers |

### Artifact Dependencies

```
flow_runtime.json
  ‚Üì
evidence_registry.json
  ‚Üì
answers_report.json
  ‚îú‚Üí answers_sample.json
  ‚îî‚Üí module_to_questions_matrix.csv
       ‚Üì
     coverage_report.json
       ‚Üì
     results_bundle.json ‚Üê flow_runtime.json
       ‚Üì                 ‚Üê evidence_registry.json
     final_results.json
       ‚Üì
     nonrepudiation_bundle.zip (all artifacts)
```

### Usage in CI/CD Pipeline

```bash
# Generate artifacts
python run_evaluation.py

# Validate artifacts
python system_validators.py --post --artifacts artifacts/

# Generate traceability
python trace_matrix.py

# Verify coverage
python verify_coverage_metric.py

# All artifacts now in artifacts/ for archival
```

### Artifact Immutability Contract

Once generated, artifacts in `artifacts/` are **immutable**:
- No modification after creation
- Cryptographic hashes verify integrity
- Any change invalidates dependent artifacts
- Reproducibility requires identical inputs ‚Üí identical artifacts

---

See also:
- [Dependency Flows](DEPENDENCY_FLOWS.md)
- [Critical Paths](CRITICAL_PATHS.md)
- [Data Contracts](DATA_CONTRACTS.md)
- [Component Diagram](COMPONENT_DIAGRAM.md)
- Canonical flow order: `tools/flow_doc.json`
