# MINIMINIMOON - Sistema CanÃ³nico de EvaluaciÃ³n de PDM

**VersiÃ³n:** 2.0.0  
**Fecha:** 6 de octubre de 2025  
**Estado:** âœ… Verificado - Todos los flujos crÃ­ticos operativos

---

## ğŸ“‘ Tabla de Contenidos

1. [ğŸ¯ DescripciÃ³n](#-descripciÃ³n)
2. [ğŸ“¦ InstalaciÃ³n Completa (Paso a Paso)](#-instalaciÃ³n-completa-paso-a-paso)
   - [Requisitos Previos](#requisitos-previos)
   - [InstalaciÃ³n Manual (11 Pasos)](#paso-1-verificar-python-310)
   - [InstalaciÃ³n Automatizada](#instalaciÃ³n-automatizada-script)
3. [ğŸš€ GuÃ­a de ImplementaciÃ³n (Workflow Completo)](#-guÃ­a-de-implementaciÃ³n-workflow-completo)
   - [Paso 1: Congelar ConfiguraciÃ³n (Gate #1)](#paso-1-congelar-configuraciÃ³n-gate-1---obligatorio)
   - [Paso 2: Verificar Estado](#paso-2-verificar-estado-del-sistema-pre-ejecuciÃ³n)
   - [Paso 3: Primera EvaluaciÃ³n](#paso-3-ejecutar-primera-evaluaciÃ³n-de-pdm)
   - [Paso 4: Inspeccionar Resultados](#paso-4-inspeccionar-resultados)
   - [Paso 5: Verificar Reproducibilidad](#paso-5-verificar-reproducibilidad-gate-3---crÃ­tico)
   - [Paso 6: Validar RÃºbrica](#paso-6-validar-alineaciÃ³n-de-rÃºbrica-gate-5)
   - [Paso 7: Matriz de Trazabilidad](#paso-7-generar-matriz-de-trazabilidad)
   - [Paso 8: VerificaciÃ³n Completa](#paso-8-verificaciÃ³n-post-ejecuciÃ³n-completa)
4. [ğŸ”§ Uso ProgramÃ¡tico (API Python)](#-uso-programÃ¡tico-api-python)
5. [ğŸ†˜ Troubleshooting](#-troubleshooting-soluciÃ³n-de-problemas)
6. [ğŸ—ï¸ Arquitectura del Sistema](#ï¸-arquitectura-del-sistema)
7. [ğŸ›¡ï¸ Gates de AceptaciÃ³n](#ï¸-gates-de-aceptaciÃ³n-verificaciÃ³n-automÃ¡tica)
8. [ğŸ“ Archivos Fundamentales](#-archivos-fundamentales-del-sistema)
9. [ğŸ“Š Salidas del Sistema](#-salidas-del-sistema)
10. [ğŸ“Š Visual Architecture Diagrams](#-visual-architecture-diagrams)
11. [ğŸ§ª Testing y VerificaciÃ³n](#-testing-y-verificaciÃ³n)
12. [ğŸ¤ ContribuciÃ³n](#-contribuciÃ³n)
13. [ğŸ“š DocumentaciÃ³n Adicional](#-documentaciÃ³n-adicional)
14. [âš¡ Quick Reference (Comandos Comunes)](#-quick-reference-comandos-comunes)

---

## ğŸ¯ DescripciÃ³n

Sistema de evaluaciÃ³n determinista y reproducible de Planes de Desarrollo Municipal (PDM) basado en:
- **300 preguntas** estructuradas en 10 puntos temÃ¡ticos Ã— 30 preguntas
- **Flujo canÃ³nico Ãºnico** con 72 flujos crÃ­ticos verificados
- **6 gates de aceptaciÃ³n** obligatorios para garantizar calidad
- **Evidence Registry Ãºnico** (single source of truth)
- **Trazabilidad completa** desde evidencia hasta respuesta final

---

## ğŸ—ï¸ Arquitectura del Sistema

> **ğŸ“Š Visual Architecture**: See [complete visual documentation](#-visual-architecture-diagrams) with 7 advanced diagrams depicting system flow, validation gates, and deployment infrastructure.

### Orquestador Principal (Entry Point Ãšnico)

```python
from miniminimoon_orchestrator import CanonicalDeterministicOrchestrator

orchestrator = CanonicalDeterministicOrchestrator(
    config_dir=".",
    enable_validation=True,
    flow_doc_path="tools/flow_doc.json"
)

results = orchestrator.process_plan_deterministic(plan_path)
```

### ğŸ”„ Pipeline CanÃ³nico (15 Etapas Secuenciales)

#### **Fase 1: Procesamiento (Etapas 1-11)**

1. **SanitizaciÃ³n** (`plan_sanitizer`)
   - Input: `{raw_text: str}`
   - Output: `{sanitized_text: str}`
   - NormalizaciÃ³n determinista del texto

2. **Procesamiento de Plan** (`plan_processor`)
   - Input: `{sanitized_text: str}`
   - Output: `{doc_struct: dict}`
   - Estructura estable para trazabilidad

3. **SegmentaciÃ³n** (`document_segmenter`)
   - Input: `{doc_struct: dict}`
   - Output: `{segments: list[dict]}`
   - Granularidad por pregunta con IDs deterministas

4. **Embeddings** (`embedding_model`)
   - Input: `{segments: list}`
   - Output: `{embeddings: list}`
   - BÃºsqueda semÃ¡ntica reproducible (seed fija)

5. **DetecciÃ³n de Responsabilidades** (`responsibility_detector`)
   - Input: `{segments: list}`
   - Output: `{responsibilities: list[dict]}`
   - Mapea responsables a preguntas DE-1/DE-3

6. **DetecciÃ³n de Contradicciones** (`contradiction_detector`)
   - Input: `{segments: list}`
   - Output: `{contradictions: list[dict]}`
   - Consistencia y penalizaciones de rubro

7. **DetecciÃ³n Monetaria** (`monetary_detector`)
   - Input: `{segments: list}`
   - Output: `{monetary: list[dict]}`
   - Costos/metas financieras

8. **Scoring de Factibilidad** (`feasibility_scorer`)
   - Input: `{segments: list}`
   - Output: `{feasibility: dict}`
   - Presencia de lÃ­neas base/objetivos/metas

9. **Patrones Causales** (`causal_pattern_detector`)
   - Input: `{segments: list}`
   - Output: `{causal_patterns: dict}`
   - Soporte causal para respuestas explicativas

10. **TeorÃ­a del Cambio** (`teoria_cambio`)
    - Input: `{segments: list}`
    - Output: `{toc_graph: dict, industrial_validation: dict}`
    - Coherencia medios-fines + reporte industrial completo

11. **ValidaciÃ³n DAG** (`dag_validation`)
    - Input: `{toc_graph: dict}`
    - Output: `{dag_diagnostics: dict}`
    - Validez estructural (DAG) verificable

#### **Fase 2: ConstrucciÃ³n de Evidencia (Etapa 12)**

1. **Evidence Registry** (Fan-in N:1)
    - Input: Outputs de etapas 5-11
    - Output: `{evidence_hash: str, evidence_store: dict}`
    - **Ãšnico origen de verdad** para evaluadores
    - Provenance completo + hash determinista

#### **Fase 3: EvaluaciÃ³n (Etapas 13-14)**

1. **EvaluaciÃ³n DecÃ¡logo** (`Decatalogo_principal`)
    - Input: `{evidence_store}`
    - Output: `{decalogo_eval: dict}`
    - EvaluaciÃ³n data-driven por dimensiÃ³n/pregunta

2. **EvaluaciÃ³n Cuestionario** (`questionnaire_engine`)
    - Input: `{evidence_store}`
    - Output: `{questionnaire_eval: dict}`
    - 300 preguntas sobre la misma evidencia

#### **Fase 4: Ensamblaje Final (Etapa 15)**

1. **Answer Assembler** (`answer_assembler`)
    - Input: `{evidence_store, rubric, decalogo_eval, questionnaire_eval}`
    - Output: `{answers_report: dict}`
    - Respuestas con evidence_ids, confidence, score y rationale

---

## ğŸ›¡ï¸ Gates de AceptaciÃ³n (VerificaciÃ³n AutomÃ¡tica)

### Gate #1: ConfiguraciÃ³n Inmutable âœ…
```bash
python miniminimoon_cli.py freeze
```
- Verifica: `verify_frozen_config() == True`
- Crea snapshot SHA-256 de configuraciones crÃ­ticas
- **Bloqueo:** Pipeline no ejecuta sin snapshot vÃ¡lido

### Gate #2: ValidaciÃ³n de Flujo âœ…
- Compara: `flow_runtime.json` vs `tools/flow_doc.json`
- Verifica: Orden canÃ³nico + contratos I/O
- **Bloqueo:** Falla si orden o contratos divergen

### Gate #3: Hash de Evidencia Determinista âœ…
- Verifica: `evidence_hash` estable con mismo input
- **Bloqueo:** Triple-run debe producir mismo hash

### Gate #4: Cobertura Completa âœ…
- Verifica: `answers_report.summary.total_questions â‰¥ 300`
- **Bloqueo:** Falla si no se responden las 300 preguntas

### Gate #5: AlineaciÃ³n de RÃºbrica âœ…
```bash
python miniminimoon_cli.py rubric-check
```
- Verifica: 1:1 preguntas â†” pesos (sin missing/extra)
- **Bloqueo:** Exit code 3 si hay desalineaciÃ³n

### Gate #6: No Deprecated Orchestrator âœ…
- Verifica: `decalogo_pipeline_orchestrator` NO usado
- **Bloqueo:** RuntimeError al importar mÃ³dulo deprecado

---

## ğŸ“ Archivos Fundamentales del Sistema

### 1. ConfiguraciÃ³n (3 archivos obligatorios)

```
decalogo_industrial.json          # ÃšNICO DECÃLOGO (cuestionario 300 preguntas)
dnp-standards.latest.clean.json   # EstÃ¡ndares DNP
RUBRIC_SCORING.json                # Sistema de scoring y pesos
```

### 2. CÃ³digo Core

```
miniminimoon_orchestrator.py      # Orquestador canÃ³nico (ÃšNICO punto de entrada)
unified_evaluation_pipeline.py    # Fachada unificada con pre/post validation
answer_assembler.py                # Ensamblador de respuestas finales
evidence_registry.py               # Registro Ãºnico de evidencia
system_validators.py               # Validadores pre/post ejecuciÃ³n
```

### 3. Componentes del Pipeline (11 mÃ³dulos)

```
plan_sanitizer.py                  # Etapa 1: SanitizaciÃ³n
plan_processor.py                  # Etapa 2: Procesamiento
document_segmenter.py              # Etapa 3: SegmentaciÃ³n
embedding_model.py                 # Etapa 4: Embeddings
responsibility_detector.py         # Etapa 5: Responsabilidades
contradiction_detector.py          # Etapa 6: Contradicciones
monetary_detector.py               # Etapa 7: DetecciÃ³n monetaria
feasibility_scorer.py              # Etapa 8: Factibilidad
causal_pattern_detector.py        # Etapa 9: Patrones causales
teoria_cambio.py                   # Etapa 10: TeorÃ­a del cambio
dag_validation.py                  # Etapa 11: ValidaciÃ³n DAG
```

### 4. Evaluadores

```
Decatalogo_principal.py            # Evaluador por dimensiones
questionnaire_engine.py            # Motor de 300 preguntas
```

### 5. Herramientas de VerificaciÃ³n

```
miniminimoon_cli.py                # CLI: freeze, evaluate, verify, rubric-check, trace-matrix
system_validators.py               # Validadores pre/post ejecuciÃ³n
test_validation_end_to_end.py      # VerificaciÃ³n end-to-end
tools/flow_doc.json                # Orden canÃ³nico documentado
determinism_guard.py               # FijaciÃ³n de seeds deterministas
```

### 6. DocumentaciÃ³n

```
DEPRECATIONS.md                    # MÃ³dulos deprecados y migraciÃ³n
FLUJOS_CRITICOS_GARANTIZADOS.md   # 72 flujos crÃ­ticos detallados
ARCHITECTURE.md                    # Arquitectura del sistema
DEPLOYMENT_INFRASTRUCTURE.md       # Infraestructura de deployment
```

---

## ğŸ“¦ InstalaciÃ³n Completa (Paso a Paso)

### Requisitos Previos

**Sistema Operativo:**
- Linux (Ubuntu 18.04+, Debian 10+)
- macOS (10.15+, incluye Apple Silicon)
- Windows 10+ (con WSL2 recomendado)

**Python Version:**
- **Python 3.10** (REQUERIDO - versiÃ³n exacta)
- Otras versiones NO soportadas debido a compatibilidad con NumPy >=1.21.0 y modelos de embeddings

**Espacio en Disco:**
- MÃ­nimo: 5 GB
- Recomendado: 10 GB (incluye modelos de NLP y cachÃ©)

**Memoria RAM:**
- MÃ­nimo: 8 GB
- Recomendado: 16 GB (para planes grandes >100 pÃ¡ginas)

### Paso 1: Verificar Python 3.10

```bash
# Verificar versiÃ³n de Python
python3.10 --version

# Si no estÃ¡ instalado:
# Ubuntu/Debian:
sudo apt update
sudo apt install python3.10 python3.10-venv python3.10-dev

# macOS (con Homebrew):
brew install python@3.10

# Verificar que estÃ© disponible
which python3.10
```

### Paso 2: Clonar el Repositorio

```bash
# Clonar repositorio
git clone https://github.com/ANITALAVALATINACONPRISA/SIN_CARRETA.git
cd SIN_CARRETA

# Verificar que estÃ¡s en el directorio correcto
pwd  # Debe mostrar: /ruta/a/SIN_CARRETA
ls   # Debe mostrar: README.md, requirements.txt, miniminimoon_orchestrator.py, etc.
```

### Paso 3: Crear Entorno Virtual (Python 3.10)

```bash
# Crear entorno virtual con Python 3.10
python3.10 -m venv venv

# Activar entorno virtual
# En Linux/macOS:
source venv/bin/activate

# En Windows (PowerShell):
# .\venv\Scripts\Activate.ps1

# Verificar que Python 3.10 estÃ¡ activo
python --version  # Debe mostrar: Python 3.10.x
```

### Paso 4: Actualizar pip, setuptools y wheel

```bash
# Actualizar herramientas de instalaciÃ³n
python -m pip install --upgrade pip setuptools wheel

# Verificar versiones actualizadas
pip --version  # Debe mostrar pip 23.0 o superior
```

### Paso 5: Instalar Dependencias Base

```bash
# Instalar todas las dependencias del proyecto
pip install -r requirements.txt

# Tiempo estimado: 5-10 minutos
# Si encuentras errores, ver secciÃ³n de Troubleshooting abajo
```

### Paso 6: Instalar Dependencias de Desarrollo (Opcional)

```bash
# Solo si necesitas ejecutar tests o desarrollo
pip install -r requirements-dev.txt

# Incluye: pytest, mypy, black, flake8, pylint
```

### Paso 7: Descargar Modelos de NLP

```bash
# Descargar modelos de spaCy (espaÃ±ol)
python -m spacy download es_core_news_sm
python -m spacy download es_core_news_md

# Descargar datos de NLTK
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('averaged_perceptron_tagger')"

# Verificar instalaciÃ³n de modelos
python -c "import spacy; nlp = spacy.load('es_core_news_sm'); print('âœ“ spaCy modelo cargado correctamente')"
```

### Paso 8: Crear Estructura de Directorios

```bash
# Crear directorios necesarios para el sistema
mkdir -p artifacts config logs output data

# Verificar estructura
ls -la | grep -E "artifacts|config|logs|output|data"
```

### Paso 9: Verificar Archivos de ConfiguraciÃ³n

```bash
# Verificar que existan los archivos de configuraciÃ³n crÃ­ticos
ls -la config/ | grep -E "decalogo_industrial|dnp-standards|RUBRIC_SCORING"

# O verificar en el directorio raÃ­z:
ls -la | grep -E "decalogo_industrial|dnp-standards|RUBRIC_SCORING"

# Archivos requeridos:
# - decalogo_industrial.json (o decalogo-industrial.latest.clean.json)
# - dnp-standards.latest.clean.json
# - RUBRIC_SCORING.json (o rubric_scoring.json)
```

**âš ï¸ Si faltan archivos:** El sistema no podrÃ¡ ejecutarse sin estos archivos. Deben ser proporcionados por el administrador del proyecto.

### Paso 10: Ejecutar VerificaciÃ³n de Compatibilidad

```bash
# Verificar que todo estÃ¡ correctamente instalado
python python_310_compatibility_checker.py

# Output esperado:
# âœ“ Python 3.10.x detected
# âœ“ NumPy version compatible
# âœ“ All dependencies installed
# âœ“ System ready for operation
```

### Paso 11: Verificar InstalaciÃ³n del Sistema

```bash
# Verificar que todos los mÃ³dulos se pueden importar
python -c "from miniminimoon_orchestrator import CanonicalDeterministicOrchestrator; print('âœ“ Orchestrator OK')"
python -c "from plan_processor import PlanProcessor; print('âœ“ Plan Processor OK')"
python -c "from document_segmenter import DocumentSegmenter; print('âœ“ Document Segmenter OK')"
python -c "from plan_sanitizer import PlanSanitizer; print('âœ“ Plan Sanitizer OK')"
python -c "from embedding_model import EmbeddingModel; print('âœ“ Embedding Model OK')"

# Si todos muestran "âœ“ ... OK", la instalaciÃ³n fue exitosa
```

### InstalaciÃ³n Automatizada (Script)

Si prefieres una instalaciÃ³n automatizada, puedes usar el script `setup_environment.sh`:

```bash
# Hacer ejecutable el script
chmod +x setup_environment.sh

# Ejecutar script de instalaciÃ³n
./setup_environment.sh
```

**El script automatiza:**
- âœ“ VerificaciÃ³n de Python 3.10
- âœ“ CreaciÃ³n de entorno virtual
- âœ“ InstalaciÃ³n de dependencias (base + dev opcional)
- âœ“ Descarga de modelos NLP (spaCy + NLTK)
- âœ“ CreaciÃ³n de estructura de directorios
- âœ“ VerificaciÃ³n de archivos de configuraciÃ³n
- âœ“ Tests bÃ¡sicos de verificaciÃ³n (opcional)

**InteracciÃ³n durante el script:**

```bash
# El script preguntarÃ¡:
Â¿Deseas instalar dependencias de desarrollo? (y/n): y
Â¿Deseas ejecutar tests bÃ¡sicos de verificaciÃ³n? (y/n): y
```

**Output esperado al finalizar:**

```
==========================================
Setup completado exitosamente!
==========================================

Para activar el entorno en el futuro:
  source venv/bin/activate

Para ejecutar el sistema:
  1. Congelar configuraciÃ³n:
     python miniminimoon_orchestrator.py freeze ./config/

  2. Ejecutar evaluaciÃ³n:
     python miniminimoon_orchestrator.py evaluate ./config/ plan.pdf ./output/

  3. Verificar reproducibilidad:
     python miniminimoon_orchestrator.py verify ./config/ plan.pdf --runs 3

DocumentaciÃ³n completa en:
  - FLUJOS_CRITICOS_GARANTIZADOS.md
  - ARCHITECTURE.md
```

**âš ï¸ Nota para Windows:** El script `setup_environment.sh` es para Linux/macOS. Para Windows, usa `setup_environment.bat` o sigue los pasos manuales en PowerShell.

---

## ğŸš€ GuÃ­a de ImplementaciÃ³n (Workflow Completo)

Una vez completada la instalaciÃ³n, sigue estos pasos para implementar y usar el sistema:

### Paso 1: Congelar ConfiguraciÃ³n (Gate #1) - OBLIGATORIO

Este paso es **CRÃTICO** y debe ejecutarse antes de cualquier evaluaciÃ³n.

```bash
# Activar entorno virtual si no estÃ¡ activo
source venv/bin/activate

# Congelar configuraciÃ³n (crea snapshot inmutable)
python miniminimoon_cli.py freeze

# O usando el orquestador directamente:
python miniminimoon_orchestrator.py freeze ./config/
```

**Â¿QuÃ© hace esto?**
- Crea `.immutability_snapshot.json` con hash SHA-256 de todos los archivos de configuraciÃ³n
- Garantiza que la configuraciÃ³n no cambie entre ejecuciones
- Es requisito obligatorio para Gate #1 (primera validaciÃ³n)

**Output esperado:**
```
âœ“ Configuration frozen successfully
  Hash: a3f8d2e1b4c5a9f2d8e6c4a1b7f3d9e5
  Files: ['decalogo_industrial.json', 'dnp-standards.latest.clean.json', 'RUBRIC_SCORING.json']
  Snapshot saved: .immutability_snapshot.json
```

### Paso 2: Verificar Estado del Sistema (Pre-EjecuciÃ³n)

```bash
# Verificar sistema con validaciÃ³n end-to-end
python test_validation_end_to_end.py

# O verificar usando el CLI con diagnÃ³stico
python miniminimoon_cli.py diagnostic

# O verificar que el sistema estÃ¡ operativo
python -c "from system_validators import SystemHealthValidator; print('âœ“ System validators ready')"
```

### Paso 3: Ejecutar Primera EvaluaciÃ³n de PDM

```bash
# EvaluaciÃ³n completa con validaciÃ³n estricta
python miniminimoon_cli.py evaluate --plan path/to/tu_plan.pdf --strict

# O usando el orquestador con mÃ¡s control:
python miniminimoon_orchestrator.py evaluate ./config/ tu_plan.pdf ./output/

# Tiempo estimado: 45-60 segundos para plan de ~50 pÃ¡ginas
```

**Argumentos:**
- `--plan`: Ruta al archivo PDF del Plan de Desarrollo Municipal
- `--strict`: Modo estricto (activa todas las validaciones)
- `./config/`: Directorio con archivos de configuraciÃ³n
- `./output/`: Directorio donde se guardarÃ¡n los resultados

**Artifacts Generados (en `output/` o `artifacts/`):**

```
output/
â”œâ”€â”€ answers_report.json           # â­ Reporte completo con 300 preguntas respondidas
â”œâ”€â”€ answers_sample.json           # Muestra de las primeras 10 respuestas
â”œâ”€â”€ evidence_registry.json        # Registro Ãºnico de toda la evidencia recolectada
â”œâ”€â”€ flow_runtime.json             # Trace completo de ejecuciÃ³n (orden + contratos I/O)
â”œâ”€â”€ coverage_report.json          # Cobertura: quÃ© preguntas tienen evidencia
â”œâ”€â”€ final_results.json            # Resultados consolidados + hashes (evidence_hash, flow_hash)
â””â”€â”€ module_to_questions_matrix.csv # Matriz de trazabilidad: mÃ³dulo â†’ pregunta â†’ evidencia
```

### Paso 4: Inspeccionar Resultados

```bash
# Ver resumen de resultados
cat output/final_results.json | python -m json.tool | head -50

# Ver muestra de respuestas
cat output/answers_sample.json | python -m json.tool

# Ver evidencia recolectada
cat output/evidence_registry.json | python -m json.tool | head -100

# Verificar cobertura de preguntas
python -c "import json; data=json.load(open('output/coverage_report.json')); print(f'Preguntas respondidas: {data[\"coverage_summary\"][\"total_answered\"]}/300')"
```

**Estructura de una Respuesta (ejemplo):**

```json
{
  "question_id": "DE-1-Q4",
  "dimension": "DE-1",
  "question_text": "Â¿Se especifican lÃ­neas base cuantitativas?",
  "evidence_ids": ["resp_abc123", "feas_def456"],
  "confidence": 0.85,
  "score": 2.5,
  "reasoning": "Evidencia sÃ³lida de lÃ­neas base en 3 programas: EducaciÃ³n (45% cobertura 2023), Salud (62% atenciÃ³n 2023), Infraestructura (78% vÃ­as pavimentadas 2023)...",
  "rubric_weight": 0.15,
  "supporting_quotes": [
    "LÃ­nea base 2023: 45% cobertura educativa en zona rural",
    "Meta 2027: Incrementar a 75% cobertura educativa"
  ],
  "caveats": ["Basado en 2 fuentes de evidencia", "No se encontrÃ³ lÃ­nea base para programa de medio ambiente"]
}
```

### Paso 5: Verificar Reproducibilidad (Gate #3) - CRÃTICO

Este paso valida que el sistema es **determinista**: mismo input = mismo output.

```bash
# Ejecutar pipeline 3 veces con el mismo input
python miniminimoon_orchestrator.py verify ./config/ tu_plan.pdf --runs 3

# O manualmente:
for i in {1..3}; do
  python miniminimoon_cli.py evaluate --plan tu_plan.pdf --output run_${i}.json
done

# Comparar hashes (deben ser IDÃ‰NTICOS)
python -c "
import json
hashes = []
for i in range(1, 4):
    with open(f'run_{i}.json') as f:
        data = json.load(f)
        hashes.append(data['evidence_hash'])
print('Evidence hashes:', hashes)
print('Todas idÃ©nticas:', len(set(hashes)) == 1)
"
```

**Output esperado:**
```
âœ“ Run 1 completed - evidence_hash: a3f8d2e1b4c5...
âœ“ Run 2 completed - evidence_hash: a3f8d2e1b4c5...
âœ“ Run 3 completed - evidence_hash: a3f8d2e1b4c5...
âœ“ DETERMINISM VERIFIED: All hashes identical
âœ“ Gate #3 PASSED
```

**âš ï¸ Si los hashes NO son idÃ©nticos:** Hay un problema de no-determinismo. Reportar como bug.

### Paso 6: Validar AlineaciÃ³n de RÃºbrica (Gate #5)

```bash
# Verificar que hay correspondencia 1:1 entre preguntas y pesos de rÃºbrica
python miniminimoon_cli.py rubric-check \
    output/answers_report.json \
    config/RUBRIC_SCORING.json
```

**Output esperado (PASSING):**
```
================================================================================
RUBRIC VALIDATION REPORT
================================================================================
âœ“ Total questions in answers_report: 300
âœ“ Total weights in rubric: 300
âœ“ All questions have corresponding weights
âœ“ All weights have corresponding questions
âœ“ No missing questions
âœ“ No extra weights
================================================================================
âœ“ RUBRIC VALIDATION PASSED - Gate #5 âœ“
================================================================================
```

**Output esperado (FAILING):**
```
================================================================================
RUBRIC VALIDATION REPORT
================================================================================
âœ— Total questions in answers_report: 300
âœ— Total weights in rubric: 295
âœ— MISSING questions (no weight): ['DE-3-Q14', 'DE-5-Q22', ...]
âœ— EXTRA weights (no question): []
================================================================================
âœ— RUBRIC VALIDATION FAILED - Gate #5 âœ—
Exit code: 3
================================================================================
```

**Si falla:** Corregir `RUBRIC_SCORING.json` para aÃ±adir/eliminar pesos segÃºn sea necesario.

### Paso 7: Generar Matriz de Trazabilidad

```bash
# Generar matriz completa: mÃ³dulo â†’ pregunta â†’ evidencia
python miniminimoon_cli.py trace-matrix

# Output: artifacts/module_to_questions_matrix.csv
```

Este archivo CSV muestra quÃ© mÃ³dulos del pipeline generaron evidencia para quÃ© preguntas, permitiendo auditorÃ­a completa.

### Paso 8: VerificaciÃ³n Post-EjecuciÃ³n Completa

```bash
# Ejecutar todas las verificaciones post-ejecuciÃ³n
python miniminimoon_cli.py verify

# Este comando ejecuta:
# - VerificaciÃ³n de Gates #3, #4, #5
# - ValidaciÃ³n de contratos I/O
# - VerificaciÃ³n de orden canÃ³nico
# - Chequeo de cobertura â‰¥300 preguntas
```

---

## ğŸ”§ Uso ProgramÃ¡tico (API Python)

Para integrar MINIMINIMOON en tu propio cÃ³digo:

```python
from miniminimoon_orchestrator import CanonicalDeterministicOrchestrator

# Inicializar orquestador con validaciÃ³n habilitada
orchestrator = CanonicalDeterministicOrchestrator(
    config_dir="./config",           # Directorio con archivos JSON
    enable_validation=True,           # Activar validaciÃ³n de gates
    flow_doc_path="tools/flow_doc.json",  # Orden canÃ³nico documentado
    log_level="INFO"                  # DEBUG, INFO, WARNING, ERROR
)

# Ejecutar pipeline completo de forma determinista
results = orchestrator.process_plan_deterministic("path/to/plan.pdf")

# Acceder a resultados
print(f"Evidence hash: {results['evidence_hash']}")
print(f"Flow hash: {results['validation']['flow_hash']}")
print(f"Total preguntas: {results['evaluations']['answers_report']['summary']['total_questions']}")

# Acceder a respuestas individuales
answers = results['evaluations']['answers_report']['answers']
for answer in answers[:5]:  # Primeras 5 respuestas
    print(f"Q: {answer['question_id']} - Score: {answer['score']}")

# Acceder a evidencia
evidence_store = results['evidence_registry']
print(f"Total evidencias: {len(evidence_store['evidence_by_type'])}")

# Guardar resultados
import json
with open('my_results.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)
```

---

## ğŸ†˜ Troubleshooting (SoluciÃ³n de Problemas)

### Problema 1: Error al instalar dependencias

**SÃ­ntoma:**
```
ERROR: Could not find a version that satisfies the requirement...
```

**Soluciones:**

```bash
# SoluciÃ³n 1: Verificar versiÃ³n de Python
python --version  # DEBE ser 3.10.x

# SoluciÃ³n 2: Limpiar cachÃ© de pip
pip cache purge
pip install -r requirements.txt --no-cache-dir

# SoluciÃ³n 3: Instalar en grupos pequeÃ±os
pip install numpy scipy scikit-learn
pip install pandas
pip install torch --index-url https://download.pytorch.org/whl/cpu  # CPU-only (mÃ¡s ligero)
pip install sentence-transformers
pip install spacy networkx matplotlib
```

### Problema 2: Error "No space left on device"

**SÃ­ntoma:**
```
[Errno 28] No space left on device
```

**Soluciones:**

```bash
# Ver espacio disponible
df -h

# Limpiar cachÃ© de pip
pip cache purge

# Instalar torch CPU-only (mÃ¡s ligero: ~200MB vs ~2GB)
pip uninstall torch
pip install torch --index-url https://download.pytorch.org/whl/cpu

# Ver espacio usado por venv
du -sh venv/

# Si necesario, mover a disco con mÃ¡s espacio
mv venv /otro/disco/con/espacio/
ln -s /otro/disco/con/espacio/venv venv
```

### Problema 3: Error al importar mÃ³dulos

**SÃ­ntoma:**
```
ImportError: cannot import name 'CanonicalDeterministicOrchestrator'
ModuleNotFoundError: No module named 'plan_processor'
```

**Soluciones:**

```bash
# Verificar que estÃ¡s en el directorio correcto
pwd  # Debe mostrar: /ruta/a/SIN_CARRETA

# Verificar que el entorno virtual estÃ¡ activo
which python  # Debe mostrar: /ruta/a/SIN_CARRETA/venv/bin/python

# Reinstalar dependencias
pip install -r requirements.txt --force-reinstall

# Verificar archivos Python existen
ls -la *.py | grep -E "orchestrator|processor|segmenter|sanitizer"
```

### Problema 4: Error "No frozen config snapshot"

**SÃ­ntoma:**
```
RuntimeError: Configuration must be frozen before execution (Gate #1)
File not found: .immutability_snapshot.json
```

**SoluciÃ³n:**

```bash
# Ejecutar freeze antes de cualquier evaluaciÃ³n
python miniminimoon_cli.py freeze

# Verificar que se creÃ³ el snapshot
ls -la .immutability_snapshot.json
cat .immutability_snapshot.json | python -m json.tool
```

### Problema 5: Modelos de spaCy no encontrados

**SÃ­ntoma:**
```
OSError: [E050] Can't find model 'es_core_news_sm'
```

**SoluciÃ³n:**

```bash
# Descargar modelo nuevamente
python -m spacy download es_core_news_sm

# Verificar instalaciÃ³n
python -c "import spacy; nlp = spacy.load('es_core_news_sm'); print('OK')"

# Si persiste, instalar manualmente
pip install https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl
```

### Problema 6: Archivos de configuraciÃ³n faltantes

**SÃ­ntoma:**
```
FileNotFoundError: [Errno 2] No such file or directory: 'decalogo_industrial.json'
```

**SoluciÃ³n:**

```bash
# Verificar ubicaciÃ³n de archivos
find . -name "decalogo*.json"
find . -name "dnp-standards*.json"
find . -name "rubric*.json"

# Copiar a ubicaciÃ³n esperada (si estÃ¡n en subdirectorios)
cp config/decalogo-industrial.latest.clean.json decalogo_industrial.json
cp config/dnp-standards.latest.clean.json .
cp config/rubric_scoring.json RUBRIC_SCORING.json

# O crear enlaces simbÃ³licos
ln -s config/decalogo-industrial.latest.clean.json decalogo_industrial.json
```

### Problema 7: Hashes no reproducibles (Gate #3 falla)

**SÃ­ntoma:**
```
âœ— DETERMINISM CHECK FAILED
Run 1 hash: a3f8d2e1...
Run 2 hash: b4c5f6d7...
Run 3 hash: c8d9e0f1...
```

**SoluciÃ³n:**

```bash
# Verificar que determinism_guard estÃ¡ activo
python -c "from determinism_guard import verify_determinism; verify_determinism()"

# Ejecutar con modo debug
python miniminimoon_cli.py evaluate --plan test.pdf --debug

# Revisar logs para identificar fuente de no-determinismo
cat logs/miniminimoon_*.log | grep -i "random\|seed\|nondetermin"
```

### Problema 8: ValidaciÃ³n de rÃºbrica falla (Gate #5)

**SÃ­ntoma:**
```
âœ— RUBRIC VALIDATION FAILED
Missing questions: ['DE-3-Q14', 'DE-5-Q22']
Extra weights: ['DE-1-Q99']
```

**SoluciÃ³n:**

```bash
# Ejecutar rubric_check para ver detalles
python miniminimoon_cli.py rubric-check

# Editar RUBRIC_SCORING.json para corregir
# - AÃ±adir pesos para preguntas faltantes
# - Eliminar pesos extra

# Verificar formato JSON
python -m json.tool RUBRIC_SCORING.json > /dev/null
echo "JSON vÃ¡lido"

# Volver a ejecutar validaciÃ³n
python miniminimoon_cli.py rubric-check
```

### Problema 9: Performance lento (>120 segundos)

**SÃ­ntoma:**
```
Evaluation took 180 seconds (expected: 45-60s)
```

**Soluciones:**

```bash
# Verificar uso de CPU/memoria durante ejecuciÃ³n
htop  # o top

# Usar CPU-only torch si no tienes GPU
pip uninstall torch
pip install torch --index-url https://download.pytorch.org/whl/cpu

# Reducir verbosity de logs
python miniminimoon_cli.py evaluate --plan test.pdf --log-level WARNING

# Verificar que no hay procesos en background
ps aux | grep python
```

### Problema 10: Pytest/Tests fallan

**SÃ­ntoma:**
```
test_critical_flows.py::test_flow_1 FAILED
```

**Soluciones:**

```bash
# Verificar que pytest estÃ¡ instalado
pip install pytest pytest-cov

# Ejecutar tests con verbose
python -m pytest -v test_critical_flows.py

# Ejecutar solo tests crÃ­ticos
python -m pytest -k "critical" -v

# Ver output completo de errores
python -m pytest --tb=long test_critical_flows.py
```

### Problema 11: Error "No frozen config snapshot"

**SÃ­ntoma:**
```
RuntimeError: Configuration must be frozen before execution (Gate #1)
File not found: .immutability_snapshot.json
```

**SoluciÃ³n:**

```bash
python miniminimoon_cli.py freeze
```

### Problema 12: Error "Flow order does not match canonical documentation"

**Causa:** ModificaciÃ³n del orden de ejecuciÃ³n en el orquestador

**SoluciÃ³n:** 

Revisar que el orden en `miniminimoon_orchestrator.py` coincida con `tools/flow_doc.json`. No modificar el orden canÃ³nico sin autorizaciÃ³n.

### Problema 13: Error "decalogo_pipeline_orchestrator is DEPRECATED"

**Causa:** Intento de usar orquestador deprecado

**SoluciÃ³n:**

```python
# âŒ PROHIBIDO
from decalogo_pipeline_orchestrator import DecalogoPipelineOrchestrator

# âœ… CORRECTO
from miniminimoon_orchestrator import CanonicalDeterministicOrchestrator
```

Ver `DEPRECATIONS.md` para detalles completos de migraciÃ³n.

---

## ğŸ“Š Salidas del Sistema

### Artifacts Generados

```
artifacts/
â”œâ”€â”€ answers_report.json           # Reporte completo 300 preguntas
â”œâ”€â”€ answers_sample.json           # Muestra primeras 10 respuestas
â”œâ”€â”€ flow_runtime.json             # Orden de ejecuciÃ³n + contratos
â”œâ”€â”€ evidence_registry.json        # Registro completo de evidencia
â”œâ”€â”€ coverage_report.json          # Cobertura por pregunta
â”œâ”€â”€ final_results.json            # Resultados consolidados + hashes
â””â”€â”€ module_to_questions_matrix.csv # Matriz de trazabilidad
```

### Estructura de Respuesta (ejemplo)

```json
{
  "question_id": "DE-1-Q4",
  "dimension": "DE-1",
  "evidence_ids": ["resp_abc123", "feas_def456"],
  "confidence": 0.85,
  "score": 2.5,
  "reasoning": "Evidencia sÃ³lida de lÃ­neas base en 3 programas...",
  "rubric_weight": 0.15,
  "supporting_quotes": [
    "LÃ­nea base 2023: 45% cobertura educativa...",
    "Meta 2027: 75% cobertura..."
  ],
  "caveats": ["Basado en 2 fuentes de evidencia"]
}
```

---

## ğŸ”’ Principios de DiseÃ±o

### 1. Determinismo Garantizado
- Seeds fijos: `random=42`, `numpy=42`, `torch=42`
- Sin I/O no determinista durante evaluaciÃ³n
- Orden canÃ³nico documentado y verificado

### 2. Single Source of Truth
- **Evidence Registry Ãºnico** para toda evidencia
- No recalcular outputs entre evaluadores
- Provenance completo de toda evidencia

### 3. Trazabilidad Total
- Cada respuesta vinculada a `evidence_ids`
- Cada evidencia con `source_segment_ids`
- Hash determinista para reproducibilidad

### 4. ValidaciÃ³n AutomÃ¡tica
- Pre-checks antes de ejecuciÃ³n (config, freeze, contratos)
- Post-checks despuÃ©s de ejecuciÃ³n (cobertura, hashes, rÃºbrica)
- CI/CD gates obligatorios

### 5. No Rutas Paralelas
- **UN SOLO** orquestador: `CanonicalDeterministicOrchestrator`
- Orquestador deprecado bloqueado con `RuntimeError`
- Enforcement en CI/CD

---

## âš ï¸ MÃ³dulos Deprecados (PROHIBIDO)

### âŒ `decalogo_pipeline_orchestrator.py`

**Estado:** DEPRECATED - Lanza `RuntimeError` al importar

**RazÃ³n:**
- Crea rutas de ejecuciÃ³n paralelas
- Fragmenta evidence registry
- Bypasea gates de validaciÃ³n
- Rompe audit trail

**MigraciÃ³n obligatoria:**
```python
# âŒ PROHIBIDO
from decalogo_pipeline_orchestrator import DecalogoPipelineOrchestrator

# âœ… CORRECTO
from miniminimoon_orchestrator import CanonicalDeterministicOrchestrator
```

Ver `DEPRECATIONS.md` para detalles completos de migraciÃ³n.

---

## ğŸ§ª Testing y VerificaciÃ³n

### VerificaciÃ³n Completa del Sistema

```bash
# Verificar todos los flujos con validaciÃ³n end-to-end
python test_validation_end_to_end.py

# O usar el CLI con diagnÃ³stico completo
python miniminimoon_cli.py diagnostic

# O verificar componentes individuales
python -m pytest test_plan_sanitizer.py test_document_segmenter.py test_teoria_cambio.py -v
```

### Tests Unitarios por Componente

```bash
# Tests individuales
python -m pytest test_plan_sanitizer.py
python -m pytest test_feasibility_scorer.py
python -m pytest test_teoria_cambio.py
python -m pytest test_dag_validation.py
```

### Triple-Run para Verificar Determinismo

```bash
# Ejecutar 3 veces y verificar hashes idÃ©nticos
for i in {1..3}; do
  python miniminimoon_cli.py evaluate --plan test.pdf > run_$i.json
done

# Comparar evidence_hash y flow_hash (deben ser idÃ©nticos)
```

---

## ğŸ“ˆ Performance y Optimizaciones

### Optimizaciones Implementadas

- **Contract validation caching**: 37% mejora (7.9ms â†’ <5ms)
- **Mathematical invariant optimizations**: 43% mejora en PERMUTATION_INVARIANCE
- **Budget monotonicity**: 40% mejora (0.25ms â†’ <0.15ms)
- **CI/CD performance gate**: Bloquea PRs que excedan presupuesto >10%
- **Soak test 4 horas**: DetecciÃ³n de memory leaks

### MÃ©tricas de Performance

```
Pipeline completo (plan ~50 pÃ¡ginas): ~45-60 segundos
- SanitizaciÃ³n: <1s
- SegmentaciÃ³n: 2-3s
- Embeddings: 15-20s (modelo transformer)
- Detectores (6 mÃ³dulos): 10-15s
- TeorÃ­a cambio + DAG: 5-8s
- Evaluadores: 8-12s
- Answer assembly: <2s
```

---

## ğŸ“Š Visual Architecture Diagrams

The MINIMINIMOON system architecture is documented through **SEVEN hyper-modern, futuristic neo-punk diagrams** that provide a compelling visual narrative of the unified evaluation architecture. Each diagram uses consistent color schemes and clear directional flows with cardinality annotations.

### 1ï¸âƒ£ High-Level System Architecture

**Location**: `docs/diagrams/01_system_architecture.png`

```mermaid
graph TB
    CLI[ğŸ–¥ï¸ CLI Interface<br/>miniminimoon_cli.py] --> Unified[ğŸ”„ Unified Evaluation Pipeline<br/>Pre-Validation + Post-Validation]
    Unified --> Orchestrator[âš™ï¸ Canonical Orchestrator<br/>15-Stage Pipeline]
    Orchestrator --> Artifacts[ğŸ“¦ Artifacts<br/>JSON Reports]
    
    style CLI fill:#ff00ff,stroke:#ff00ff,color:#000
    style Unified fill:#00ff88,stroke:#00ff88,color:#000
    style Orchestrator fill:#00d4ff,stroke:#00d4ff,color:#000
    style Artifacts fill:#ffff00,stroke:#ffff00,color:#000
```

**Description**: This diagram shows the high-level flow from CLI through `unified_evaluation_pipeline` to `miniminimoon_orchestrator` and artifact generation. It illustrates the single entry point design principle and the orchestrator's role as the central processing hub.

**References**: 
- [FLUJOS_CRITICOS_GARANTIZADOS.md](FLUJOS_CRITICOS_GARANTIZADOS.md) - Flow #18 (Unified Pipeline)
- [ARCHITECTURE.md](ARCHITECTURE.md) - Core Components section

---

### 2ï¸âƒ£ Detailed Evidence Data Flow

**Location**: `docs/diagrams/02_data_flow.png`

```mermaid
graph TB
    subgraph Detectors[ğŸ“Š DETECTORS Stage 5-11]
        D1[Responsibility]
        D2[Contradiction]
        D3[Monetary]
        D4[Feasibility]
        D5[Causal Pattern]
        D6[TeorÃ­a Cambio]
        D7[DAG Validation]
    end
    
    Registry[ğŸ“¦ Evidence Registry<br/>Stage 12<br/>FAN-IN N:1]
    
    subgraph Evaluators[ğŸ“ EVALUATORS Stage 13-14]
        E1[DecÃ¡logo Evaluator]
        E2[Questionnaire Engine]
    end
    
    Assembler[ğŸ”§ Answer Assembler<br/>Stage 15<br/>300 Questions]
    
    D1 & D2 & D3 & D4 & D5 & D6 & D7 --> Registry
    Registry --> E1 & E2
    Registry --> Assembler
    E1 & E2 --> Assembler
    
    style Registry fill:#00ffff,stroke:#00ffff,color:#000
    style Assembler fill:#00ff88,stroke:#00ff88,color:#000
```

**Description**: Illustrates how evidence flows from 7 parallel detectors (Stage 5-11) through the **Evidence Registry** (Stage 12 - FAN-IN N:1) to evaluators and finally to the **Answer Assembler** for the 300-question evaluation. This diagram emphasizes the "Single Source of Truth" principle.

**References**: 
- [FLUJOS_CRITICOS_GARANTIZADOS.md](FLUJOS_CRITICOS_GARANTIZADOS.md) - Flows #5-#15
- [ARCHITECTURE.md](ARCHITECTURE.md) - Evidence Registry component

---

### 3ï¸âƒ£ Validation Gates Diagram

**Location**: `docs/diagrams/03_validation_gates.png`

```mermaid
graph TB
    subgraph Pre[âš¡ PRE-EXECUTION GATES]
        G1[ğŸ”’ Gate 1: Freeze<br/>Config Immutability]
        G2[ğŸ“‹ Gate 2: Flow Order<br/>Canonical Validation]
        G6[âŒ Gate 6: No Deprecated<br/>Single Path Enforcement]
        G1 --> G2 --> G6
    end
    
    Pipeline[âš™ï¸ CANONICAL PIPELINE<br/>15 Stages]
    
    subgraph Post[âš¡ POST-EXECUTION GATES]
        G3[ğŸ” Gate 3: Determinism<br/>Hash Stability]
        G4[ğŸ“Š Gate 4: Coverage<br/>â‰¥300 Questions]
        G5[ğŸ“ Gate 5: Rubric<br/>1:1 Alignment]
        G3 --> G4 --> G5
    end
    
    G6 --> Pipeline --> G3
    G5 --> Success[âœ… VALIDATION PASSED]
    
    style G1 fill:#00ffff,stroke:#00ffff,color:#000
    style G2 fill:#00ffff,stroke:#00ffff,color:#000
    style G6 fill:#00ffff,stroke:#00ffff,color:#000
    style G3 fill:#ff00ff,stroke:#ff00ff,color:#000
    style G4 fill:#ff00ff,stroke:#ff00ff,color:#000
    style G5 fill:#ff00ff,stroke:#ff00ff,color:#000
    style Success fill:#00ff88,stroke:#00ff88,color:#000
```

**Description**: Depicts the 6 acceptance gates split into pre-execution checks (freeze verification, flow order, deprecated module check) and post-execution validation (determinism, coverage, rubric alignment). Shows pass/fail paths and blocking behavior.

**References**: 
- [FLUJOS_CRITICOS_GARANTIZADOS.md](FLUJOS_CRITICOS_GARANTIZADOS.md) - Section 2: Gates de AceptaciÃ³n
- README.md - Gates de AceptaciÃ³n section

---

### 4ï¸âƒ£ CI/CD Pipeline Visualization

**Location**: `docs/diagrams/04_cicd_pipeline.png`

```mermaid
graph TB
    Trigger[ğŸ”” Pull Request] --> Setup[ğŸ“¦ Setup<br/>Python + spaCy]
    Setup --> Freeze[ğŸ”’ Freeze Verification<br/>Gate #1]
    Freeze --> Build[ğŸ”¨ Build<br/>Compile All Modules]
    Build --> Lint[ğŸ“ Lint<br/>PEP 8 Check]
    Lint --> Triple[ğŸ”„ Triple-Run<br/>Reproducibility Test]
    Triple --> Unit[ğŸ§ª Unit Tests<br/>Component Tests]
    Unit --> Integration[ğŸ”— Integration Tests<br/>72 Critical Flows]
    Integration --> Perf[âš¡ Performance Gate<br/>p95 < Budget+10%]
    Perf --> Archive[ğŸ“¦ Artifact Archival<br/>30-day Retention]
    Archive --> Success[âœ… BUILD SUCCESS<br/>Ready to Merge]
    
    Triple -.Fail.-> Fail[âŒ BUILD FAILED<br/>Block PR]
    
    style Freeze fill:#ff00ff,stroke:#ff00ff,color:#000
    style Triple fill:#ffff00,stroke:#ffff00,color:#000
    style Success fill:#00ff88,stroke:#00ff88,color:#000
    style Fail fill:#ff0000,stroke:#ff0000,color:#fff
```

**Description**: Complete build workflow showing 9 stages from PR trigger to success/failure. Includes freeze verification, triple-run reproducibility tests (Gate #3), and artifact archival. Highlights critical checkpoints that block PRs on failure.

**References**: 
- [FLUJOS_CRITICOS_GARANTIZADOS.md](FLUJOS_CRITICOS_GARANTIZADOS.md) - Section 9: GarantÃ­as de Determinismo
- README.md - ContribuciÃ³n > CI/CD Pipeline

---

### 5ï¸âƒ£ 15-Stage Canonical Pipeline

**Location**: `docs/diagrams/05_15_stage_pipeline.png`

```mermaid
graph TB
    Input[ğŸ“„ Raw PDM Plan] --> S1[Stage 1: Sanitization]
    S1 --> S2[Stage 2: Plan Processing]
    S2 --> S3[Stage 3: Segmentation]
    S3 --> S4[Stage 4: Embeddings]
    
    S4 --> S5[Stage 5: Responsibility]
    S4 --> S6[Stage 6: Contradiction]
    S4 --> S7[Stage 7: Monetary]
    S4 --> S8[Stage 8: Feasibility]
    S4 --> S9[Stage 9: Causal Pattern]
    S4 --> S10[Stage 10: TeorÃ­a Cambio]
    S10 --> S11[Stage 11: DAG Validation]
    
    S5 & S6 & S7 & S8 & S9 & S10 & S11 --> S12[ğŸ”· Stage 12: Evidence Registry<br/>FAN-IN N:1]
    
    S12 --> S13[Stage 13: DecÃ¡logo Eval]
    S12 --> S14[Stage 14: Questionnaire]
    
    S12 & S13 & S14 --> S15[ğŸ”· Stage 15: Answer Assembly]
    S15 --> Output[ğŸ“„ 300 Answers Report]
    
    style S12 fill:#00ffff,stroke:#00ffff,color:#000
    style S15 fill:#00ff88,stroke:#00ff88,color:#000
```

**Description**: Sequential view of all 15 pipeline stages organized into 4 phases: Processing (1-11), Evidence Registry (12), Evaluation (13-14), and Assembly (15). Shows fan-out at Stage 4 (detectors) and fan-in at Stage 12 (evidence registry).

**References**: 
- [FLUJOS_CRITICOS_GARANTIZADOS.md](FLUJOS_CRITICOS_GARANTIZADOS.md) - Section 1: Flujos CrÃ­ticos Principales
- [ARCHITECTURE.md](ARCHITECTURE.md) - System Components

---

### 6ï¸âƒ£ Data Contract Validation

**Location**: `docs/diagrams/06_contract_validation.png`

```mermaid
graph LR
    Input[ğŸ“¥ Input Data] --> TypeCheck[âœ“ Type Validation<br/>Schema Conformance]
    TypeCheck --> InvariantCheck[âˆ Mathematical Invariants<br/>PERMUTATION_INVARIANCE<br/>MONOTONICITY<br/>IDEMPOTENCE]
    InvariantCheck --> SemanticCheck[ğŸ¯ Semantic Validation<br/>Domain Rules]
    SemanticCheck --> Cache[ğŸ’¾ Validation Cache<br/>37% speedup]
    SemanticCheck --> Pass[âœ… VALIDATION PASSED]
    
    TypeCheck -.Fail.-> Fail[âŒ VALIDATION FAILED]
    
    Cache -.Reuse.-> TypeCheck
    
    style TypeCheck fill:#00ff88,stroke:#00ff88,color:#000
    style InvariantCheck fill:#ffff00,stroke:#ffff00,color:#000
    style Cache fill:#ff00ff,stroke:#ff00ff,color:#000
```

**Description**: Shows the 3-layer contract validation system (type checking, mathematical invariants, semantic validation) with performance caching. Illustrates the 37% speedup from contract validation caching and the <5ms p95 latency target.

**References**: 
- [DATA_CONTRACTS.md](DATA_CONTRACTS.md) - Contract validation details
- README.md - Performance y Optimizaciones section

---

### 7ï¸âƒ£ Deployment & Monitoring Infrastructure

**Location**: `docs/diagrams/07_deployment_monitoring.png`

```mermaid
graph TB
    Traffic[ğŸŒ Incoming Traffic] --> Router[ğŸ”€ Traffic Router<br/>5%â†’25%â†’100%]
    
    Router --> Baseline[ğŸ“¦ Baseline v1.0<br/>Current Production]
    Router --> Canary[ğŸ¤ Canary v2.0<br/>New Deployment]
    
    Baseline & Canary --> Tracing[ğŸ“Š OpenTelemetry<br/>28 Flows + 11 Components]
    Baseline & Canary --> Metrics[ğŸ“ˆ Metrics Collector<br/>Error/Latency/Availability]
    
    Metrics --> SLO[ğŸ¯ SLO Monitor<br/>99.5% Avail, 200ms p95, 0.1% Error]
    Tracing -.Correlate.-> SLO
    
    SLO --> Decision[ğŸ§  Decision Engine<br/>Rollback or Promote]
    Decision --> Promote[âœ… PROMOTE<br/>Canaryâ†’100%]
    Decision --> Rollback[âš ï¸ ROLLBACK<br/>to Baseline]
    
    style Router fill:#00ff88,stroke:#00ff88,color:#000
    style Tracing fill:#ff00ff,stroke:#ff00ff,color:#000
    style SLO fill:#ffff00,stroke:#ffff00,color:#000
    style Promote fill:#00ff88,stroke:#00ff88,color:#000
    style Rollback fill:#ff0000,stroke:#ff0000,color:#fff
```

**Description**: Illustrates canary deployment with progressive traffic routing (5%â†’25%â†’100%), OpenTelemetry distributed tracing for 28 critical flows, and SLO monitoring with automated rollback triggers. Shows integration between tracing, metrics collection, and decision engine.

**References**: 
- [DEPLOYMENT_INFRASTRUCTURE.md](DEPLOYMENT_INFRASTRUCTURE.md) - Complete deployment documentation
- README.md - Deployment Infrastructure section

---

### ğŸ¨ Diagram Design Principles

All diagrams follow these **HYPER MODERN, FUTURISTIC NEO-PUNK** design principles:

**Color Scheme**:
- ğŸŸ£ **Magenta (#ff00ff)**: CLI/Entry points/Critical gates
- ğŸŸ¢ **Cyan (#00ffff)**: Core processing/Evidence registry
- ğŸŸ¡ **Yellow (#ffff00)**: Evaluation/SLO monitoring
- ğŸŸ¢ **Green (#00ff88)**: Success states/Validation passed
- ğŸ”´ **Red (#ff0000)**: Failure states/Rollback actions
- ğŸ”µ **Blue (#00d4ff)**: Orchestration/Components

**Typography**: JetBrains Mono (monospace, technical aesthetic)

**Cardinality Annotations**: All edges labeled with relationship cardinality (1:1, 1:N, N:1)

**Graph Types**:
- **TB (Top-Bottom)**: Sequential flows, pipelines, CI/CD
- **LR (Left-Right)**: Data validation, contract checking

**Background**: Dark theme (#0a0e27) for high contrast and modern feel

---

### ğŸ“¥ Generating High-Resolution Images

To regenerate PNG images from DOT source files:

```bash
cd docs/diagrams
python3 generate_images.py
```

Requirements:
- Graphviz installed: `brew install graphviz` (macOS) or `apt-get install graphviz` (Linux)
- Python 3.7+

Output: 300 DPI PNG files suitable for documentation and presentations.

---

## ğŸ“š DocumentaciÃ³n Adicional

- **Visual Architecture Diagrams:** 7 advanced diagrams (see [Visual Architecture section](#-visual-architecture-diagrams))
- **Flujos CrÃ­ticos Detallados:** `FLUJOS_CRITICOS_GARANTIZADOS.md`
- **Arquitectura Completa:** `ARCHITECTURE.md`
- **Deprecations y MigraciÃ³n:** `DEPRECATIONS.md`
- **Deployment:** `DEPLOYMENT_INFRASTRUCTURE.md`
- **Data Contracts:** `DATA_CONTRACTS.md`
- **Component Diagram:** `COMPONENT_DIAGRAM.md`

---

## ğŸ¤ ContribuciÃ³n

### Reglas para PRs

1. **Nunca** modificar el orden canÃ³nico de flujos sin actualizar `tools/flow_doc.json`
2. **Siempre** ejecutar verificaciÃ³n antes de commit: `python test_validation_end_to_end.py`
3. **Siempre** verificar que rÃºbrica pase: `python miniminimoon_cli.py rubric-check`
4. **Nunca** importar mÃ³dulos deprecados
5. **Siempre** mantener determinismo (seeds fijos, sin randomness)

### CI/CD Pipeline

```yaml
on: [pull_request]
jobs:
  validate:
    - freeze_configuration
    - verify_pre_execution
    - run_evaluation_triple
    - verify_post_execution
    - rubric_check
    - trace_matrix_generation
    - performance_gate (p95 latency < budget + 10%)
```

---

## ğŸ“ Soporte

**DocumentaciÃ³n:** Ver carpeta `docs/`  
**VerificaciÃ³n:** `python miniminimoon_cli.py diagnostic` o `python test_validation_end_to_end.py`  
**CLI Help:** `python miniminimoon_cli.py --help`  
**Issues:** Reportar con logs de `artifacts/` adjuntos

---

## ğŸ“„ Licencia

Ver archivo `LICENSE`

---

## ğŸ¯ Estado del Sistema

**Ãšltima verificaciÃ³n:** 6 de octubre de 2025  
**Estado:** âœ… OPERATIVO  
**Flujos crÃ­ticos:** 72/72 verificados  
**Gates de aceptaciÃ³n:** 6/6 activos  
**Cobertura:** 300/300 preguntas

---

## âš¡ Quick Reference (Comandos Comunes)

### Comandos de InstalaciÃ³n
```bash
# Crear entorno virtual con Python 3.10
python3.10 -m venv venv
source venv/bin/activate  # Linux/macOS
# .\venv\Scripts\Activate.ps1  # Windows PowerShell

# Instalar dependencias
pip install -r requirements.txt

# Descargar modelos NLP
python -m spacy download es_core_news_sm
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

### Comandos de OperaciÃ³n
```bash
# 1. Congelar configuraciÃ³n (obligatorio antes de cualquier evaluaciÃ³n)
python miniminimoon_cli.py freeze

# 2. Evaluar un plan
python miniminimoon_cli.py evaluate --plan mi_plan.pdf --strict

# 3. Verificar resultados
python miniminimoon_cli.py verify

# 4. Validar rÃºbrica
python miniminimoon_cli.py rubric-check output/answers_report.json config/RUBRIC_SCORING.json

# 5. Generar matriz de trazabilidad
python miniminimoon_cli.py trace-matrix
```

### Comandos de VerificaciÃ³n
```bash
# Verificar que todos los mÃ³dulos funcionan
python test_validation_end_to_end.py

# DiagnÃ³stico completo del sistema
python miniminimoon_cli.py diagnostic

# Verificar reproducibilidad (triple-run)
for i in {1..3}; do python miniminimoon_cli.py evaluate --plan test.pdf > run_$i.json; done

# Ver versiÃ³n y estado
python miniminimoon_cli.py version
```

### Comandos de Desarrollo
```bash
# Ejecutar tests unitarios
python -m pytest test_plan_sanitizer.py -v
python -m pytest test_document_segmenter.py -v
python -m pytest test_teoria_cambio.py -v

# Verificar todos los tests
python -m pytest -v

# Ver cobertura de tests
python -m pytest --cov=. --cov-report=html
```
