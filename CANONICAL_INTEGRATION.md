# Canonical Integration Validation

Comprehensive validation system for the MINIMINIMOON pipeline's 11 canonical nodes with smoke tests for 5 critical target components.

## Overview

The canonical integration validation system ensures proper integration of all pipeline components by:

1. **Validating all 11 canonical nodes** using `CanonicalFlowValidator.validate_node_execution()`
2. **Smoke testing 5 target components** for rapid verification
3. **Generating validation reports** with pass/fail status
4. **Capturing baseline performance metrics** for regression detection
5. **CI/CD integration** with automatic PR blocking on failures
6. **Dashboard metrics output** for real-time monitoring

## 11 Canonical Pipeline Nodes

The MINIMINIMOON pipeline consists of 11 canonical nodes that process documents through a structured flow:

| Node # | Node Name | Description | Dependencies |
|--------|-----------|-------------|--------------|
| 1 | `sanitization` | Clean and normalize raw text input | None |
| 2 | `plan_processing` | Extract plan metadata and structure | sanitization |
| 3 | `document_segmentation` | Split document into semantic segments | sanitization |
| 4 | `embedding` | Generate vector embeddings for segments | document_segmentation |
| 5 | `responsibility_detection` | Identify responsible entities (PERSON/ORG) | sanitization |
| 6 | `contradiction_detection` | Find logical contradictions | sanitization |
| 7 | `monetary_detection` | Extract monetary values and budget info | sanitization |
| 8 | `feasibility_scoring` | Score technical/financial/social feasibility | sanitization |
| 9 | `causal_detection` | Detect causal patterns and relationships | sanitization |
| 10 | `teoria_cambio` | Build Theory of Change graph | responsibility_detection, causal_detection, monetary_detection |
| 11 | `dag_validation` | Validate DAG structure and cycles | teoria_cambio |

## 5 Target Components (Highlighted)

Critical components that require smoke testing for rapid verification:

| Component | Module | Critical | Purpose |
|-----------|--------|----------|---------|
| **Decatalogo_principal** | `Decatalogo_principal.py` | ‚úì | Main evaluation system with frontier AI capabilities |
| **dag_validation** | `dag_validation.py` | ‚úì | DAG structure validator with Monte Carlo methods |
| **embedding_model** | `embedding_model.py` | ‚úì | Industrial embedding model with MPNet‚ÜíMiniLM fallback |
| **plan_processor** | `plan_processor.py` | ‚úì | Plan metadata extractor and document mapper |
| **validate_teoria_cambio** | `validate_teoria_cambio.py` | ‚úì | Theory of Change validation script |

## Complete Data Flow Diagram

```mermaid
graph TB
    %% Input
    INPUT[Raw Text Input]
    
    %% Node 1: Sanitization
    INPUT --> N1[1. Sanitization]
    N1 -->|sanitized_text| N2[2. Plan Processing]
    N1 -->|sanitized_text| N3[3. Document Segmentation]
    N1 -->|sanitized_text| N5[5. Responsibility Detection]
    N1 -->|sanitized_text| N6[6. Contradiction Detection]
    N1 -->|sanitized_text| N7[7. Monetary Detection]
    N1 -->|sanitized_text| N8[8. Feasibility Scoring]
    N1 -->|sanitized_text| N9[9. Causal Detection]
    
    %% Node 2: Plan Processing (Target Component)
    N2 -->|metadata| STORE1[(Metadata Store)]
    
    %% Node 3: Document Segmentation
    N3 -->|segments| N4[4. Embedding]
    
    %% Node 4: Embedding (Target Component)
    N4 -->|embeddings| STORE2[(Embedding Store)]
    
    %% Node 5-9: Parallel Detection
    N5 -->|entities| N10[10. Theory of Change]
    N6 -->|contradictions| STORE3[(Analysis Store)]
    N7 -->|monetary_values| N10
    N8 -->|feasibility_scores| STORE3
    N9 -->|causal_patterns| N10
    
    %% Node 10: Theory of Change (Target Component)
    N10 -->|teoria_cambio| N11[11. DAG Validation]
    
    %% Node 11: DAG Validation (Target Component)
    N11 -->|dag_structure| OUTPUT[Final Output]
    
    %% Highlight target components
    style N2 fill:#ff9,stroke:#333,stroke-width:4px
    style N4 fill:#ff9,stroke:#333,stroke-width:4px
    style N10 fill:#ff9,stroke:#333,stroke-width:4px
    style N11 fill:#ff9,stroke:#333,stroke-width:4px
    style N1 fill:#9ff,stroke:#333,stroke-width:2px
    
    %% Add Decatalogo_principal as orchestrator
    DECATALOGO[Decatalogo_principal<br/>Main Orchestrator]
    DECATALOGO -.->|orchestrates| N1
    DECATALOGO -.->|orchestrates| N2
    DECATALOGO -.->|orchestrates| N3
    DECATALOGO -.->|orchestrates| N4
    DECATALOGO -.->|orchestrates| N5
    DECATALOGO -.->|orchestrates| N6
    DECATALOGO -.->|orchestrates| N7
    DECATALOGO -.->|orchestrates| N8
    DECATALOGO -.->|orchestrates| N9
    DECATALOGO -.->|orchestrates| N10
    DECATALOGO -.->|orchestrates| N11
    style DECATALOGO fill:#f96,stroke:#333,stroke-width:4px
```

**Legend:**
- üü® **Yellow boxes**: Target components requiring smoke tests
- üü¶ **Blue box**: Core sanitization node (entry point)
- üü• **Red box**: Main orchestrator (Decatalogo_principal)
- **Solid arrows**: Data flow between nodes
- **Dashed arrows**: Orchestration control

## Input/Output Contracts

### Node 1: Sanitization
**Inputs:**
- `raw_text` (string): Raw document text

**Outputs:**
- `sanitized_text` (string): Cleaned and normalized text

**Validators:**
- `non_empty`: Input text cannot be empty

---

### Node 2: Plan Processing
**Inputs:**
- `sanitized_text` (string): Cleaned document text

**Outputs:**
- `metadata` (dict): Plan metadata (author, date, version, etc.)

**Validators:**
- `text_length`: Sanitized text must be ‚â•100 characters

---

### Node 3: Document Segmentation
**Inputs:**
- `sanitized_text` (string): Cleaned document text

**Outputs:**
- `segments` (list[string]): Document segments

**Validators:**
- `valid_text`: Sanitized text must be string type

---

### Node 4: Embedding
**Inputs:**
- `segments` (list[string]): Document segments

**Outputs:**
- `embeddings` (list[list[float]]): Vector embeddings (384-dim)

**Validators:**
- `segments_list`: Segments must be non-empty list

---

### Node 5: Responsibility Detection
**Inputs:**
- `sanitized_text` (string): Cleaned document text

**Outputs:**
- `entities` (list[dict]): Detected entities with type and confidence
  - `text` (string): Entity text
  - `type` (string): PERSON or ORG
  - `confidence` (float): Detection confidence [0-1]

**Validators:**
- `text_present`: Text must be ‚â•50 characters for entity detection

---

### Node 6: Contradiction Detection
**Inputs:**
- `sanitized_text` (string): Cleaned document text

**Outputs:**
- `contradictions` (list[dict]): Detected contradictions
  - `statement_1` (string): First statement
  - `statement_2` (string): Contradicting statement
  - `type` (string): Contradiction type
  - `confidence` (float): Detection confidence [0-1]

**Validators:** None

---

### Node 7: Monetary Detection
**Inputs:**
- `sanitized_text` (string): Cleaned document text

**Outputs:**
- `monetary_values` (list[dict]): Extracted monetary values
  - `amount` (float): Monetary amount
  - `currency` (string): Currency code (COP, USD, etc.)
  - `context` (string): Context of mention

**Validators:** None

---

### Node 8: Feasibility Scoring
**Inputs:**
- `sanitized_text` (string): Cleaned document text

**Outputs:**
- `feasibility_scores` (dict): Feasibility scores
  - `technical` (float): Technical feasibility [0-1]
  - `financial` (float): Financial feasibility [0-1]
  - `social` (float): Social feasibility [0-1]

**Validators:** None

---

### Node 9: Causal Detection
**Inputs:**
- `sanitized_text` (string): Cleaned document text

**Outputs:**
- `causal_patterns` (list[dict]): Detected causal patterns
  - `cause` (string): Cause element
  - `effect` (string): Effect element
  - `confidence` (float): Pattern confidence [0-1]

**Validators:** None

---

### Node 10: Theory of Change
**Inputs:**
- `sanitized_text` (string): Cleaned document text
- `entities` (list[dict]): Responsible entities
- `causal_patterns` (list[dict]): Causal relationships
- `monetary_values` (list[dict]): Budget information

**Outputs:**
- `teoria_cambio` (dict): Theory of Change graph
  - `nodes` (list[string]): Graph nodes (activities, outcomes)
  - `edges` (list[tuple]): Graph edges (causal links)
  - `metadata` (dict): Additional metadata

**Validators:** None

---

### Node 11: DAG Validation
**Inputs:**
- `teoria_cambio` (dict): Theory of Change graph

**Outputs:**
- `dag_structure` (dict): DAG validation results
  - `is_valid` (bool): Whether DAG is valid
  - `cycles` (list): Detected cycles (if any)
  - `disconnected` (list): Disconnected components
  - `validation_metadata` (dict): Validation details

**Validators:** None

---

## Performance Baselines

Target performance baselines for each node (p95 latency in milliseconds):

| Node | Baseline (ms) | Description |
|------|---------------|-------------|
| `sanitization` | 5.0 | Fast text cleaning |
| `plan_processing` | 10.0 | Metadata extraction |
| `document_segmentation` | 15.0 | Segment splitting |
| `embedding` | 50.0 | Vector generation (batch optimized) |
| `responsibility_detection` | 20.0 | NER with spaCy |
| `contradiction_detection` | 15.0 | Logical analysis |
| `monetary_detection` | 10.0 | Regex-based extraction |
| `feasibility_scoring` | 15.0 | Multi-dimensional scoring |
| `causal_detection` | 20.0 | Pattern matching |
| `teoria_cambio` | 30.0 | Graph construction (cached) |
| `dag_validation` | 25.0 | Monte Carlo validation |

**Total Pipeline**: ~215ms target for full execution

## Usage

### Running Validation

```bash
# Basic validation
python3 validate_canonical_integration.py

# CI mode (exit with error code on failure)
python3 validate_canonical_integration.py --ci

# Custom output paths
python3 validate_canonical_integration.py \
    --output reports/validation.json \
    --dashboard-output reports/dashboard.json \
    --baseline-output reports/baseline.json

# Disable caching for clean run
python3 validate_canonical_integration.py --no-cache
```

### Output Files

1. **Validation Report** (`reports/canonical_integration_validation.json`):
   - Complete validation results
   - Node-by-node status
   - Smoke test results
   - Performance metrics
   - SLO compliance

2. **Dashboard Metrics** (`reports/dashboard_metrics.json`):
   - Real-time component health
   - Node execution metrics
   - SLO compliance status
   - Aggregate statistics

3. **Baseline Metrics** (`reports/baseline_metrics.json`):
   - Performance baselines for each node
   - Used for regression detection

### Validation Report Structure

```json
{
  "timestamp": "2024-01-15T10:30:00",
  "overall_success": true,
  "total_nodes": 11,
  "passed_nodes": 11,
  "failed_nodes": 0,
  "node_results": {
    "sanitization": {
      "node_name": "sanitization",
      "success": true,
      "execution_time_ms": 4.23,
      "input_valid": true,
      "errors": [],
      "cached": false
    }
  },
  "smoke_test_results": {
    "Decatalogo_principal": true,
    "dag_validation": true,
    "embedding_model": true,
    "plan_processor": true,
    "validate_teoria_cambio": true
  },
  "performance_metrics": {
    "total_validation_time_ms": 198.45,
    "average_node_time_ms": 18.04,
    "slowest_node": "embedding",
    "cached_validations": 0
  },
  "slo_compliance": {
    "availability": true,
    "p95_latency": true,
    "error_rate": true
  },
  "ci_exit_code": 0
}
```

### Dashboard Metrics Structure

```json
{
  "timestamp": "2024-01-15T10:30:00",
  "summary": {
    "overall_health": "healthy",
    "node_success_rate": 1.0,
    "component_health_rate": 1.0
  },
  "components": {
    "Decatalogo_principal": {
      "status": "healthy",
      "timestamp": "2024-01-15T10:30:00",
      "critical": true
    }
  },
  "nodes": {
    "sanitization": {
      "success": true,
      "execution_time_ms": 4.23,
      "baseline_ms": 5.0,
      "baseline_deviation_pct": -15.4,
      "cached": false,
      "timestamp": "2024-01-15T10:30:00"
    }
  },
  "slo_compliance": {
    "availability": true,
    "p95_latency": true,
    "error_rate": true
  }
}
```

## CI/CD Integration

### GitHub Actions

```yaml
name: Canonical Integration Validation

on:
  pull_request:
    branches: [ main, develop ]
  push:
    branches: [ main, develop ]

jobs:
  validate-integration:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m venv venv
        source venv/bin/activate
        pip install -r requirements.txt
        python -m spacy download es_core_news_sm
        
    - name: Run canonical integration validation
      run: |
        source venv/bin/activate
        python validate_canonical_integration.py --ci \
          --output reports/validation.json \
          --dashboard-output reports/dashboard.json
          
    - name: Upload validation report
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: validation-report
        path: reports/
        
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const report = JSON.parse(fs.readFileSync('reports/validation.json'));
          const status = report.overall_success ? '‚úÖ PASSED' : '‚ùå FAILED';
          const body = `## Canonical Integration Validation ${status}
          
          **Summary:**
          - Total Nodes: ${report.total_nodes}
          - Passed: ${report.passed_nodes}
          - Failed: ${report.failed_nodes}
          - Average Node Time: ${report.performance_metrics.average_node_time_ms.toFixed(2)}ms
          
          See artifacts for full report.`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          });
```

### TeamCity

```xml
<build-type>
  <name>Canonical Integration Validation</name>
  <description>Validates all 11 canonical nodes and 5 target components</description>
  
  <build-runners>
    <runner id="setup_venv" name="Setup Environment" type="simpleRunner">
      <parameters>
        <param name="script.content"><![CDATA[
#!/bin/bash
set -e
python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
python3 -m spacy download es_core_news_sm
        ]]></param>
      </parameters>
    </runner>
    
    <runner id="validate" name="Run Validation" type="simpleRunner">
      <parameters>
        <param name="script.content"><![CDATA[
#!/bin/bash
set -e
source venv/bin/activate
python3 validate_canonical_integration.py --ci \
  --output reports/validation.json \
  --dashboard-output reports/dashboard.json \
  --baseline-output reports/baseline.json
        ]]></param>
      </parameters>
    </runner>
  </build-runners>
  
  <build-triggers>
    <trigger type="vcs">
      <param name="triggerRules">+:*</param>
    </trigger>
  </build-triggers>
  
  <failure-conditions>
    <failure-condition type="exitCode">
      <param name="exitCode">1</param>
    </failure-condition>
    <failure-condition type="metricChange">
      <param name="metric">custom:node_success_rate</param>
      <param name="threshold">95</param>
      <param name="comparison">less</param>
    </failure-condition>
  </failure-conditions>
  
  <artifact-rules>
    reports/ => validation-reports.zip
  </artifact-rules>
</build-type>
```

### Pre-commit Hook

```bash
#!/bin/bash
# .git/hooks/pre-commit

echo "Running canonical integration validation..."
python3 validate_canonical_integration.py --ci

if [ $? -ne 0 ]; then
    echo "‚ùå Canonical integration validation failed!"
    echo "Fix integration issues before committing."
    exit 1
fi

echo "‚úÖ Canonical integration validation passed!"
exit 0
```

## SLO Tracking

The validation system integrates with the SLO monitoring system to track:

### Availability SLO
- **Target**: 99.5%
- **Measurement**: Percentage of successful node validations
- **Alert**: Triggered when availability drops below threshold

### Latency SLO
- **Target**: P95 latency ‚â§ 200ms
- **Measurement**: 95th percentile of node execution times
- **Alert**: Triggered when p95 exceeds threshold

### Error Rate SLO
- **Target**: ‚â§ 0.1%
- **Measurement**: Percentage of failed validations
- **Alert**: Triggered when error rate exceeds threshold

### Performance Regression SLO
- **Target**: ‚â§ 10% deviation from baseline
- **Measurement**: Comparison against baseline metrics
- **Alert**: Triggered when deviation exceeds threshold

## Troubleshooting

### Common Issues

**Issue: Smoke test failures**
```
‚úó embedding_model import failed: No module named 'sentence_transformers'
```
**Solution**: Install missing dependencies
```bash
pip install -r requirements.txt
```

---

**Issue: Node validation failures**
```
‚úó sanitization validation failed: Missing required input: raw_text
```
**Solution**: Check data_flow_contract.py for required inputs

---

**Issue: Performance regression detected**
```
‚ö† embedding node exceeded baseline by 45%
```
**Solution**: Check for inefficient code or missing optimizations

---

**Issue: SLO breach**
```
‚úó p95_latency SLO BREACH (actual: 250ms, threshold: 200ms)
```
**Solution**: Optimize slow nodes or adjust SLO thresholds

## Integration with Dashboard

The validation script outputs metrics compatible with the real-time dashboard created in previous work:

### Component Health Panel
- Displays health status of 5 target components
- Color-coded indicators (green/yellow/red)
- Timestamp of last validation

### Node Execution Panel
- Execution time for each of 11 nodes
- Baseline comparison with deviation percentage
- Cache hit indicators

### SLO Compliance Panel
- Real-time SLO compliance status
- Visual indicators for availability, latency, error rate
- Historical trend charts

### Performance Metrics Panel
- Total validation time
- Average node execution time
- Slowest node identification
- Cache hit rate

## Performance Optimization

The validation system includes several optimizations:

1. **Validation Caching**: Results cached based on input hash (~60-80% overhead reduction)
2. **Parallel Validation**: Independent nodes validated concurrently
3. **Mock Data Generation**: Lightweight mock data for fast validation
4. **Incremental Validation**: Only validate changed nodes (future enhancement)

## Future Enhancements

- [ ] Incremental validation (only changed nodes)
- [ ] Parallel node validation for independent nodes
- [ ] Historical trend analysis and anomaly detection
- [ ] Automatic performance baseline updates
- [ ] Integration with distributed tracing (OpenTelemetry)
- [ ] Real-time dashboard streaming with WebSocket
- [ ] Canary deployment validation integration
- [ ] Contract-based fuzzing for robustness testing

## References

- [Data Flow Contract Documentation](data_flow_contract.py)
- [SLO Monitoring System](DEPLOYMENT_INFRASTRUCTURE.md)
- [Performance Optimization Guide](PERFORMANCE_OPTIMIZATIONS.md)
- [CI/CD Configuration](TEAMCITY_CI_CONFIGURATION.md)
