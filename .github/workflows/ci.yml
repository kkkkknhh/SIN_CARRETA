name: CI

on:
  push:
    branches: ["main", "master"]
  pull_request:

jobs:
  tests:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Verify Python bytecode compilation
        run: python -m compileall -q .

      - name: Lint with flake8
        run: flake8 .
        continue-on-error: true

      - name: Type check with mypy
        run: mypy . --config-file pyproject.toml
        continue-on-error: true

      - name: Run pytest
        run: pytest -q
  
  performance:
    runs-on: ubuntu-latest
    needs: tests
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download es_core_news_sm

      - name: Run performance suite (100 iterations per component)
        id: perf_test
        run: |
          echo "Running performance benchmark suite..."
          python3 performance_test_suite.py > performance_output.txt 2>&1 || echo "PERF_FAILED=true" >> $GITHUB_ENV
          cat performance_output.txt

      - name: Upload performance report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-report
          path: |
            performance_report.json
            performance_output.txt

      - name: Check performance budgets
        run: |
          if [ "$PERF_FAILED" == "true" ]; then
            echo "‚ùå Performance budgets exceeded - blocking PR"
            echo "Review performance_report.json for details"
            exit 1
          else
            echo "‚úÖ All performance budgets met"
          fi

      - name: Comment PR with performance results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const report = JSON.parse(fs.readFileSync('performance_report.json', 'utf8'));
              const summary = report.summary;
              const results = report.results;
              
              let comment = '## üöÄ Performance Test Results\n\n';
              comment += `- **Total Components**: ${summary.total_components}\n`;
              comment += `- **Passed**: ${summary.passed} ‚úÖ\n`;
              comment += `- **Failed**: ${summary.failed} ‚ùå\n\n`;
              
              comment += '### Component Performance\n\n';
              comment += '| Component | p95 Latency | Budget | Status |\n';
              comment += '|-----------|-------------|--------|--------|\n';
              
              for (const [name, result] of Object.entries(results)) {
                const status = result.budget_passed ? '‚úÖ' : '‚ùå';
                comment += `| ${name} | ${result.p95_ms.toFixed(2)}ms | ${result.budget_message} | ${status} |\n`;
              }
              
              if (summary.failed > 0) {
                comment += '\n‚ö†Ô∏è **Performance budgets exceeded - PR blocked**\n';
              }
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Could not read performance report:', error);
            }
  
  soak_test:
    runs-on: ubuntu-latest
    needs: performance
    if: github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'run-soak-test')
    timeout-minutes: 300
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download es_core_news_sm

      - name: Run 4-hour soak test
        id: soak_test
        run: |
          echo "Starting 4-hour soak test for memory leak detection..."
          python3 performance_test_suite.py --soak > soak_test_output.txt 2>&1 || echo "SOAK_FAILED=true" >> $GITHUB_ENV
          cat soak_test_output.txt

      - name: Upload soak test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: soak-test-results
          path: |
            performance_report.json
            soak_test_output.txt

      - name: Check for memory leaks
        run: |
          if [ "$SOAK_FAILED" == "true" ]; then
            echo "‚ùå Memory leak detected in 4-hour soak test - blocking PR"
            exit 1
          else
            echo "‚úÖ No memory leaks detected"
          fi

      - name: Comment PR with soak test results
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const report = JSON.parse(fs.readFileSync('performance_report.json', 'utf8'));
              const soak = report.soak_test;
              
              if (soak) {
                let comment = '## üî¨ 4-Hour Soak Test Results\n\n';
                comment += `- **Duration**: ${soak.duration_hours} hours\n`;
                comment += `- **Iterations**: ${soak.iterations}\n`;
                comment += `- **Memory Growth**: ${soak.memory_growth_mb_per_hour.toFixed(2)} MB/hour\n`;
                comment += `- **Initial Memory**: ${soak.initial_memory_mb.toFixed(2)} MB\n`;
                comment += `- **Final Memory**: ${soak.final_memory_mb.toFixed(2)} MB\n\n`;
                
                if (soak.leak_detected) {
                  comment += '‚ùå **Memory leak detected - PR blocked**\n';
                } else {
                  comment += '‚úÖ **No memory leaks detected**\n';
                }
                
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: comment
                });
              }
            } catch (error) {
              console.log('Could not read soak test report:', error);
            }

  batch_load_test:
    runs-on: ubuntu-latest
    needs: performance
    services:
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download es_core_news_sm

      - name: Initialize batch metrics directory
        run: |
          mkdir -p batch_metrics
          echo "Batch metrics directory created for artifact archival"

      - name: Start mock API server (background)
        run: |
          echo "Mock API server initialization (no actual server needed for mock tests)"
          echo "API_READY=true" >> $GITHUB_ENV

      - name: Initialize mock Celery worker (background)
        run: |
          echo "Mock Celery worker initialization (no actual worker needed for mock tests)"
          echo "WORKER_READY=true" >> $GITHUB_ENV

      - name: Wait for services to be ready
        run: |
          echo "Checking Redis availability..."
          timeout 30 bash -c 'until redis-cli -h localhost ping; do sleep 1; done'
          echo "‚úÖ Redis is ready"
          echo "‚úÖ API server ready: $API_READY"
          echo "‚úÖ Celery worker ready: $WORKER_READY"

      - name: Run batch load test (10 concurrent documents)
        id: batch_load
        run: |
          echo "Running batch load test with 10 concurrent documents..."
          echo "Target: >= 170 documents/hour (max 21.2 seconds per document)"
          pytest test_batch_load.py -v --tb=short || echo "BATCH_LOAD_FAILED=true" >> $GITHUB_ENV

      - name: Copy batch load metrics to batch_metrics directory
        if: always()
        run: |
          cp -f processing_times.json batch_metrics/ 2>/dev/null || echo "No processing_times.json"
          cp -f throughput_report.json batch_metrics/ 2>/dev/null || echo "No throughput_report.json"
          cp -f latency_distribution.json batch_metrics/ 2>/dev/null || echo "No latency_distribution.json"
          cp -f queue_depth.json batch_metrics/ 2>/dev/null || echo "No queue_depth.json"

      - name: Check batch load performance
        run: |
          if [ "$BATCH_LOAD_FAILED" == "true" ]; then
            echo "‚ùå Batch load test failed - throughput below 170 documents/hour"
            exit 1
          else
            echo "‚úÖ Batch load test passed"
            if [ -f throughput_report.json ]; then
              cat throughput_report.json
            fi
          fi

      - name: Run stress test (50 concurrent uploads)
        id: stress_test
        run: |
          echo "Running stress test with 50 concurrent uploads..."
          echo "Monitoring worker memory usage for leak detection"
          echo "Threshold: Memory growth <= 20% of baseline"
          pytest test_stress_test.py -v --tb=short || echo "STRESS_TEST_FAILED=true" >> $GITHUB_ENV

      - name: Copy stress test metrics to batch_metrics directory
        if: always()
        run: |
          cp -f memory_profile.json batch_metrics/ 2>/dev/null || echo "No memory_profile.json"
          cp -f worker_resource_utilization.json batch_metrics/ 2>/dev/null || echo "No worker_resource_utilization.json"

      - name: Check stress test results
        run: |
          if [ "$STRESS_TEST_FAILED" == "true" ]; then
            echo "‚ùå Stress test failed - memory growth exceeds 20%"
            exit 1
          else
            echo "‚úÖ Stress test passed - no memory leaks detected"
            if [ -f memory_profile.json ]; then
              cat memory_profile.json
            fi
          fi

      - name: Archive batch processing metrics
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: batch-metrics
          path: |
            batch_metrics/

      - name: Comment PR with combined test results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let comment = '## üî• Batch Load & Stress Test Results\n\n';
            
            // Batch Load Test Results
            try {
              const throughput = JSON.parse(fs.readFileSync('throughput_report.json', 'utf8'));
              const metrics = throughput.metrics;
              
              comment += '### ‚ö° Batch Load Test (10 concurrent)\n\n';
              comment += `- **Throughput**: ${metrics.throughput.toFixed(2)} docs/hour\n`;
              comment += `- **Threshold**: ${metrics.threshold} docs/hour\n`;
              comment += `- **Status**: ${metrics.passed ? '‚úÖ PASSED' : '‚ùå FAILED'}\n\n`;
              
              comment += '#### Performance Summary\n';
              comment += `- **Avg Time/Doc**: ${throughput.performance_summary.avg_ms_per_doc.toFixed(2)}ms\n`;
              comment += `- **Total Time**: ${throughput.performance_summary.total_time_seconds.toFixed(2)}s\n`;
              comment += `- **Documents**: ${throughput.performance_summary.documents_processed}\n\n`;
              
              if (!metrics.passed) {
                comment += '‚ö†Ô∏è **Throughput below threshold - performance regression detected**\n\n';
              }
            } catch (error) {
              comment += '### ‚ö° Batch Load Test\n\n‚ùå Could not read throughput report\n\n';
            }
            
            // Stress Test Results
            try {
              const profile = JSON.parse(fs.readFileSync('memory_profile.json', 'utf8'));
              const stats = profile.memory_stats;
              
              comment += '### üí™ Stress Test (50 concurrent)\n\n';
              comment += `- **Initial Memory**: ${stats.initial_memory_mb.toFixed(2)} MB\n`;
              comment += `- **Final Memory**: ${stats.final_memory_mb.toFixed(2)} MB\n`;
              comment += `- **Memory Growth**: ${stats.memory_growth_mb.toFixed(2)} MB (${stats.memory_growth_percent.toFixed(2)}%)\n`;
              comment += `- **Threshold**: ${stats.threshold_percent}%\n`;
              comment += `- **Status**: ${stats.memory_growth_percent <= stats.threshold_percent ? '‚úÖ PASSED' : '‚ùå FAILED'}\n\n`;
              
              comment += '#### Memory Profile Over Time\n';
              comment += '| Batch | Documents | Memory (MB) |\n';
              comment += '|-------|-----------|-------------|\n';
              
              for (const sample of profile.memory_samples_over_time) {
                comment += `| ${sample.batch} | ${sample.documents_processed} | ${sample.memory_mb.toFixed(2)} |\n`;
              }
              
              if (stats.memory_growth_percent > stats.threshold_percent) {
                comment += '\n‚ö†Ô∏è **Memory leak detected - memory growth exceeds threshold**\n';
              }
            } catch (error) {
              comment += '### üí™ Stress Test\n\n‚ùå Could not read memory profile\n';
            }
            
            comment += '\nüì¶ **Batch metrics archived as CI artifacts in `batch_metrics/` directory**\n';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          

  rubric-validation:
    runs-on: ubuntu-latest
    needs: tests
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create artifacts directory
        run: mkdir -p artifacts

      - name: Create mock input files for testing
        run: |
          echo '{"answers": [{"question_id": "D1-Q1"}, {"question_id": "D1-Q2"}]}' > artifacts/answers_report.json
          echo '{"weights": {"D1-Q1": 0.00333, "D1-Q2": 0.00333}}' > mock_rubric.json

      - name: Run rubric validation
        id: rubric_check
        run: |
          python3 tools/rubric_check.py --answers artifacts/answers_report.json --rubric mock_rubric.json
          EXIT_CODE=$?
          echo "exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT
          if [ $EXIT_CODE -eq 2 ]; then
            echo "‚ùå Rubric validation failed: Missing input files"
            exit 1
          elif [ $EXIT_CODE -eq 3 ]; then
            echo "‚ùå Rubric validation failed: Question-weight mismatch detected"
            exit 1
          elif [ $EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Rubric validation passed: All weights match"
            exit 0
          else
            echo "‚ùå Rubric validation failed with unexpected exit code: $EXIT_CODE"
            exit 1
          fi

      - name: Upload rubric validation report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: rubric-validation-report
          path: |
            artifacts/answers_report.json
            rubric_scoring.json

  deterministic-pipeline-validation:
    runs-on: ubuntu-latest
    needs: rubric-validation
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download es_core_news_sm

      - name: Create validation directories
        run: |
          mkdir -p artifacts
          mkdir -p validation_run_1
          mkdir -p validation_run_2
          mkdir -p validation_run_3

      - name: Execute freeze verification
        run: |
          echo "üîí Freezing configuration snapshot..."
          python3 miniminimoon_cli.py freeze --repo .
          if [ $? -ne 0 ]; then
            echo "‚ùå Freeze verification failed"
            exit 1
          fi
          echo "‚úÖ Freeze verification passed"

      - name: Run pre-execution validation
        run: |
          echo "üîç Running pre-execution contract and wiring validation..."
          python3 -c "
          from system_validators import SystemValidator
          validator = SystemValidator()
          result = validator.validate_pre_execution()
          import json
          print(json.dumps(result, indent=2))
          if not result.get('valid', False):
              print('‚ùå Pre-execution validation failed')
              exit(1)
          print('‚úÖ Pre-execution validation passed')
          "
          if [ $? -ne 0 ]; then
            echo "‚ùå Pre-execution validation failed"
            exit 1
          fi

      - name: Execute evaluation pipeline - Run 1
        run: |
          echo "üöÄ Executing evaluation pipeline - Run 1 of 3..."
          python3 miniminimoon_cli.py evaluate --repo . --output validation_run_1
          if [ $? -ne 0 ]; then
            echo "‚ùå Evaluation pipeline Run 1 failed"
            exit 1
          fi
          if [ ! -f validation_run_1/flow_runtime.json ]; then
            echo "‚ùå flow_runtime.json not generated in Run 1"
            exit 1
          fi
          echo "‚úÖ Evaluation pipeline Run 1 completed"

      - name: Execute evaluation pipeline - Run 2
        run: |
          echo "üöÄ Executing evaluation pipeline - Run 2 of 3..."
          python3 miniminimoon_cli.py evaluate --repo . --output validation_run_2
          if [ $? -ne 0 ]; then
            echo "‚ùå Evaluation pipeline Run 2 failed"
            exit 1
          fi
          if [ ! -f validation_run_2/flow_runtime.json ]; then
            echo "‚ùå flow_runtime.json not generated in Run 2"
            exit 1
          fi
          echo "‚úÖ Evaluation pipeline Run 2 completed"

      - name: Execute evaluation pipeline - Run 3
        run: |
          echo "üöÄ Executing evaluation pipeline - Run 3 of 3..."
          python3 miniminimoon_cli.py evaluate --repo . --output validation_run_3
          if [ $? -ne 0 ]; then
            echo "‚ùå Evaluation pipeline Run 3 failed"
            exit 1
          fi
          if [ ! -f validation_run_3/flow_runtime.json ]; then
            echo "‚ùå flow_runtime.json not generated in Run 3"
            exit 1
          fi
          echo "‚úÖ Evaluation pipeline Run 3 completed"

      - name: Compare flow_runtime.json outputs for determinism
        run: |
          echo "üî¨ Comparing flow_runtime.json outputs for deterministic ordering and evidence hashes..."
          python3 -c "
          import json
          import sys
          
          def load_flow_runtime(path):
              with open(path, 'r') as f:
                  return json.load(f)
          
          run1 = load_flow_runtime('validation_run_1/flow_runtime.json')
          run2 = load_flow_runtime('validation_run_2/flow_runtime.json')
          run3 = load_flow_runtime('validation_run_3/flow_runtime.json')
          
          # Compare ordering
          def get_execution_order(data):
              if 'flows' in data:
                  return [f.get('flow_id', f.get('name', '')) for f in data['flows']]
              return []
          
          order1 = get_execution_order(run1)
          order2 = get_execution_order(run2)
          order3 = get_execution_order(run3)
          
          if order1 != order2 or order1 != order3:
              print('‚ùå Flow ordering not deterministic across runs')
              print(f'Run 1: {order1}')
              print(f'Run 2: {order2}')
              print(f'Run 3: {order3}')
              sys.exit(1)
          
          # Compare evidence hashes if available
          def get_evidence_hashes(data):
              hashes = []
              if 'flows' in data:
                  for flow in data['flows']:
                      if 'evidence_hash' in flow:
                          hashes.append(flow['evidence_hash'])
              return hashes
          
          hashes1 = get_evidence_hashes(run1)
          hashes2 = get_evidence_hashes(run2)
          hashes3 = get_evidence_hashes(run3)
          
          if hashes1 and (hashes1 != hashes2 or hashes1 != hashes3):
              print('‚ùå Evidence hashes not deterministic across runs')
              sys.exit(1)
          
          print('‚úÖ Flow ordering and evidence hashes are deterministic across all runs')
          "
          if [ $? -ne 0 ]; then
            echo "‚ùå Determinism check failed"
            exit 1
          fi

      - name: Run trace matrix generation and validation
        run: |
          echo "üîç Generating provenance traceability matrix..."
          cd validation_run_1
          python3 ../tools/trace_matrix.py
          EXIT_CODE=$?
          if [ $EXIT_CODE -eq 2 ]; then
            echo "‚ùå Trace matrix generation failed: Missing input files (answers_report.json)"
            exit 1
          elif [ $EXIT_CODE -eq 3 ]; then
            echo "‚ùå Trace matrix generation failed: Malformed data in answers_report.json"
            exit 1
          elif [ $EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Trace matrix generated successfully"
            if [ -f ../artifacts/module_to_questions_matrix.csv ]; then
              echo "‚úÖ Output artifact verified: artifacts/module_to_questions_matrix.csv"
            else
              echo "‚ùå Expected output artifact not found: artifacts/module_to_questions_matrix.csv"
              exit 1
            fi
          else
            echo "‚ùå Trace matrix generation failed with unexpected exit code: $EXIT_CODE"
            exit 1
          fi
          cd ..

      - name: Archive trace matrix artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: trace-matrix-validation
          path: |
            artifacts/module_to_questions_matrix.csv

      - name: Run post-execution validation
        run: |
          echo "üîç Running post-execution validation (coverage 300/300)..."
          python3 -c "
          from system_validators import SystemHealthValidator
          import json
          validator = SystemHealthValidator()
          result = validator.validate_post_execution(artifacts_dir='validation_run_1', check_rubric_strict=True)
          print(json.dumps(result, indent=2))
          if not result.get('ok', False):
              print('‚ùå Post-execution validation failed')
              for error in result.get('errors', []):
                  print(f'  - {error}')
              exit(1)
          
          # Check for 300/300 coverage
          if not result.get('ok_coverage', False):
              print('‚ùå Coverage validation failed: expected ‚â•300 questions')
              exit(1)
          
          # Check trace matrix validation
          if not result.get('ok_trace_matrix', True):
              print('‚ùå Trace matrix validation failed')
              exit(1)
          
          print('‚úÖ Post-execution validation passed with 300/300 coverage')
          "
          if [ $? -ne 0 ]; then
            echo "‚ùå Post-execution validation failed"
            exit 1
          fi

      - name: Run rubric alignment check
        run: |
          echo "üìã Running rubric check for 1:1 question-to-rubric alignment..."
          python3 tools/rubric_check.py --answers validation_run_1/answers_report.json --rubric rubric_scoring.json --output-dir artifacts
          EXIT_CODE=$?
          if [ $EXIT_CODE -eq 3 ]; then
            echo "‚ùå Rubric check failed: Question-weight mismatch detected (exit code 3)"
            exit 1
          elif [ $EXIT_CODE -eq 2 ]; then
            echo "‚ùå Rubric check failed: Missing input files (exit code 2)"
            exit 1
          elif [ $EXIT_CODE -ne 0 ]; then
            echo "‚ùå Rubric check failed with exit code: $EXIT_CODE"
            exit 1
          fi
          echo "‚úÖ Rubric alignment check passed (exit code 0)"

      - name: Archive validation artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: deterministic-validation-artifacts
          path: |
            validation_run_1/flow_runtime.json
            validation_run_2/flow_runtime.json
            validation_run_3/flow_runtime.json
            validation_run_1/coverage_report.json
            validation_run_1/answers_report.json
            artifacts/rubric_weight_diff.txt
            artifacts/validation_*.json
            .immutability_snapshot.json
