name: CI

on:
  push:
    branches: ["main", "master"]
  pull_request:

jobs:
  tests:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Verify Python bytecode compilation
        run: python -m compileall -q .

      - name: Lint with flake8
        run: flake8 .

      - name: Run pytest
        run: pytest -q
  
  performance:
    runs-on: ubuntu-latest
    needs: tests
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download es_core_news_sm

      - name: Run performance suite (100 iterations per component)
        id: perf_test
        run: |
          echo "Running performance benchmark suite..."
          python3 performance_test_suite.py > performance_output.txt 2>&1 || echo "PERF_FAILED=true" >> $GITHUB_ENV
          cat performance_output.txt

      - name: Upload performance report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-report
          path: |
            performance_report.json
            performance_output.txt

      - name: Check performance budgets
        run: |
          if [ "$PERF_FAILED" == "true" ]; then
            echo "‚ùå Performance budgets exceeded - blocking PR"
            echo "Review performance_report.json for details"
            exit 1
          else
            echo "‚úÖ All performance budgets met"
          fi

      - name: Comment PR with performance results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const report = JSON.parse(fs.readFileSync('performance_report.json', 'utf8'));
              const summary = report.summary;
              const results = report.results;
              
              let comment = '## üöÄ Performance Test Results\n\n';
              comment += `- **Total Components**: ${summary.total_components}\n`;
              comment += `- **Passed**: ${summary.passed} ‚úÖ\n`;
              comment += `- **Failed**: ${summary.failed} ‚ùå\n\n`;
              
              comment += '### Component Performance\n\n';
              comment += '| Component | p95 Latency | Budget | Status |\n';
              comment += '|-----------|-------------|--------|--------|\n';
              
              for (const [name, result] of Object.entries(results)) {
                const status = result.budget_passed ? '‚úÖ' : '‚ùå';
                comment += `| ${name} | ${result.p95_ms.toFixed(2)}ms | ${result.budget_message} | ${status} |\n`;
              }
              
              if (summary.failed > 0) {
                comment += '\n‚ö†Ô∏è **Performance budgets exceeded - PR blocked**\n';
              }
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Could not read performance report:', error);
            }
  
  soak_test:
    runs-on: ubuntu-latest
    needs: performance
    if: github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'run-soak-test')
    timeout-minutes: 300
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download es_core_news_sm

      - name: Run 4-hour soak test
        id: soak_test
        run: |
          echo "Starting 4-hour soak test for memory leak detection..."
          python3 performance_test_suite.py --soak > soak_test_output.txt 2>&1 || echo "SOAK_FAILED=true" >> $GITHUB_ENV
          cat soak_test_output.txt

      - name: Upload soak test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: soak-test-results
          path: |
            performance_report.json
            soak_test_output.txt

      - name: Check for memory leaks
        run: |
          if [ "$SOAK_FAILED" == "true" ]; then
            echo "‚ùå Memory leak detected in 4-hour soak test - blocking PR"
            exit 1
          else
            echo "‚úÖ No memory leaks detected"
          fi

      - name: Comment PR with soak test results
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            try {
              const report = JSON.parse(fs.readFileSync('performance_report.json', 'utf8'));
              const soak = report.soak_test;
              
              if (soak) {
                let comment = '## üî¨ 4-Hour Soak Test Results\n\n';
                comment += `- **Duration**: ${soak.duration_hours} hours\n`;
                comment += `- **Iterations**: ${soak.iterations}\n`;
                comment += `- **Memory Growth**: ${soak.memory_growth_mb_per_hour.toFixed(2)} MB/hour\n`;
                comment += `- **Initial Memory**: ${soak.initial_memory_mb.toFixed(2)} MB\n`;
                comment += `- **Final Memory**: ${soak.final_memory_mb.toFixed(2)} MB\n\n`;
                
                if (soak.leak_detected) {
                  comment += '‚ùå **Memory leak detected - PR blocked**\n';
                } else {
                  comment += '‚úÖ **No memory leaks detected**\n';
                }
                
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: comment
                });
              }
            } catch (error) {
              console.log('Could not read soak test report:', error);
            }

  batch_load_test:
    runs-on: ubuntu-latest
    needs: performance
    services:
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download es_core_news_sm

      - name: Initialize batch metrics directory
        run: |
          mkdir -p batch_metrics
          echo "Batch metrics directory created for artifact archival"

      - name: Start mock API server (background)
        run: |
          echo "Mock API server initialization (no actual server needed for mock tests)"
          echo "API_READY=true" >> $GITHUB_ENV

      - name: Initialize mock Celery worker (background)
        run: |
          echo "Mock Celery worker initialization (no actual worker needed for mock tests)"
          echo "WORKER_READY=true" >> $GITHUB_ENV

      - name: Wait for services to be ready
        run: |
          echo "Checking Redis availability..."
          timeout 30 bash -c 'until redis-cli -h localhost ping; do sleep 1; done'
          echo "‚úÖ Redis is ready"
          echo "‚úÖ API server ready: $API_READY"
          echo "‚úÖ Celery worker ready: $WORKER_READY"

      - name: Run batch load test (10 concurrent documents)
        id: batch_load
        run: |
          echo "Running batch load test with 10 concurrent documents..."
          echo "Target: >= 170 documents/hour (max 21.2 seconds per document)"
          pytest test_batch_load.py -v --tb=short || echo "BATCH_LOAD_FAILED=true" >> $GITHUB_ENV

      - name: Copy batch load metrics to batch_metrics directory
        if: always()
        run: |
          cp -f processing_times.json batch_metrics/ 2>/dev/null || echo "No processing_times.json"
          cp -f throughput_report.json batch_metrics/ 2>/dev/null || echo "No throughput_report.json"
          cp -f latency_distribution.json batch_metrics/ 2>/dev/null || echo "No latency_distribution.json"
          cp -f queue_depth.json batch_metrics/ 2>/dev/null || echo "No queue_depth.json"

      - name: Check batch load performance
        run: |
          if [ "$BATCH_LOAD_FAILED" == "true" ]; then
            echo "‚ùå Batch load test failed - throughput below 170 documents/hour"
            exit 1
          else
            echo "‚úÖ Batch load test passed"
            if [ -f throughput_report.json ]; then
              cat throughput_report.json
            fi
          fi

      - name: Run stress test (50 concurrent uploads)
        id: stress_test
        run: |
          echo "Running stress test with 50 concurrent uploads..."
          echo "Monitoring worker memory usage for leak detection"
          echo "Threshold: Memory growth <= 20% of baseline"
          pytest test_stress_test.py -v --tb=short || echo "STRESS_TEST_FAILED=true" >> $GITHUB_ENV

      - name: Copy stress test metrics to batch_metrics directory
        if: always()
        run: |
          cp -f memory_profile.json batch_metrics/ 2>/dev/null || echo "No memory_profile.json"
          cp -f worker_resource_utilization.json batch_metrics/ 2>/dev/null || echo "No worker_resource_utilization.json"

      - name: Check stress test results
        run: |
          if [ "$STRESS_TEST_FAILED" == "true" ]; then
            echo "‚ùå Stress test failed - memory growth exceeds 20%"
            exit 1
          else
            echo "‚úÖ Stress test passed - no memory leaks detected"
            if [ -f memory_profile.json ]; then
              cat memory_profile.json
            fi
          fi

      - name: Archive batch processing metrics
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: batch-metrics
          path: |
            batch_metrics/

      - name: Comment PR with combined test results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let comment = '## üî• Batch Load & Stress Test Results\n\n';
            
            // Batch Load Test Results
            try {
              const throughput = JSON.parse(fs.readFileSync('throughput_report.json', 'utf8'));
              const metrics = throughput.metrics;
              
              comment += '### ‚ö° Batch Load Test (10 concurrent)\n\n';
              comment += `- **Throughput**: ${metrics.throughput.toFixed(2)} docs/hour\n`;
              comment += `- **Threshold**: ${metrics.threshold} docs/hour\n`;
              comment += `- **Status**: ${metrics.passed ? '‚úÖ PASSED' : '‚ùå FAILED'}\n\n`;
              
              comment += '#### Performance Summary\n';
              comment += `- **Avg Time/Doc**: ${throughput.performance_summary.avg_ms_per_doc.toFixed(2)}ms\n`;
              comment += `- **Total Time**: ${throughput.performance_summary.total_time_seconds.toFixed(2)}s\n`;
              comment += `- **Documents**: ${throughput.performance_summary.documents_processed}\n\n`;
              
              if (!metrics.passed) {
                comment += '‚ö†Ô∏è **Throughput below threshold - performance regression detected**\n\n';
              }
            } catch (error) {
              comment += '### ‚ö° Batch Load Test\n\n‚ùå Could not read throughput report\n\n';
            }
            
            // Stress Test Results
            try {
              const profile = JSON.parse(fs.readFileSync('memory_profile.json', 'utf8'));
              const stats = profile.memory_stats;
              
              comment += '### üí™ Stress Test (50 concurrent)\n\n';
              comment += `- **Initial Memory**: ${stats.initial_memory_mb.toFixed(2)} MB\n`;
              comment += `- **Final Memory**: ${stats.final_memory_mb.toFixed(2)} MB\n`;
              comment += `- **Memory Growth**: ${stats.memory_growth_mb.toFixed(2)} MB (${stats.memory_growth_percent.toFixed(2)}%)\n`;
              comment += `- **Threshold**: ${stats.threshold_percent}%\n`;
              comment += `- **Status**: ${stats.memory_growth_percent <= stats.threshold_percent ? '‚úÖ PASSED' : '‚ùå FAILED'}\n\n`;
              
              comment += '#### Memory Profile Over Time\n';
              comment += '| Batch | Documents | Memory (MB) |\n';
              comment += '|-------|-----------|-------------|\n';
              
              for (const sample of profile.memory_samples_over_time) {
                comment += `| ${sample.batch} | ${sample.documents_processed} | ${sample.memory_mb.toFixed(2)} |\n`;
              }
              
              if (stats.memory_growth_percent > stats.threshold_percent) {
                comment += '\n‚ö†Ô∏è **Memory leak detected - memory growth exceeds threshold**\n';
              }
            } catch (error) {
              comment += '### üí™ Stress Test\n\n‚ùå Could not read memory profile\n';
            }
            
            comment += '\nüì¶ **Batch metrics archived as CI artifacts in `batch_metrics/` directory**\n';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          

  rubric-validation:
    runs-on: ubuntu-latest
    needs: tests
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Create artifacts directory
        run: mkdir -p artifacts

      - name: Create mock input files for testing
        run: |
          echo '{"answers": [{"question_id": "q1"}, {"question_id": "q2"}]}' > artifacts/answers_report.json
          echo '{"weights": {"q1": 0.5, "q2": 0.5}}' > rubric_scoring.json

      - name: Run rubric validation
        id: rubric_check
        run: |
          python3 rubric_check.py --answers artifacts/answers_report.json --rubric rubric_scoring.json --output-dir artifacts
          EXIT_CODE=$?
          echo "exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT
          if [ $EXIT_CODE -eq 2 ]; then
            echo "‚ùå Rubric validation failed: Missing input files"
            exit 1
          elif [ $EXIT_CODE -eq 3 ]; then
            echo "‚ùå Rubric validation failed: Question-weight mismatch detected"
            exit 1
          elif [ $EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Rubric validation passed: All weights match"
            exit 0
          else
            echo "‚ùå Rubric validation failed with unexpected exit code: $EXIT_CODE"
            exit 1
          fi

      - name: Upload rubric validation report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: rubric-validation-report
          path: |
            artifacts/rubric_weight_diff.txt
            artifacts/answers_report.json
            rubric_scoring.json
