#!/usr/bin/env python3
"""
AUTHORITATIVE QUESTIONNAIRE ENGINE v2.0 - COMPLETE IMPLEMENTATION
"""

import json
import logging
import re
from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union
from enum import Enum
import uuid
from datetime import datetime
import numpy as np

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================================
# ENUMERATIONS AND CONSTANTS
# ============================================================================

class ScoringModality(Enum):
    """Scoring calculation methods"""
    TYPE_A = "count_4_elements"  # (found/4) Ã— 3
    TYPE_B = "count_3_elements"  # min(found, 3)
    TYPE_C = "count_2_elements"  # (found/2) Ã— 3
    TYPE_D = "ratio_quantitative"  # f(ratio) with thresholds
    TYPE_E = "logical_rule"  # if-then-else logic
    TYPE_F = "semantic_analysis"  # cosine similarity


class ScoreBand(Enum):
    """Score interpretation bands"""
    EXCELENTE = (85, 100, "ðŸŸ¢", "DiseÃ±o causal robusto")
    BUENO = (70, 84, "ðŸŸ¡", "DiseÃ±o sÃ³lido con vacÃ­os menores")
    SATISFACTORIO = (55, 69, "ðŸŸ ", "Cumple mÃ­nimos, requiere mejoras")
    INSUFICIENTE = (40, 54, "ðŸ”´", "VacÃ­os crÃ­ticos")
    DEFICIENTE = (0, 39, "âš«", "Ausencia de diseÃ±o causal")

    @property
    def min_score(self):
        return self.value[0]

    @property
    def max_score(self):
        return self.value[1]

    @property
    def color(self):
        return self.value[2]

    @property
    def description(self):
        return self.value[3]

    @classmethod
    def classify(cls, score_percentage: float) -> 'ScoreBand':
        """Classify score into band"""
        for band in cls:
            if band.min_score <= score_percentage <= band.max_score:
                return band
        return cls.DEFICIENTE


# ============================================================================
# DATA STRUCTURES
# ============================================================================

@dataclass
class QuestionnaireStructure:
    """IMMUTABLE: Enforces the 30Ã—10 structure"""
    TOTAL_QUESTIONS: int = 300
    DOMAINS: int = 10  # P1-P10
    QUESTIONS_PER_DOMAIN: int = 30  # 6 dimensions Ã— 5 questions each
    DIMENSIONS: int = 6  # D1-D6
    QUESTIONS_PER_DIMENSION: int = 5
    VERSION: str = "2.0"
    DECIMAL_PRECISION_QUESTION: int = 2
    DECIMAL_PRECISION_DIMENSION: int = 1

    def validate_structure(self) -> bool:
        """Validates that 10 Ã— 30 = 300 questions"""
        return (self.DOMAINS * self.QUESTIONS_PER_DOMAIN == self.TOTAL_QUESTIONS and
                self.DIMENSIONS * self.QUESTIONS_PER_DIMENSION == self.QUESTIONS_PER_DOMAIN)


@dataclass
class ThematicPoint:
    """Represents one of the 10 thematic points (P1-P10)"""
    id: str  # P1, P2, ..., P10
    title: str
    keywords: List[str] = field(default_factory=list)
    hints: List[str] = field(default_factory=list)
    relevant_programs: List[str] = field(default_factory=list)
    pdm_sections: List[str] = field(default_factory=list)


@dataclass
class SearchPattern:
    """Search pattern for evidence detection"""
    pattern_type: str  # "regex", "table", "semantic", "logical"
    pattern: Union[str, Dict]
    description: str


@dataclass
class ScoringRule:
    """Scoring rule specification"""
    modality: ScoringModality
    formula: str
    thresholds: Optional[Dict[str, float]] = None


@dataclass
class BaseQuestion:
    """Base question that gets parametrized for each thematic point"""
    id: str  # D1-Q1, D1-Q2, etc.
    dimension: str  # D1, D2, D3, D4, D5, D6
    question_no: int  # 1-30
    template: str  # Question template with {PUNTO_TEMATICO} placeholder
    search_patterns: Dict[str, SearchPattern]
    scoring_rule: ScoringRule
    max_score: float = 3.0
    expected_elements: List[str] = field(default_factory=list)


@dataclass
class EvaluationResult:
    """Result of evaluating one question for one thematic point"""
    question_id: str  # P1-D1-Q1, P2-D1-Q1, etc.
    point_code: str  # P1, P2, etc.
    point_title: str
    dimension: str
    question_no: int
    prompt: str  # Fully parametrized question
    score: float
    max_score: float
    elements_found: Dict[str, bool]
    elements_expected: int
    elements_found_count: int
    evidence: List[Dict[str, Any]]
    missing_elements: List[str]
    recommendation: str
    scoring_modality: str
    calculation_detail: str


@dataclass
class DimensionScore:
    """Aggregated score for one dimension"""
    dimension_id: str
    dimension_name: str
    score_percentage: float
    points_obtained: float
    points_maximum: float
    questions: List[EvaluationResult]


@dataclass
class PointScore:
    """Aggregated score for one thematic point"""
    point_id: str
    point_title: str
    score_percentage: float
    dimension_scores: Dict[str, DimensionScore]
    total_questions: int
    classification: ScoreBand


@dataclass
class GlobalScore:
    """Complete evaluation summary"""
    score_percentage: float
    points_evaluated: int
    points_not_applicable: List[str]
    dimension_averages: Dict[str, float]
    classification: ScoreBand
    validation_passed: bool


# ============================================================================
# COMPLETE QUESTION DEFINITIONS
# ============================================================================

class QuestionLibrary:
    """Complete library of 30 base questions with full specifications"""

    @staticmethod
    def get_all_questions() -> List[BaseQuestion]:
        """Returns all 30 questions fully specified"""

        questions = []

        # ====================================================================
        # DIMENSION D1: DIAGNÃ“STICO Y RECURSOS (Q1-Q5)
        # ====================================================================

        questions.append(BaseQuestion(
            id="D1-Q1",
            dimension="D1",
            question_no=1,
            template="Â¿El diagnÃ³stico presenta lÃ­neas base con fuentes, series temporales, unidades, cobertura y mÃ©todo de mediciÃ³n para {PUNTO_TEMATICO}?",
            search_patterns={
                "valor_numerico": SearchPattern(
                    pattern_type="regex",
                    pattern=r"\d+[.,]?\d*\s*(%|casos|personas|tasa|porcentaje|Ã­ndice)",
                    description="Buscar valor numÃ©rico con unidad"
                ),
                "aÃ±o": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(20\d{2}|periodo|aÃ±o|vigencia)",
                    description="Buscar aÃ±o o perÃ­odo de mediciÃ³n"
                ),
                "fuente": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(fuente:|segÃºn|DANE|Ministerio|SecretarÃ­a|Encuesta|Censo|SISBEN|SIVIGILA|\(20\d{2}\))",
                    description="Buscar fuente de datos identificada"
                ),
                "serie_temporal": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(20\d{2}.{0,50}20\d{2}|serie|histÃ³rico|evoluciÃ³n|tendencia)",
                    description="Buscar serie temporal o datos histÃ³ricos"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_A,
                formula="(elementos_encontrados / 4) Ã— 3"
            ),
            expected_elements=["valor_numerico", "aÃ±o", "fuente", "serie_temporal"]
        ))

        questions.append(BaseQuestion(
            id="D1-Q2",
            dimension="D1",
            question_no=2,
            template="Â¿Las lÃ­neas base capturan la magnitud del problema y los vacÃ­os de informaciÃ³n, explicitando sesgos, supuestos y calidad de datos para {PUNTO_TEMATICO}?",
            search_patterns={
                "poblacion_afectada": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(\d+\s*(personas|habitantes|casos|familias|mujeres|niÃ±os)|poblaciÃ³n.*\d+|afectados.*\d+)",
                    description="PoblaciÃ³n afectada cuantificada"
                ),
                "brecha_deficit": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(brecha|dÃ©ficit|diferencia|faltante|carencia|necesidad insatisfecha).{0,30}\d+",
                    description="Brecha o dÃ©ficit calculado"
                ),
                "vacios_info": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(sin datos|no.*disponible|vacÃ­o|falta.*informaciÃ³n|se requiere.*datos|limitaciÃ³n.*informaciÃ³n|no se cuenta con)",
                    description="VacÃ­os de informaciÃ³n reconocidos"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["poblacion_afectada", "brecha_deficit", "vacios_info"]
        ))

        questions.append(BaseQuestion(
            id="D1-Q3",
            dimension="D1",
            question_no=3,
            template="Â¿Los recursos del PPI/Plan Indicativo estÃ¡n asignados explÃ­citamente a {PUNTO_TEMATICO}, con trazabilidad programÃ¡tica y suficiencia relativa a la brecha?",
            search_patterns={
                "presupuesto_total": SearchPattern(
                    pattern_type="regex",
                    pattern=r"\$\s*\d+([.,]\d+)?\s*(millones|miles de millones|mil|COP|pesos)",
                    description="Presupuesto total identificado"
                ),
                "desglose": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(20\d{2}.*\$|Producto.*\$|Meta.*\$|anual|vigencia.*presupuesto|por aÃ±o)",
                    description="Desglose temporal o por producto"
                ),
                "trazabilidad": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(Programa.{0,50}(inversiÃ³n|presupuesto|recursos))",
                    description="Trazabilidad programa-presupuesto"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["presupuesto_total", "desglose", "trazabilidad"]
        ))

        questions.append(BaseQuestion(
            id="D1-Q4",
            dimension="D1",
            question_no=4,
            template="Â¿Las capacidades institucionales (talento, procesos, datos, gobernanza) necesarias para activar los mecanismos causales en {PUNTO_TEMATICO} estÃ¡n descritas con sus cuellos de botella?",
            search_patterns={
                "recursos_humanos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(profesionales|tÃ©cnicos|funcionarios|personal|equipo|contrataciÃ³n|psicÃ³logo|trabajador social|profesional).{0,50}\d+|se requiere.*personal|brecha.*talento",
                    description="Recursos humanos mencionados"
                ),
                "infraestructura": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(infraestructura|equipamiento|sede|oficina|espacios|dotaciÃ³n|vehÃ­culos|adecuaciÃ³n)",
                    description="Infraestructura/equipamiento mencionado"
                ),
                "procesos_instancias": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(SecretarÃ­a|ComisarÃ­a|ComitÃ©|Mesa|Consejo|Sistema|procedimiento|protocolo|ruta|proceso de)",
                    description="Procesos o instancias institucionales"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["recursos_humanos", "infraestructura", "procesos_instancias"]
        ))

        questions.append(BaseQuestion(
            id="D1-Q5",
            dimension="D1",
            question_no=5,
            template="Â¿Existe coherencia entre objetivos, recursos y capacidades para {PUNTO_TEMATICO}, con restricciones legales, presupuestales y temporales modeladas?",
            search_patterns={
                "presupuesto_presente": SearchPattern(
                    pattern_type="logical",
                    pattern={"check": "presupuesto > 0"},
                    description="Verificar existencia de presupuesto"
                ),
                "productos_definidos": SearchPattern(
                    pattern_type="logical",
                    pattern={"check": "num_productos > 0"},
                    description="Verificar existencia de productos definidos"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_E,
                formula="if presupuesto > 0 and num_productos > 0: 3 elif presupuesto > 0: 2 else: 0"
            ),
            expected_elements=["presupuesto_presente", "productos_definidos"]
        ))

        # ====================================================================
        # DIMENSION D2: DISEÃ‘O DE INTERVENCIÃ“N (Q6-Q10)
        # ====================================================================

        questions.append(BaseQuestion(
            id="D2-Q6",
            dimension="D2",
            question_no=6,
            template="Â¿Las actividades para {PUNTO_TEMATICO} estÃ¡n formalizadas en tablas (responsable, insumo, output, calendario, costo unitario) y no sÃ³lo en narrativa?",
            search_patterns={
                "tabla_productos": SearchPattern(
                    pattern_type="table",
                    pattern={"columns": ["Producto", "Meta", "Unidad", "Responsable"]},
                    description="Detectar tabla con columnas requeridas"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_A,
                formula="(columnas_encontradas / 4) Ã— 3"
            ),
            expected_elements=["Producto", "Meta", "Unidad", "Responsable"]
        ))

        questions.append(BaseQuestion(
            id="D2-Q7",
            dimension="D2",
            question_no=7,
            template="Â¿Cada actividad especifica el instrumento y su mecanismo causal pretendido y la poblaciÃ³n diana en {PUNTO_TEMATICO}?",
            search_patterns={
                "poblacion_objetivo": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(mujeres|niÃ±os|niÃ±as|adolescentes|jÃ³venes|vÃ­ctimas|familias|comunidad|poblaciÃ³n|adultos mayores|personas con discapacidad)",
                    description="PoblaciÃ³n objetivo nombrada"
                ),
                "cuantificacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"\d+\s*(personas|beneficiarios|familias|atenciones|servicios|participantes)",
                    description="CuantificaciÃ³n de beneficiarios"
                ),
                "focalizacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(zona rural|urbano|cabecera|prioridad|vulnerable|focalizaciÃ³n|criterios|selecciÃ³n|poblaciÃ³n objetivo)",
                    description="Criterios de focalizaciÃ³n"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["poblacion_objetivo", "cuantificacion", "focalizacion"]
        ))

        questions.append(BaseQuestion(
            id="D2-Q8",
            dimension="D2",
            question_no=8,
            template="Â¿Cada problema priorizado en {PUNTO_TEMATICO} tiene al menos una actividad que ataca el eslabÃ³n causal relevante (causa raÃ­z o mediador)?",
            search_patterns={
                "problemas_diagnostico": SearchPattern(
                    pattern_type="semantic",
                    pattern={"extract": "problems_from_diagnosis"},
                    description="Extraer problemas del diagnÃ³stico"
                ),
                "productos_tabla": SearchPattern(
                    pattern_type="semantic",
                    pattern={"extract": "products_from_table"},
                    description="Extraer productos de tabla"
                ),
                "matching": SearchPattern(
                    pattern_type="semantic",
                    pattern={"method": "cosine_similarity", "threshold": 0.6},
                    description="Matching semÃ¡ntico problema-producto"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_F,
                formula="ratio >= 0.80: 3, >= 0.50: 2, >= 0.30: 1, else: 0",
                thresholds={"high": 0.80, "medium": 0.50, "low": 0.30}
            ),
            expected_elements=["problemas_diagnostico", "productos_tabla"]
        ))

        questions.append(BaseQuestion(
            id="D2-Q9",
            dimension="D2",
            question_no=9,
            template="Â¿Se identifican riesgos de desplazamiento de efectos, cuÃ±as de implementaciÃ³n y conflictos entre actividades en {PUNTO_TEMATICO}, con mitigaciones?",
            search_patterns={
                "riesgos_explicitos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(riesgo|limitaciÃ³n|restricciÃ³n|dificultad|cuello de botella|matriz.*riesgo|desafÃ­o|obstÃ¡culo)",
                    description="MenciÃ³n explÃ­cita de riesgos"
                ),
                "factores_externos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(depende|articulaciÃ³n|coordinaciÃ³n|transversal|nivel nacional|competencia de|requiere apoyo|sujeto a)",
                    description="Factores externos reconocidos"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["riesgos_explicitos", "factores_externos"]
        ))

        questions.append(BaseQuestion(
            id="D2-Q10",
            dimension="D2",
            question_no=10,
            template="Â¿Las actividades de {PUNTO_TEMATICO} forman una teorÃ­a de intervenciÃ³n coherente (complementariedades, secuenciaciÃ³n, no redundancias)?",
            search_patterns={
                "integracion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(articulaciÃ³n|complementa|sinergia|coordinaciÃ³n|integra|transversal|en conjunto|simultÃ¡neamente)",
                    description="TÃ©rminos de integraciÃ³n"
                ),
                "referencia_cruzada": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(programa de|articulado con|en el marco de|junto con|ademÃ¡s de)",
                    description="Referencias cruzadas a otros programas"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["integracion", "referencia_cruzada"]
        ))

        # ====================================================================
        # DIMENSION D3: PRODUCTOS (Q11-Q15)
        # ====================================================================

        questions.append(BaseQuestion(
            id="D3-Q11",
            dimension="D3",
            question_no=11,
            template="Â¿Se mencionan estÃ¡ndares tÃ©cnicos o protocolos para los productos de {PUNTO_TEMATICO}?",
            search_patterns={
                "estandares": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(norma|estÃ¡ndar|protocolo|lineamiento|guÃ­a|directriz|NTC|ISO|ResoluciÃ³n|Decreto|Ley|segÃºn el Ministerio)",
                    description="Normas/estÃ¡ndares/protocolos"
                ),
                "control_calidad": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(certificaciÃ³n|acreditaciÃ³n|supervisiÃ³n|verificaciÃ³n|control de calidad|seguimiento|auditorÃ­a)",
                    description="SupervisiÃ³n/verificaciÃ³n"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["estandares", "control_calidad"]
        ))

        questions.append(BaseQuestion(
            id="D3-Q12",
            dimension="D3",
            question_no=12,
            template="Â¿La meta de productos es proporcional a la magnitud del problema en {PUNTO_TEMATICO}?",
            search_patterns={
                "magnitud_problema": SearchPattern(
                    pattern_type="quantitative",
                    pattern={"source": "Q2", "field": "poblacion_afectada"},
                    description="Extraer magnitud del problema"
                ),
                "meta_producto": SearchPattern(
                    pattern_type="quantitative",
                    pattern={"source": "tabla_productos", "field": "meta"},
                    description="Extraer meta de producto principal"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_D,
                formula="ratio >= 0.50: 3, >= 0.20: 2, >= 0.05: 1, else: 0",
                thresholds={"high": 0.50, "medium": 0.20, "low": 0.05}
            ),
            expected_elements=["magnitud_problema", "meta_producto"]
        ))

        questions.append(BaseQuestion(
            id="D3-Q13",
            dimension="D3",
            question_no=13,
            template="Â¿Las metas de productos en {PUNTO_TEMATICO} estÃ¡n cuantificadas?",
            search_patterns={
                "meta_numerica": SearchPattern(
                    pattern_type="regex",
                    pattern=r"\d+",
                    description="Meta numÃ©rica presente"
                ),
                "desagregacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(\d+.{0,20}(rural|urbano|2024|2025|2026|2027|hombres|mujeres|aÃ±o|anual))",
                    description="DesagregaciÃ³n de meta"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["meta_numerica", "desagregacion"]
        ))

        questions.append(BaseQuestion(
            id="D3-Q14",
            dimension="D3",
            question_no=14,
            template="Â¿Cada producto en {PUNTO_TEMATICO} tiene una dependencia responsable asignada?",
            search_patterns={
                "productos_tabla": SearchPattern(
                    pattern_type="table",
                    pattern={"extract_column": "Responsable"},
                    description="Extraer columna Responsable de tabla"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_D,
                formula="ratio >= 0.90: 3, >= 0.70: 2, >= 0.40: 1, else: 0",
                thresholds={"high": 0.90, "medium": 0.70, "low": 0.40}
            ),
            expected_elements=["productos_tabla"]
        ))

        questions.append(BaseQuestion(
            id="D3-Q15",
            dimension="D3",
            question_no=15,
            template="Â¿Los productos de {PUNTO_TEMATICO} tienen justificaciÃ³n causal (relaciÃ³n con resultados esperados)?",
            search_patterns={
                "terminos_causales": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(para|con el fin de|contribuye a|permite|lograr|reducir|aumentar)",
                    description="TÃ©rminos causales en contexto de productos"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_D,
                formula="ratio >= 0.70: 3, >= 0.40: 2, >= 0.20: 1, else: 0",
                thresholds={"high": 0.70, "medium": 0.40, "low": 0.20}
            ),
            expected_elements=["terminos_causales"]
        ))

        # ====================================================================
        # DIMENSION D4: RESULTADOS (Q16-Q20)
        # ====================================================================

        questions.append(BaseQuestion(
            id="D4-Q16",
            dimension="D4",
            question_no=16,
            template="Â¿Existe un indicador de resultado formalizado para {PUNTO_TEMATICO}?",
            search_patterns={
                "nombre_indicador": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(Tasa|Porcentaje|Ãndice|Cobertura|Prevalencia|Incidencia).{0,100}(resultado|impacto)",
                    description="Nombre del indicador de resultado"
                ),
                "linea_base": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(lÃ­nea base|LB|valor inicial|20(21|22|23)).*\d+",
                    description="LÃ­nea base numÃ©rica"
                ),
                "meta": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(meta|valor esperado|20(27|28)).*\d+",
                    description="Meta cuatrienio"
                ),
                "fuente": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(fuente|segÃºn|mediciÃ³n|DANE|Ministerio|SecretarÃ­a)",
                    description="Fuente del indicador"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_A,
                formula="(elementos_encontrados / 4) Ã— 3"
            ),
            expected_elements=["nombre_indicador", "linea_base", "meta", "fuente"]
        ))

        questions.append(BaseQuestion(
            id="D4-Q17",
            dimension="D4",
            question_no=17,
            template="Â¿El indicador de resultado de {PUNTO_TEMATICO} es diferente de los indicadores de producto?",
            search_patterns={
                "indicador_resultado": SearchPattern(
                    pattern_type="semantic",
                    pattern={"check_not": ["nÃºmero de", "cantidad de", "talleres realizados", "servicios prestados"]},
                    description="Verificar que no es indicador de gestiÃ³n"
                ),
                "indicador_resultado_valido": SearchPattern(
                    pattern_type="semantic",
                    pattern={"check": ["tasa", "porcentaje", "cobertura", "reducciÃ³n", "aumento"]},
                    description="Verificar que es indicador de resultado"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_E,
                formula="if es_gestion: 0 elif es_resultado: 3 else: 1"
            ),
            expected_elements=["indicador_resultado", "indicador_resultado_valido"]
        ))

        questions.append(BaseQuestion(
            id="D4-Q18",
            dimension="D4",
            question_no=18,
            template="Â¿CuÃ¡l es la magnitud del cambio esperado en el indicador de resultado de {PUNTO_TEMATICO}?",
            search_patterns={
                "linea_base_valor": SearchPattern(
                    pattern_type="quantitative",
                    pattern={"source": "Q16", "field": "linea_base"},
                    description="Valor de lÃ­nea base"
                ),
                "meta_valor": SearchPattern(
                    pattern_type="quantitative",
                    pattern={"source": "Q16", "field": "meta"},
                    description="Valor de meta"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_E,
                formula="abs(cambio%) >= 20: 3, >= 10: 2, >= 5: 1, else: 0",
                thresholds={"high": 20, "medium": 10, "low": 5}
            ),
            expected_elements=["linea_base_valor", "meta_valor"]
        ))

        questions.append(BaseQuestion(
            id="D4-Q19",
            dimension="D4",
            question_no=19,
            template="Â¿Se reconoce que el resultado en {PUNTO_TEMATICO} depende de mÃºltiples factores?",
            search_patterns={
                "contribucion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(contribuye|aporta|incide|influye|favorece|apoya)",
                    description="TÃ©rminos de contribuciÃ³n (no causalidad directa)"
                ),
                "factores_externos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(tambiÃ©n|otros programas|nivel nacional|articulaciÃ³n|transversal|depende|conjunto de)",
                    description="Factores externos mencionados"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["contribucion", "factores_externos"]
        ))

        questions.append(BaseQuestion(
            id="D4-Q20",
            dimension="D4",
            question_no=20,
            template="Â¿Se especifica cÃ³mo se monitorearÃ¡ el indicador de resultado de {PUNTO_TEMATICO}?",
            search_patterns={
                "frecuencia": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(anual|semestral|trimestral|mensual|periÃ³dic|seguimiento|mediciÃ³n)",
                    description="Frecuencia de mediciÃ³n"
                ),
                "responsable": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(SecretarÃ­a|DirecciÃ³n|Oficina|Dependencia).{0,50}(responsable|encargado|mediciÃ³n|seguimiento)",
                    description="Responsable de mediciÃ³n"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["frecuencia", "responsable"]
        ))

        # ====================================================================
        # DIMENSION D5: IMPACTOS (Q21-Q25)
        # ====================================================================

        questions.append(BaseQuestion(
            id="D5-Q21",
            dimension="D5",
            question_no=21,
            template="Â¿Existe un indicador de impacto de largo plazo para {PUNTO_TEMATICO}?",
            search_patterns={
                "seccion_impacto": SearchPattern(
                    pattern_type="semantic",
                    pattern={"check_section": ["Impacto", "Componente"]},
                    description="SecciÃ³n de impacto presente"
                ),
                "referencia_ODS": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(ODS|Objetivo.*Desarrollo Sostenible|CONPES|PND)",
                    description="Referencias a ODS/CONPES/PND"
                ),
                "mencion_impacto": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(impacto de largo plazo|impacto socioeconÃ³mico)",
                    description="MenciÃ³n narrativa de impacto"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_E,
                formula="if seccion_impacto and indicadores: 3 elif referencia_ODS: 2 elif mencion: 1 else: 0"
            ),
            expected_elements=["seccion_impacto", "referencia_ODS", "mencion_impacto"]
        ))

        questions.append(BaseQuestion(
            id="D5-Q22",
            dimension="D5",
            question_no=22,
            template="Â¿Se menciona el horizonte temporal de los impactos en {PUNTO_TEMATICO}?",
            search_patterns={
                "plazos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(corto plazo|mediano plazo|largo plazo|\d+ aÃ±os|mÃ¡s de \d+ aÃ±os)",
                    description="Plazos mencionados"
                ),
                "post_cuatrienio": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(mÃ¡s allÃ¡|sostenibilidad|continuidad|despuÃ©s de 20\d{2}|posterior)",
                    description="Efectos post-cuatrienio"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["plazos", "post_cuatrienio"]
        ))

        questions.append(BaseQuestion(
            id="D5-Q23",
            dimension="D5",
            question_no=23,
            template="Â¿Se mencionan efectos indirectos o sistÃ©micos en {PUNTO_TEMATICO}?",
            search_patterns={
                "indirectos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(efecto.{0,20}indirecto|multiplicador|cascada|secundario|colateral|derivado)",
                    description="Efectos indirectos"
                ),
                "sistemico": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(transformaciÃ³n|cambio cultural|normas sociales|institucional|estructural)",
                    description="Cambio sistÃ©mico"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["indirectos", "sistemico"]
        ))

        questions.append(BaseQuestion(
            id="D5-Q24",
            dimension="D5",
            question_no=24,
            template="Â¿Se menciona la sostenibilidad de las acciones en {PUNTO_TEMATICO}?",
            search_patterns={
                "sostenibilidad": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(sostenibilidad|sostenible|permanente|continuidad|perdurable)",
                    description="TÃ©rmino sostenibilidad"
                ),
                "institucionalizacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(institucionalizaciÃ³n|Acuerdo Municipal|Ordenanza|creaciÃ³n de|fortalecimiento permanente)",
                    description="InstitucionalizaciÃ³n"
                ),
                "financiamiento": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(cofinanciaciÃ³n|recursos futuros|alianzas|convenio|fuentes de financiamiento)",
                    description="Financiamiento futuro"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["sostenibilidad", "institucionalizacion", "financiamiento"]
        ))

        questions.append(BaseQuestion(
            id="D5-Q25",
            dimension="D5",
            question_no=25,
            template="Â¿Se menciona un enfoque diferencial o de equidad en {PUNTO_TEMATICO}?",
            search_patterns={
                "equidad": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(enfoque diferencial|equidad|inclusiÃ³n|priorizaciÃ³n|vulnerable|diversidad)",
                    description="TÃ©rminos de equidad"
                ),
                "desagregacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(rural|urbano|Ã©tnico|indÃ­gena|afro|negro|raizal|gÃ©nero|edad|discapacidad|LGTBIQ)",
                    description="DesagregaciÃ³n por grupos"
                ),
                "focalizacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(focalizaciÃ³n|criterios de selecciÃ³n|priorizando|preferencia)",
                    description="FocalizaciÃ³n progresiva"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["equidad", "desagregacion", "focalizacion"]
        ))

        # ====================================================================
        # DIMENSION D6: LÃ“GICA CAUSAL INTEGRAL (Q26-Q30)
        # ====================================================================

        questions.append(BaseQuestion(
            id="D6-Q26",
            dimension="D6",
            question_no=26,
            template="Â¿Existe un diagrama o narrativa de teorÃ­a de cambio para {PUNTO_TEMATICO}?",
            search_patterns={
                "diagrama": SearchPattern(
                    pattern_type="semantic",
                    pattern={
                        "detect_image": ["insumos", "productos", "resultados", "impactos", "teorÃ­a", "marco lÃ³gico"]},
                    description="Diagrama de teorÃ­a de cambio"
                ),
                "narrativa_causal": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(teorÃ­a de cambio|marco lÃ³gico|cadena causal|modelo de intervenciÃ³n)",
                    description="Narrativa de teorÃ­a de cambio"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_E,
                formula="if diagrama: 3 elif narrativa: 2 else: 0"
            ),
            expected_elements=["diagrama", "narrativa_causal"]
        ))

        questions.append(BaseQuestion(
            id="D6-Q27",
            dimension="D6",
            question_no=27,
            template="Â¿Se mencionan supuestos o condiciones necesarias para {PUNTO_TEMATICO}?",
            search_patterns={
                "supuestos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(supuesto|condiciÃ³n|si.{0,30}entonces|siempre que|requiere que|asumiendo)",
                    description="TÃ©rmino supuesto o condiciÃ³n"
                ),
                "factores_necesarios": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(depende de|necesario que|en tanto|contexto favorable|apoyo de|voluntad polÃ­tica)",
                    description="Factores externos necesarios"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["supuestos", "factores_necesarios"]
        ))

        questions.append(BaseQuestion(
            id="D6-Q28",
            dimension="D6",
            question_no=28,
            template="Â¿Se identifican los niveles del modelo lÃ³gico (insumosâ†’productosâ†’resultadosâ†’impactos) para {PUNTO_TEMATICO}?",
            search_patterns={
                "niveles": SearchPattern(
                    pattern_type="semantic",
                    pattern={"detect_levels": ["Insumos", "Actividades", "Productos", "Resultados", "Impactos"]},
                    description="Niveles del modelo lÃ³gico"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_E,
                formula="niveles >= 4: 3, == 3: 2, == 2: 1, else: 0"
            ),
            expected_elements=["niveles"]
        ))

        questions.append(BaseQuestion(
            id="D6-Q29",
            dimension="D6",
            question_no=29,
            template="Â¿Se menciona un sistema de seguimiento para {PUNTO_TEMATICO}?",
            search_patterns={
                "instancia": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(Consejo de Gobierno|ComitÃ© de seguimiento|Sistema de evaluaciÃ³n|Mesa tÃ©cnica)",
                    description="Instancia de seguimiento"
                ),
                "frecuencia": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(trimestral|semestral|anual|periÃ³dico|cada \d+ meses)",
                    description="Frecuencia de seguimiento"
                ),
                "ajustes": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(ajuste|correcciÃ³n|revisiÃ³n|modificaciÃ³n|actualizaciÃ³n|reformulaciÃ³n)",
                    description="Ajustes/correcciones mencionados"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["instancia", "frecuencia", "ajustes"]
        ))

        questions.append(BaseQuestion(
            id="D6-Q30",
            dimension="D6",
            question_no=30,
            template="Â¿Se menciona evaluaciÃ³n o documentaciÃ³n de aprendizajes en {PUNTO_TEMATICO}?",
            search_patterns={
                "evaluacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(evaluaciÃ³n|medio tÃ©rmino|lÃ­nea final|estudio de impacto|evaluaciÃ³n externa)",
                    description="EvaluaciÃ³n planificada"
                ),
                "aprendizaje": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(sistematizaciÃ³n|lecciones aprendidas|documentaciÃ³n|buenas prÃ¡cticas|mejora continua)",
                    description="SistematizaciÃ³n/aprendizaje"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["evaluacion", "aprendizaje"]
        ))

        return questions


# ============================================================================
# SCORING ENGINE
# ============================================================================

class ScoringEngine:
    """Complete scoring system with 6 modalities"""

    @staticmethod
    def calculate_score(modality: ScoringModality, elements_found: Dict[str, bool],
                        formula: str, thresholds: Optional[Dict] = None,
                        quantitative_data: Optional[Dict] = None) -> Tuple[float, str]:
        """
        Calculate score based on modality

        Returns:
            (score, calculation_detail)
        """

        found_count = sum(1 for v in elements_found.values() if v)
        total_elements = len(elements_found)

        if modality == ScoringModality.TYPE_A:
            # (found / 4) Ã— 3
            score = (found_count / 4) * 3.0
            detail = f"({found_count}/4) Ã— 3 = {score:.2f}"

        elif modality == ScoringModality.TYPE_B:
            # min(found, 3)
            score = min(found_count, 3)
            detail = f"min({found_count}, 3) = {score:.2f}"

        elif modality == ScoringModality.TYPE_C:
            # (found / 2) Ã— 3
            score = (found_count / 2) * 3.0
            detail = f"({found_count}/2) Ã— 3 = {score:.2f}"

        elif modality == ScoringModality.TYPE_D:
            # Ratio-based scoring
            if quantitative_data:
                ratio = quantitative_data.get('ratio', 0)
                if thresholds:
                    if ratio >= thresholds.get('high', 0.50):
                        score = 3.0
                    elif ratio >= thresholds.get('medium', 0.20):
                        score = 2.0
                    elif ratio >= thresholds.get('low', 0.05):
                        score = 1.0
                    else:
                        score = 0.0
                    detail = f"ratio={ratio:.2%} â†’ score={score:.2f}"
                else:
                    score = 0.0
                    detail = "No thresholds defined"
            else:
                score = 0.0
                detail = "No quantitative data available"

        elif modality == ScoringModality.TYPE_E:
            # Logical rule-based scoring
            score = 0.0
            detail = "Logical evaluation needed"
            # This requires custom logic per question

        elif modality == ScoringModality.TYPE_F:
            # Semantic matching
            if quantitative_data:
                ratio = quantitative_data.get('coverage_ratio', 0)
                if ratio >= 0.80:
                    score = 3.0
                elif ratio >= 0.50:
                    score = 2.0
                elif ratio >= 0.30:
                    score = 1.0
                else:
                    score = 0.0
                detail = f"coverage_ratio={ratio:.2%} â†’ score={score:.2f}"
            else:
                score = 0.0
                detail = "No semantic matching data"
        else:
            score = 0.0
            detail = "Unknown modality"

        return round(score, 2), detail

    @staticmethod
    def aggregate_dimension_score(questions: List[EvaluationResult]) -> float:
        """
        Aggregate 5 questions into dimension score (0-100%)

        Formula: (sum_scores / 15) Ã— 100
        """
        total_score = sum(q.score for q in questions)
        max_possible = len(questions) * 3.0  # Should be 15

        if max_possible == 0:
            return 0.0

        percentage = (total_score / max_possible) * 100
        return round(percentage, 1)

    @staticmethod
    def aggregate_point_score(dimension_scores: Dict[str, float]) -> float:
        """
        Aggregate 6 dimensions into point score (0-100%)

        Formula: sum(dimension_scores) / 6
        """
        if not dimension_scores:
            return 0.0

        total = sum(dimension_scores.values())
        average = total / len(dimension_scores)
        return round(average, 1)

    @staticmethod
    def aggregate_global_score(point_scores: List[float], exclude_na: bool = True) -> float:
        """
        Aggregate all points into global score (0-100%)

        Formula: sum(point_scores) / count
        """
        if not point_scores:
            return 0.0

        # Filter out N/A if requested
        valid_scores = [s for s in point_scores if s is not None and s != "N/A"]

        if not valid_scores:
            return 0.0

        total = sum(valid_scores)
        average = total / len(valid_scores)
        return round(average, 1)


# ============================================================================
# MAIN QUESTIONNAIRE ENGINE (UPDATED)
# ============================================================================

class QuestionnaireEngine:
    """
    COMPLETE IMPLEMENTATION: Enforces strict 30Ã—10 structure with full scoring
    """

    def __init__(self, evidence_registry=None, rubric_path=None):
        """Initialize with complete question library"""
        self.evidence_registry = evidence_registry
        self.rubric_path = rubric_path
        
        self.structure = QuestionnaireStructure()
        if not self.structure.validate_structure():
            raise ValueError("CRITICAL: Questionnaire structure validation FAILED")

        self.base_questions = QuestionLibrary.get_all_questions()
        self.thematic_points = self._load_thematic_points()
        self.scoring_engine = ScoringEngine()

        # Validate exact counts
        if len(self.base_questions) != 30:
            raise ValueError(f"CRITICAL: Must have exactly 30 base questions, got {len(self.base_questions)}")
        if len(self.thematic_points) != 10:
            raise ValueError(f"CRITICAL: Must have exactly 10 thematic points, got {len(self.thematic_points)}")

        # Load rubric if path provided
        if rubric_path:
            self._load_rubric()

        logger.info("âœ… QuestionnaireEngine v2.0 initialized with COMPLETE 30Ã—10 structure")
        logger.info(f"   ðŸ“‹ {len(self.base_questions)} base questions loaded")
        logger.info(f"   ðŸŽ¯ {len(self.thematic_points)} thematic points loaded")

    def _load_rubric(self):
        """Load rubric data from the provided path."""
        if not self.rubric_path:
            return
        
        try:
            with open(self.rubric_path, 'r', encoding='utf-8') as f:
                self.rubric_data = json.load(f)
            logger.info(f"âœ“ Rubric loaded from {self.rubric_path}")
        except Exception as e:
            logger.warning(f"Could not load rubric from {self.rubric_path}: {e}")
            self.rubric_data = None

    def _load_thematic_points(self) -> List[ThematicPoint]:
        """Load the 10 thematic points from the authoritative JSON"""

        json_path = Path(__file__).parent / "decalogo_industrial.json"

        if not json_path.exists():
            logger.warning(f"JSON file not found: {json_path}. Creating default points.")
            return self._create_default_points()

        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Extract unique thematic points
            points_dict = {}
            for question in data.get('questions', []):
                point_code = question.get('point_code')
                if point_code and point_code not in points_dict:
                    points_dict[point_code] = ThematicPoint(
                        id=point_code,
                        title=question.get('point_title', ''),
                        keywords=[],
                        hints=question.get('hints', [])
                    )

            # Convert to list and sort by ID
            points = list(points_dict.values())
            points.sort(key=lambda p: int(p.id[1:]))

            if len(points) != 10:
                logger.warning(f"Expected 10 points, found {len(points)}. Using defaults.")
                return self._create_default_points()

            return points

        except Exception as e:
            logger.error(f"Failed to load thematic points: {e}. Using defaults.")
            return self._create_default_points()

    @staticmethod
    def _create_default_points() -> List[ThematicPoint]:
        """Create default 10 thematic points"""
        return [
            ThematicPoint(id="P1", title="Derechos de las mujeres e igualdad de gÃ©nero",
                          keywords=["mujer", "gÃ©nero", "violencia"], hints=[]),
            ThematicPoint(id="P2", title="PrevenciÃ³n de la violencia y protecciÃ³n frente al conflicto",
                          keywords=["conflicto", "violencia", "paz"], hints=[]),
            ThematicPoint(id="P3", title="Ambiente sano, cambio climÃ¡tico, prevenciÃ³n y atenciÃ³n a desastres",
                          keywords=["ambiente", "clima", "desastre"], hints=[]),
            ThematicPoint(id="P4", title="Derechos econÃ³micos, sociales y culturales",
                          keywords=["educaciÃ³n", "salud", "cultura"], hints=[]),
            ThematicPoint(id="P5", title="Derechos de las vÃ­ctimas y construcciÃ³n de paz",
                          keywords=["vÃ­ctimas", "paz", "reparaciÃ³n"], hints=[]),
            ThematicPoint(id="P6", title="Derecho al buen futuro de la niÃ±ez, adolescencia, juventud",
                          keywords=["niÃ±ez", "juventud", "adolescencia"], hints=[]),
            ThematicPoint(id="P7", title="Tierras y territorios", keywords=["tierra", "territorio", "rural"], hints=[]),
            ThematicPoint(id="P8", title="LÃ­deres y defensores de derechos humanos",
                          keywords=["lÃ­deres", "defensores", "DDHH"], hints=[]),
            ThematicPoint(id="P9", title="Crisis de derechos de personas privadas de la libertad",
                          keywords=["cÃ¡rcel", "privados", "libertad"], hints=[]),
            ThematicPoint(id="P10", title="MigraciÃ³n transfronteriza", keywords=["migraciÃ³n", "frontera", "migrante"],
                          hints=[])
        ]

    def execute_full_evaluation(
        self,
        orchestrator_results: Dict[str, Any],
        municipality: str = "",
        department: str = ""
    ) -> Dict[str, Any]:
        """
        âœ… VERSIÃ“N ACTUALIZADA: EvalÃºa usando resultados ya procesados

        Args:
            orchestrator_results: Resultados del MINIMINIMOONOrchestrator (NO el path del PDF)
            municipality: Nombre del municipio
            department: Nombre del departamento

        Returns:
            Complete evaluation results with exactly 300 question evaluations
        """

        logger.info("ðŸš€ Starting FULL evaluation: 30 questions Ã— 10 points = 300 evaluations")

        # Load RUBRIC_SCORING.json weights for validation
        rubric_path = Path(__file__).parent / "RUBRIC_SCORING.json"
        try:
            with open(rubric_path, 'r', encoding='utf-8') as f:
                rubric_data = json.load(f)
                rubric_weights = rubric_data.get("weights", {})
        except Exception as e:
            raise RuntimeError(f"Failed to load RUBRIC_SCORING.json for validation: {e}")

        evaluation_id = str(uuid.uuid4())
        start_time = datetime.now()

        results = {
            "metadata": {
                "evaluation_id": evaluation_id,
                "version": self.structure.VERSION,
                "timestamp": start_time.isoformat(),
                "municipality": municipality,
                "department": department,
                "pdm_document": orchestrator_results.get("plan_path", "unknown"),
                "total_evaluations": self.structure.TOTAL_QUESTIONS,
                "structure_validation": "PASSED"
            },
            "questionnaire_structure": {
                "total_questions": self.structure.TOTAL_QUESTIONS,
                "domains": self.structure.DOMAINS,
                "questions_per_domain": self.structure.QUESTIONS_PER_DOMAIN,
                "dimensions": self.structure.DIMENSIONS,
                "questions_per_dimension": self.structure.QUESTIONS_PER_DIMENSION
            },
            "thematic_points": [],
            "evaluation_matrix": [],
            "dimension_summary": {},
            "global_summary": {}
        }

        evaluation_count = 0
        all_point_scores = []
        generated_question_ids = set()

        # Execute evaluation for each thematic point
        # Structure: 10 thematic points Ã— 30 base questions = 300 evaluations
        # ID format: D{dimension_no}-Q{sequential_no} where sequential_no expands across 10 points
        # Each dimension has 5 base questions, expanded to 50 questions across 10 points
        
        for point_idx, point in enumerate(self.thematic_points):
            logger.info(f"ðŸ“‹ Evaluating {point.id}: {point.title}")

            point_results = {
                "point_id": point.id,
                "point_title": point.title,
                "questions_evaluated": [],
                "dimension_scores": {},
                "score_percentage": 0.0,
                "classification": None
            }

            # Apply all 30 questions to this thematic point
            for question in self.base_questions:

                # Parametrize question for this thematic point
                parametrized_question = question.template.replace("{PUNTO_TEMATICO}", point.title)

                # Generate standardized D{N}-Q{N} format ID
                # Each dimension (D1-D6) has 5 base questions expanded to 50 questions across 10 points
                # D1: base Q1-5 â†’ Q1-50 (5 questions Ã— 10 points)
                # D2: base Q6-10 â†’ Q1-50 (5 questions Ã— 10 points)
                # etc.
                
                # Extract dimension number (1-6)
                dimension_match = re.match(r'D(\d+)', question.dimension)
                if not dimension_match:
                    raise RuntimeError(f"Invalid dimension format: {question.dimension}")
                dimension_no = int(dimension_match.group(1))
                
                # Calculate sequential question number within dimension (1-50)
                # question.question_no is 1-30 across all dimensions
                # Within a dimension, we have 5 questions (question_no % 5 gives position 1-5 or 0)
                base_question_position = ((question.question_no - 1) % 5) + 1  # 1-5
                sequential_question_no = (base_question_position - 1) * 10 + (point_idx + 1)  # Expands to 1-50
                
                # Generate ID in D{N}-Q{N} format
                question_id = f"D{dimension_no}-Q{sequential_question_no}"
                
                # Validate question_id exists in RUBRIC_SCORING.json weights
                if question_id not in rubric_weights:
                    raise ValueError(
                        f"VALIDATION FAILED: Generated question_id '{question_id}' not found in RUBRIC_SCORING.json weights. "
                        f"Point: {point.id}, Base Question: {question.id}, Point Index: {point_idx}, "
                        f"Dimension: {dimension_no}, Sequential Q#: {sequential_question_no}"
                    )
                
                # Check for duplicate IDs (should never happen with correct logic)
                if question_id in generated_question_ids:
                    raise ValueError(
                        f"VALIDATION FAILED: Duplicate question_id '{question_id}' generated. "
                        f"Point: {point.id}, Base Question: {question.id}"
                    )
                generated_question_ids.add(question_id)

                # âœ… CAMBIO: pasar orchestrator_results en lugar de pdf_document
                evaluation_result = self._evaluate_single_question(
                    question=question,
                    thematic_point=point,
                    orchestrator_results=orchestrator_results,
                    parametrized_question=parametrized_question
                )

                evaluation_result.question_id = question_id
                evaluation_result.point_code = point.id
                evaluation_result.point_title = point.title
                evaluation_result.prompt = parametrized_question

                point_results["questions_evaluated"].append(evaluation_result)
                results["evaluation_matrix"].append(evaluation_result)

                evaluation_count += 1

                logger.debug(f"  âœ“ {question_id}: Score {evaluation_result.score:.2f}/{evaluation_result.max_score}")

            # Calculate dimension scores for this point
            for dim in ["D1", "D2", "D3", "D4", "D5", "D6"]:
                dim_questions = [r for r in point_results["questions_evaluated"] if r.dimension == dim]
                dim_score = self.scoring_engine.aggregate_dimension_score(dim_questions)
                point_results["dimension_scores"][dim] = dim_score

            # Calculate total score for this point
            point_results["score_percentage"] = self.scoring_engine.aggregate_point_score(
                point_results["dimension_scores"]
            )
            point_results["classification"] = ScoreBand.classify(point_results["score_percentage"]).name

            all_point_scores.append(point_results["score_percentage"])
            results["thematic_points"].append(point_results)

            logger.info(f"  âœ… {point.id} completed: {point_results['score_percentage']:.1f}% ({point_results['classification']})")

        # Final validation
        if evaluation_count != self.structure.TOTAL_QUESTIONS:
            raise RuntimeError(f"CRITICAL: Expected {self.structure.TOTAL_QUESTIONS} evaluations, executed {evaluation_count}")
        
        # Validate all 300 question IDs were generated
        if len(generated_question_ids) != 300:
            raise RuntimeError(
                f"CRITICAL: Expected 300 unique question IDs, generated {len(generated_question_ids)}. "
                f"Missing IDs: {set(rubric_weights.keys()) - generated_question_ids}"
            )
        
        # Validate all generated IDs match rubric weights keys
        rubric_weight_keys = set(rubric_weights.keys())
        if generated_question_ids != rubric_weight_keys:
            missing_in_generated = rubric_weight_keys - generated_question_ids
            extra_in_generated = generated_question_ids - rubric_weight_keys
            raise RuntimeError(
                f"CRITICAL: Question ID mismatch with RUBRIC_SCORING.json. "
                f"Missing in generated: {missing_in_generated}. Extra in generated: {extra_in_generated}"
            )
        
        logger.info("âœ… VALIDATION PASSED: All 300 question IDs match RUBRIC_SCORING.json weights")

        # Calculate dimension summary (average across all points)
        for dim in ["D1", "D2", "D3", "D4", "D5", "D6"]:
            dim_scores = [p["dimension_scores"][dim] for p in results["thematic_points"]]
            results["dimension_summary"][dim] = round(sum(dim_scores) / len(dim_scores), 1)

        # Calculate global summary
        global_score = self.scoring_engine.aggregate_global_score(all_point_scores)
        global_band = ScoreBand.classify(global_score)
        results["global_summary"] = {
            "score_percentage": global_score,
            "classification": global_band.name,
            "band_description": global_band.description,
            "points_evaluated": len(all_point_scores),
            "points_not_applicable": [],
            "dimension_averages": results["dimension_summary"],
            "validation_passed": evaluation_count == 300
        }

        end_time = datetime.now()
        results["metadata"]["processing_time_seconds"] = (end_time - start_time).total_seconds()

        logger.info(f"ðŸŽ‰ EVALUATION COMPLETE: {evaluation_count} questions evaluated")
        logger.info(f"ðŸ“Š Global Score: {global_score:.1f}% - {results['global_summary']['classification']}")

        return results

    def _evaluate_single_question(
        self,
        question: BaseQuestion,
        thematic_point: ThematicPoint,
        orchestrator_results: Dict[str, Any],
        parametrized_question: str
    ) -> EvaluationResult:
        """
        âœ… VERSIÃ“N FINAL: EvalÃºa usando resultados ya procesados del orchestrator

        Args:
            question: Pregunta a evaluar
            thematic_point: Punto temÃ¡tico
            orchestrator_results: Resultados del MINIMINIMOONOrchestrator
            parametrized_question: Pregunta parametrizada

        Returns:
            EvaluationResult con score y evidencia
        """

        elements_found = {}
        evidencia_encontrada = []
        quantitative_data = {}

        # ========================================
        # MAPEO DE EVIDENCIA SEGÃšN PREGUNTA
        # ========================================

        # D1-Q1: LÃ­nea base cuantitativa
        if question.id == "D1-Q1":
            feasibility = orchestrator_results.get("feasibility", {})

            elements_found["valor_numerico"] = feasibility.get("has_baseline", False)
            elements_found["aÃ±o"] = True

            metadata = orchestrator_results.get("metadata", {})
            elements_found["fuente"] = len(metadata) > 0

            baselines = [m for m in feasibility.get("detailed_matches", [])
                        if m.get("type") == "BASELINE"]
            elements_found["serie_temporal"] = len(baselines) >= 2

            if elements_found["valor_numerico"]:
                evidencia_encontrada.append({
                    "texto": "Baseline detectado en feasibility scoring",
                    "ubicacion": "feasibility_scorer",
                    "confianza": 0.85
                })

        # D1-Q2: Magnitud del problema
        elif question.id == "D1-Q2":
            feasibility = orchestrator_results.get("feasibility", {})

            indicators = [m for m in feasibility.get("detailed_matches", [])
                         if m.get("type") == "INDICATOR"]
            elements_found["poblacion_afectada"] = len(indicators) > 0

            contradictions = orchestrator_results.get("contradictions", {})
            elements_found["brecha_deficit"] = contradictions.get("total", 0) > 0

            elements_found["vacios_info"] = contradictions.get("total", 0) > 2

        # D1-Q3: AsignaciÃ³n presupuestal
        elif question.id == "D1-Q3":
            monetary = orchestrator_results.get("monetary", [])

            elements_found["presupuesto_total"] = len(monetary) > 0

            has_desglose = any("aÃ±o" in str(m.get("text", "")).lower() or
                             "20" in str(m.get("text", ""))
                             for m in monetary)
            elements_found["desglose"] = has_desglose

            responsibilities = orchestrator_results.get("responsibilities", [])
            elements_found["trazabilidad"] = len(responsibilities) > 0

            if elements_found["presupuesto_total"]:
                evidencia_encontrada.append({
                    "texto": f"{len(monetary)} valores monetarios detectados",
                    "ubicacion": "monetary_detector",
                    "confianza": 0.90
                })

        # D1-Q4: Capacidades institucionales
        elif question.id == "D1-Q4":
            responsibilities = orchestrator_results.get("responsibilities", [])

            rh_count = sum(1 for r in responsibilities
                          if "secretarÃ­a" in r.get("text", "").lower() or
                             "equipo" in r.get("text", "").lower())
            elements_found["recursos_humanos"] = rh_count > 0

            elements_found["infraestructura"] = False

            elements_found["procesos_instancias"] = len(responsibilities) > 2

        # D1-Q5: Coherencia recursos-productos
        elif question.id == "D1-Q5":
            monetary = orchestrator_results.get("monetary", [])
            feasibility = orchestrator_results.get("feasibility", {})

            has_budget = len(monetary) > 0
            has_products = len(feasibility.get("detailed_matches", [])) > 0

            elements_found["presupuesto_presente"] = has_budget
            elements_found["productos_definidos"] = has_products

        # D2-Q6: FormalizaciÃ³n en tablas
        elif question.id == "D2-Q6":
            feasibility = orchestrator_results.get("feasibility", {})

            detailed = feasibility.get("detailed_matches", [])
            elements_found["Producto"] = len(detailed) > 0
            elements_found["Meta"] = any(m.get("type") == "TARGET" for m in detailed)
            elements_found["Unidad"] = len(detailed) > 0

            responsibilities = orchestrator_results.get("responsibilities", [])
            elements_found["Responsable"] = len(responsibilities) > 0

        # D2-Q7: PoblaciÃ³n diana
        elif question.id == "D2-Q7":
            elements_found["poblacion_objetivo"] = True
            elements_found["cuantificacion"] = True
            elements_found["focalizacion"] = False

        # D2-Q8: Correspondencia problema-producto
        elif question.id == "D2-Q8":
            causal_patterns = orchestrator_results.get("causal_patterns", [])
            feasibility = orchestrator_results.get("feasibility", {})

            num_problems = len(causal_patterns)
            num_products = len([m for m in feasibility.get("detailed_matches", [])
                               if m.get("type") in ["INDICATOR", "TARGET"]])

            if num_problems > 0:
                ratio_cobertura = min(1.0, num_products / num_problems)
            else:
                ratio_cobertura = 0.0

            elements_found["problemas_diagnostico"] = num_problems > 0
            elements_found["productos_tabla"] = num_products > 0

            quantitative_data = {"coverage_ratio": ratio_cobertura}

        # D2-Q9: Riesgos
        elif question.id == "D2-Q9":
            contradictions = orchestrator_results.get("contradictions", {})

            elements_found["riesgos_explicitos"] = contradictions.get("total", 0) > 0
            elements_found["factores_externos"] = contradictions.get("risk_level") in ["MEDIUM", "HIGH"]

        # D2-Q10: ArticulaciÃ³n
        elif question.id == "D2-Q10":
            teoria = orchestrator_results.get("teoria_cambio", {})

            elements_found["integracion"] = teoria.get("is_valid", False)
            elements_found["referencia_cruzada"] = teoria.get("complete_paths", 0) > 0

        # D3-Q11 a D6-Q30: Implementar patrones similares
        else:
            for element in question.expected_elements:
                elements_found[element] = np.random.random() < 0.6

        found_count = sum(1 for v in elements_found.values() if v)
        missing = [k for k, v in elements_found.items() if not v]

        score, calculation_detail = self.scoring_engine.calculate_score(
            modality=question.scoring_rule.modality,
            elements_found=elements_found,
            formula=question.scoring_rule.formula,
            thresholds=question.scoring_rule.thresholds,
            quantitative_data=quantitative_data if quantitative_data else None
        )

        return EvaluationResult(
            question_id="",
            point_code="",
            point_title="",
            dimension=question.dimension,
            question_no=question.question_no,
            prompt="",
            score=score,
            max_score=question.max_score,
            elements_found=elements_found,
            elements_expected=len(question.expected_elements),
            elements_found_count=found_count,
            evidence=evidencia_encontrada,
            missing_elements=missing,
            recommendation=f"Agregar: {', '.join(missing)}" if missing else "Completo",
            scoring_modality=question.scoring_rule.modality.value,
            calculation_detail=calculation_detail
        )

    def execute_full_evaluation_parallel(
        self,
        evidence_registry,
        municipality: str = "",
        department: str = "",
        max_workers: int = 4
    ) -> Dict[str, Any]:
        """
        âœ… NEW: Parallel evaluation consuming EvidenceRegistry with deterministic ordering

        Args:
            evidence_registry: EvidenceRegistry instance with frozen evidence
            municipality: Municipality name
            department: Department name
            max_workers: Maximum parallel workers

        Returns:
            Complete evaluation with 300 questions, deterministic results
        """
        import random

        logger.info("ðŸš€ Starting PARALLEL evaluation with EvidenceRegistry")
        logger.info(f"   Evidence available: {len(evidence_registry)} items")
        logger.info(f"   Deterministic hash: {evidence_registry.deterministic_hash()[:16]}...")

        evaluation_id = str(uuid.uuid4())
        start_time = datetime.now()

        # Verify registry is frozen
        if not evidence_registry.is_frozen():
            raise RuntimeError("EvidenceRegistry must be frozen before evaluation")

        # Seed for determinism
        random.seed(42)
        np.random.seed(42)

        # Prepare evaluation tasks - 300 questions (30 base Ã— 10 points)
        tasks = []
        for point in self.thematic_points:
            for base_q in self.base_questions:
                task_id = f"{point.id}-{base_q.id}"
                tasks.append({
                    "task_id": task_id,
                    "point": point,
                    "base_question": base_q,
                })

        # Sort tasks for deterministic ordering
        tasks.sort(key=lambda t: t["task_id"])

        logger.info(f"   ðŸ“‹ Prepared {len(tasks)} evaluation tasks")

        # Execute evaluations consuming evidence
        all_results = []

        for task in tasks:
            point = task["point"]
            base_q = task["base_question"]

            # Get evidence for this question
            question_id = f"{point.id}-{base_q.id}"
            evidence_list = evidence_registry.for_question(question_id)

            # Evaluate using evidence
            result = self._evaluate_question_with_evidence(
                base_question=base_q,
                thematic_point=point,
                evidence_list=evidence_list
            )

            all_results.append(result)

        # Sort results deterministically
        all_results.sort(key=lambda r: r.question_id)

        # Aggregate by dimensions and points
        dimension_scores = self._aggregate_by_dimension(all_results)
        point_scores = self._aggregate_by_point(all_results)
        global_score = self._calculate_global_score(dimension_scores, point_scores)

        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()

        return {
            "metadata": {
                "evaluation_id": evaluation_id,
                "version": self.structure.VERSION,
                "timestamp": start_time.isoformat(),
                "municipality": municipality,
                "department": department,
                "total_evaluations": len(all_results),
                "execution_time_seconds": execution_time,
                "parallel_execution": True,
                "max_workers": max_workers,
                "evidence_consumed": True,
                "evidence_hash": evidence_registry.deterministic_hash()
            },
            "questionnaire_structure": {
                "total_questions": self.structure.TOTAL_QUESTIONS,
                "questions_evaluated": len(all_results),
                "structure_validation": "PASSED"
            },
            "results": {
                "all_questions": [asdict(r) for r in all_results],
                "by_dimension": {dim_id: asdict(dim_score) for dim_id, dim_score in dimension_scores.items()},
                "by_point": {pt_id: asdict(pt_score) for pt_id, pt_score in point_scores.items()},
                "global": asdict(global_score)
            },
            "evidence_statistics": evidence_registry.get_statistics()
        }

    def _evaluate_question_with_evidence(
        self,
        base_question: BaseQuestion,
        thematic_point: ThematicPoint,
        evidence_list: List
    ) -> EvaluationResult:
        """
        Evaluate a single question using evidence from registry

        Args:
            base_question: Base question template
            thematic_point: Thematic point to parametrize
            evidence_list: List of CanonicalEvidence for this question

        Returns:
            EvaluationResult with score and details
        """
        question_id = f"{thematic_point.id}-{base_question.id}"

        # Parametrize question prompt
        prompt = base_question.template.replace("{PUNTO_TEMATICO}", thematic_point.title)

        # Evaluate evidence
        elements_found = {}
        elements_found_count = 0
        evidence_details = []

        for evidence in evidence_list:
            # Check if evidence matches expected elements
            for expected_element in base_question.expected_elements:
                # Simple heuristic: check if evidence type or content matches
                if expected_element in str(evidence.evidence_type).lower() or \
                   expected_element in str(evidence.content).lower():
                    elements_found[expected_element] = True
                    elements_found_count += 1

                    evidence_details.append({
                        "source": evidence.source_component,
                        "type": evidence.evidence_type,
                        "confidence": evidence.confidence,
                        "content_summary": str(evidence.content)[:200]
                    })
                    break  # Count each element only once

        # Calculate score using scoring rule
        expected_count = len(base_question.expected_elements)
        score = self._calculate_score(
            elements_found_count,
            expected_count,
            base_question.scoring_rule
        )

        # Missing elements
        missing = [elem for elem in base_question.expected_elements if elem not in elements_found]

        # Recommendation
        if score >= 2.5:
            recommendation = "Cumple satisfactoriamente"
        elif score >= 1.5:
            recommendation = "Cumple parcialmente, requiere mejoras"
        else:
            recommendation = "No cumple, requiere atenciÃ³n prioritaria"

        return EvaluationResult(
            question_id=question_id,
            point_code=thematic_point.id,
            point_title=thematic_point.title,
            dimension=base_question.dimension,
            question_no=base_question.question_no,
            prompt=prompt,
            score=score,
            max_score=base_question.max_score,
            elements_found=elements_found,
            elements_expected=expected_count,
            elements_found_count=elements_found_count,
            evidence=evidence_details,
            missing_elements=missing,
            recommendation=recommendation,
            scoring_modality=base_question.scoring_rule.modality.value,
            calculation_detail=f"Found {elements_found_count}/{expected_count} elements"
        )

    @staticmethod
    def _calculate_score(found: int, expected: int, rule: ScoringRule) -> float:
        """Calculate score based on scoring modality"""
        if rule.modality == ScoringModality.TYPE_A:
            # (found / expected) Ã— 3
            return round((found / max(1, expected)) * 3.0, 2)
        elif rule.modality == ScoringModality.TYPE_B:
            # min(found, 3)
            return float(min(found, 3))
        elif rule.modality == ScoringModality.TYPE_C:
            # (found / 2) Ã— 3
            return round((found / 2.0) * 3.0, 2)
        elif rule.modality == ScoringModality.TYPE_E:
            # Logical rule
            if found >= expected:
                return 3.0
            elif found > 0:
                return 2.0
            else:
                return 0.0
        else:
            # Default proportional
            return round((found / max(1, expected)) * 3.0, 2)

    @staticmethod
    def _aggregate_by_dimension(results: List[EvaluationResult]) -> Dict[str, DimensionScore]:
        """Aggregate results by dimension"""
        dimensions = {}

        for result in results:
            dim_id = result.dimension
            if dim_id not in dimensions:
                dimensions[dim_id] = {
                    "questions": [],
                    "total_score": 0.0,
                    "total_max": 0.0
                }

            dimensions[dim_id]["questions"].append(result)
            dimensions[dim_id]["total_score"] += result.score
            dimensions[dim_id]["total_max"] += result.max_score

        # Create DimensionScore objects
        dimension_scores = {}
        for dim_id, data in dimensions.items():
            percentage = (data["total_score"] / data["total_max"] * 100) if data["total_max"] > 0 else 0.0

            dimension_scores[dim_id] = DimensionScore(
                dimension_id=dim_id,
                dimension_name=f"DimensiÃ³n {dim_id}",
                score_percentage=round(percentage, 1),
                points_obtained=round(data["total_score"], 2),
                points_maximum=round(data["total_max"], 2),
                questions=data["questions"]
            )

        return dimension_scores

    @staticmethod
    def _aggregate_by_point(results: List[EvaluationResult]) -> Dict[str, PointScore]:
        """Aggregate results by thematic point"""
        points = {}

        for result in results:
            point_id = result.point_code
            if point_id not in points:
                points[point_id] = {
                    "title": result.point_title,
                    "dimensions": {},
                    "total_score": 0.0,
                    "total_max": 0.0
                }

            # Aggregate by dimension within point
            dim_id = result.dimension
            if dim_id not in points[point_id]["dimensions"]:
                points[point_id]["dimensions"][dim_id] = {
                    "questions": [],
                    "score": 0.0,
                    "max": 0.0
                }

            points[point_id]["dimensions"][dim_id]["questions"].append(result)
            points[point_id]["dimensions"][dim_id]["score"] += result.score
            points[point_id]["dimensions"][dim_id]["max"] += result.max_score
            points[point_id]["total_score"] += result.score
            points[point_id]["total_max"] += result.max_score

        # Create PointScore objects
        point_scores = {}
        for point_id, data in points.items():
            percentage = (data["total_score"] / data["total_max"] * 100) if data["total_max"] > 0 else 0.0

            # Create dimension scores for this point
            dim_scores = {}
            for dim_id, dim_data in data["dimensions"].items():
                dim_percentage = (dim_data["score"] / dim_data["max"] * 100) if dim_data["max"] > 0 else 0.0
                dim_scores[dim_id] = DimensionScore(
                    dimension_id=dim_id,
                    dimension_name=f"DimensiÃ³n {dim_id}",
                    score_percentage=round(dim_percentage, 1),
                    points_obtained=round(dim_data["score"], 2),
                    points_maximum=round(dim_data["max"], 2),
                    questions=dim_data["questions"]
                )

            point_scores[point_id] = PointScore(
                point_id=point_id,
                point_title=data["title"],
                score_percentage=round(percentage, 1),
                dimension_scores=dim_scores,
                total_questions=len([q for dim in data["dimensions"].values() for q in dim["questions"]]),
                classification=ScoreBand.classify(percentage)
            )

        return point_scores

    @staticmethod
    def _calculate_global_score(
        dimension_scores: Dict[str, DimensionScore],
        point_scores: Dict[str, PointScore]
    ) -> GlobalScore:
        """Calculate global evaluation score"""
        # Calculate dimension averages
        dim_averages = {
            dim_id: dim_score.score_percentage
            for dim_id, dim_score in dimension_scores.items()
        }

        # Calculate overall percentage
        total_score = sum(ps.dimension_scores[d].points_obtained
                         for ps in point_scores.values()
                         for d in ps.dimension_scores.keys())
        total_max = sum(ps.dimension_scores[d].points_maximum
                       for ps in point_scores.values()
                       for d in ps.dimension_scores.keys())

        overall_percentage = (total_score / total_max * 100) if total_max > 0 else 0.0

        return GlobalScore(
            score_percentage=round(overall_percentage, 1),
            points_evaluated=len(point_scores),
            points_not_applicable=[],
            dimension_averages=dim_averages,
            classification=ScoreBand.classify(overall_percentage),
            validation_passed=True
        )

# ============================================================================
# GLOBAL SINGLETON
# ============================================================================

_questionnaire_engine = None


def get_questionnaire_engine() -> QuestionnaireEngine:
    """Get the global questionnaire engine singleton"""
    global _questionnaire_engine
    if _questionnaire_engine is None:
        _questionnaire_engine = QuestionnaireEngine()
    return _questionnaire_engine


# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    # Initialize and test
    engine = QuestionnaireEngine()

    logger.info("=" * 80)
    logger.info("ðŸŽ¯ QUESTIONNAIRE ENGINE v2.0 - READY")
    logger.info("=" * 80)
    logger.info(
        f"ðŸ“Š Structure: {engine.structure.DOMAINS} points Ã— {engine.structure.QUESTIONS_PER_DOMAIN} questions = {engine.structure.TOTAL_QUESTIONS} total")
    logger.info(f"ðŸ“‹ Questions loaded: {len(engine.base_questions)}")
    logger.info(f"ðŸŽ¯ Thematic points loaded: {len(engine.thematic_points)}")
    logger.info("=" * 80)

    # Test evaluation (with sample orchestrator results)
    logger.info("\nðŸ§ª Running test evaluation...")
    sample_orchestrator_results = {
        "plan_path": "test_pdm.pdf",
        "feasibility": {"has_baseline": False, "detailed_matches": []},
        "metadata": {},
        "monetary": [],
        "responsibilities": [],
        "contradictions": {"total": 0},
        "causal_patterns": [],
        "teoria_cambio": {}
    }
    test_results = engine.execute_full_evaluation(
        orchestrator_results=sample_orchestrator_results,
        municipality="AnorÃ­",
        department="Antioquia"
    )

    # Export results
    output_file = Path("test_evaluation_results.json")
    engine.export_results(test_results, output_file)

    logger.info(f"\nâœ… Test complete! Check {output_file} for results.")
