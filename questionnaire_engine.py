#!/usr/bin/env python3
"""
AUTHORITATIVE QUESTIONNAIRE ENGINE v2.0 - COMPLETE IMPLEMENTATION
"""

import json
import logging
import re
from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union
from enum import Enum
import uuid
from datetime import datetime
import numpy as np

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================================
# ENUMERATIONS AND CONSTANTS
# ============================================================================

class ScoringModality(Enum):
    """Scoring calculation methods"""
    TYPE_A = "count_4_elements"  # (found/4) Ã— 3
    TYPE_B = "count_3_elements"  # min(found, 3)
    TYPE_C = "count_2_elements"  # (found/2) Ã— 3
    TYPE_D = "ratio_quantitative"  # f(ratio) with thresholds
    TYPE_E = "logical_rule"  # if-then-else logic
    TYPE_F = "semantic_analysis"  # cosine similarity


class ScoreBand(Enum):
    """Score interpretation bands"""
    EXCELENTE = (85, 100, "ðŸŸ¢", "DiseÃ±o causal robusto")
    BUENO = (70, 84, "ðŸŸ¡", "DiseÃ±o sÃ³lido con vacÃ­os menores")
    SATISFACTORIO = (55, 69, "ðŸŸ ", "Cumple mÃ­nimos, requiere mejoras")
    INSUFICIENTE = (40, 54, "ðŸ”´", "VacÃ­os crÃ­ticos")
    DEFICIENTE = (0, 39, "âš«", "Ausencia de diseÃ±o causal")

    def __init__(self, min_score, max_score, color, description):
        self.min_score = min_score
        self.max_score = max_score
        self.color = color
        self.description = description

    @classmethod
    def classify(cls, score_percentage: float) -> 'ScoreBand':
        """Classify score into band"""
        for band in cls:
            if band.min_score <= score_percentage <= band.max_score:
                return band
        return cls.DEFICIENTE


# ============================================================================
# DATA STRUCTURES
# ============================================================================

@dataclass
class QuestionnaireStructure:
    """IMMUTABLE: Enforces the 30Ã—10 structure"""
    TOTAL_QUESTIONS: int = 300
    DOMAINS: int = 10  # P1-P10
    QUESTIONS_PER_DOMAIN: int = 30  # 6 dimensions Ã— 5 questions each
    DIMENSIONS: int = 6  # D1-D6
    QUESTIONS_PER_DIMENSION: int = 5
    VERSION: str = "2.0"
    DECIMAL_PRECISION_QUESTION: int = 2
    DECIMAL_PRECISION_DIMENSION: int = 1

    def validate_structure(self) -> bool:
        """Validates that 10 Ã— 30 = 300 questions"""
        return (self.DOMAINS * self.QUESTIONS_PER_DOMAIN == self.TOTAL_QUESTIONS and
                self.DIMENSIONS * self.QUESTIONS_PER_DIMENSION == self.QUESTIONS_PER_DOMAIN)


@dataclass
class ThematicPoint:
    """Represents one of the 10 thematic points (P1-P10)"""
    id: str  # P1, P2, ..., P10
    title: str
    keywords: List[str] = field(default_factory=list)
    hints: List[str] = field(default_factory=list)
    relevant_programs: List[str] = field(default_factory=list)
    pdm_sections: List[str] = field(default_factory=list)


@dataclass
class SearchPattern:
    """Search pattern for evidence detection"""
    pattern_type: str  # "regex", "table", "semantic", "logical"
    pattern: Union[str, Dict]
    description: str


@dataclass
class ScoringRule:
    """Scoring rule specification"""
    modality: ScoringModality
    formula: str
    thresholds: Optional[Dict[str, float]] = None


@dataclass
class BaseQuestion:
    """Base question that gets parametrized for each thematic point"""
    id: str  # D1-Q1, D1-Q2, etc.
    dimension: str  # D1, D2, D3, D4, D5, D6
    question_no: int  # 1-30
    template: str  # Question template with {PUNTO_TEMATICO} placeholder
    search_patterns: Dict[str, SearchPattern]
    scoring_rule: ScoringRule
    max_score: float = 3.0
    expected_elements: List[str] = field(default_factory=list)


@dataclass
class EvaluationResult:
    """Result of evaluating one question for one thematic point"""
    question_id: str  # P1-D1-Q1, P2-D1-Q1, etc.
    point_code: str  # P1, P2, etc.
    point_title: str
    dimension: str
    question_no: int
    prompt: str  # Fully parametrized question
    score: float
    max_score: float
    elements_found: Dict[str, bool]
    elements_expected: int
    elements_found_count: int
    evidence: List[Dict[str, Any]]
    missing_elements: List[str]
    recommendation: str
    scoring_modality: str
    calculation_detail: str


@dataclass
class DimensionScore:
    """Aggregated score for one dimension"""
    dimension_id: str
    dimension_name: str
    score_percentage: float
    points_obtained: float
    points_maximum: float
    questions: List[EvaluationResult]


@dataclass
class PointScore:
    """Aggregated score for one thematic point"""
    point_id: str
    point_title: str
    score_percentage: float
    dimension_scores: Dict[str, DimensionScore]
    total_questions: int
    classification: ScoreBand


@dataclass
class GlobalScore:
    """Complete evaluation summary"""
    score_percentage: float
    points_evaluated: int
    points_not_applicable: List[str]
    dimension_averages: Dict[str, float]
    classification: ScoreBand
    validation_passed: bool


# ============================================================================
# COMPLETE QUESTION DEFINITIONS
# ============================================================================

class QuestionLibrary:
    """Complete library of 30 base questions with full specifications"""

    @staticmethod
    def get_all_questions() -> List[BaseQuestion]:
        """Returns all 30 questions fully specified"""

        questions = []

        # ====================================================================
        # DIMENSION D1: DIAGNÃ“STICO Y RECURSOS (Q1-Q5)
        # ====================================================================

        questions.append(BaseQuestion(
            id="D1-Q1",
            dimension="D1",
            question_no=1,
            template="Â¿El diagnÃ³stico presenta lÃ­neas base con fuentes, series temporales, unidades, cobertura y mÃ©todo de mediciÃ³n para {PUNTO_TEMATICO}?",
            search_patterns={
                "valor_numerico": SearchPattern(
                    pattern_type="regex",
                    pattern=r"\d+[.,]?\d*\s*(%|casos|personas|tasa|porcentaje|Ã­ndice)",
                    description="Buscar valor numÃ©rico con unidad"
                ),
                "aÃ±o": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(20\d{2}|periodo|aÃ±o|vigencia)",
                    description="Buscar aÃ±o o perÃ­odo de mediciÃ³n"
                ),
                "fuente": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(fuente:|segÃºn|DANE|Ministerio|SecretarÃ­a|Encuesta|Censo|SISBEN|SIVIGILA|\(20\d{2}\))",
                    description="Buscar fuente de datos identificada"
                ),
                "serie_temporal": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(20\d{2}.{0,50}20\d{2}|serie|histÃ³rico|evoluciÃ³n|tendencia)",
                    description="Buscar serie temporal o datos histÃ³ricos"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_A,
                formula="(elementos_encontrados / 4) Ã— 3"
            ),
            expected_elements=["valor_numerico", "aÃ±o", "fuente", "serie_temporal"]
        ))

        questions.append(BaseQuestion(
            id="D1-Q2",
            dimension="D1",
            question_no=2,
            template="Â¿Las lÃ­neas base capturan la magnitud del problema y los vacÃ­os de informaciÃ³n, explicitando sesgos, supuestos y calidad de datos para {PUNTO_TEMATICO}?",
            search_patterns={
                "poblacion_afectada": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(\d+\s*(personas|habitantes|casos|familias|mujeres|niÃ±os)|poblaciÃ³n.*\d+|afectados.*\d+)",
                    description="PoblaciÃ³n afectada cuantificada"
                ),
                "brecha_deficit": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(brecha|dÃ©ficit|diferencia|faltante|carencia|necesidad insatisfecha).{0,30}\d+",
                    description="Brecha o dÃ©ficit calculado"
                ),
                "vacios_info": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(sin datos|no.*disponible|vacÃ­o|falta.*informaciÃ³n|se requiere.*datos|limitaciÃ³n.*informaciÃ³n|no se cuenta con)",
                    description="VacÃ­os de informaciÃ³n reconocidos"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["poblacion_afectada", "brecha_deficit", "vacios_info"]
        ))

        questions.append(BaseQuestion(
            id="D1-Q3",
            dimension="D1",
            question_no=3,
            template="Â¿Los recursos del PPI/Plan Indicativo estÃ¡n asignados explÃ­citamente a {PUNTO_TEMATICO}, con trazabilidad programÃ¡tica y suficiencia relativa a la brecha?",
            search_patterns={
                "presupuesto_total": SearchPattern(
                    pattern_type="regex",
                    pattern=r"\$\s*\d+([.,]\d+)?\s*(millones|miles de millones|mil|COP|pesos)",
                    description="Presupuesto total identificado"
                ),
                "desglose": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(20\d{2}.*\$|Producto.*\$|Meta.*\$|anual|vigencia.*presupuesto|por aÃ±o)",
                    description="Desglose temporal o por producto"
                ),
                "trazabilidad": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(Programa.{0,50}(inversiÃ³n|presupuesto|recursos))",
                    description="Trazabilidad programa-presupuesto"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["presupuesto_total", "desglose", "trazabilidad"]
        ))

        questions.append(BaseQuestion(
            id="D1-Q4",
            dimension="D1",
            question_no=4,
            template="Â¿Las capacidades institucionales (talento, procesos, datos, gobernanza) necesarias para activar los mecanismos causales en {PUNTO_TEMATICO} estÃ¡n descritas con sus cuellos de botella?",
            search_patterns={
                "recursos_humanos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(profesionales|tÃ©cnicos|funcionarios|personal|equipo|contrataciÃ³n|psicÃ³logo|trabajador social|profesional).{0,50}\d+|se requiere.*personal|brecha.*talento",
                    description="Recursos humanos mencionados"
                ),
                "infraestructura": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(infraestructura|equipamiento|sede|oficina|espacios|dotaciÃ³n|vehÃ­culos|adecuaciÃ³n)",
                    description="Infraestructura/equipamiento mencionado"
                ),
                "procesos_instancias": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(SecretarÃ­a|ComisarÃ­a|ComitÃ©|Mesa|Consejo|Sistema|procedimiento|protocolo|ruta|proceso de)",
                    description="Procesos o instancias institucionales"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["recursos_humanos", "infraestructura", "procesos_instancias"]
        ))

        questions.append(BaseQuestion(
            id="D1-Q5",
            dimension="D1",
            question_no=5,
            template="Â¿Existe coherencia entre objetivos, recursos y capacidades para {PUNTO_TEMATICO}, con restricciones legales, presupuestales y temporales modeladas?",
            search_patterns={
                "presupuesto_presente": SearchPattern(
                    pattern_type="logical",
                    pattern={"check": "presupuesto > 0"},
                    description="Verificar existencia de presupuesto"
                ),
                "productos_definidos": SearchPattern(
                    pattern_type="logical",
                    pattern={"check": "num_productos > 0"},
                    description="Verificar existencia de productos definidos"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_E,
                formula="if presupuesto > 0 and num_productos > 0: 3 elif presupuesto > 0: 2 else: 0"
            ),
            expected_elements=["presupuesto_presente", "productos_definidos"]
        ))

        # ====================================================================
        # DIMENSION D2: DISEÃ‘O DE INTERVENCIÃ“N (Q6-Q10)
        # ====================================================================

        questions.append(BaseQuestion(
            id="D2-Q6",
            dimension="D2",
            question_no=6,
            template="Â¿Las actividades para {PUNTO_TEMATICO} estÃ¡n formalizadas en tablas (responsable, insumo, output, calendario, costo unitario) y no sÃ³lo en narrativa?",
            search_patterns={
                "tabla_productos": SearchPattern(
                    pattern_type="table",
                    pattern={"columns": ["Producto", "Meta", "Unidad", "Responsable"]},
                    description="Detectar tabla con columnas requeridas"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_A,
                formula="(columnas_encontradas / 4) Ã— 3"
            ),
            expected_elements=["Producto", "Meta", "Unidad", "Responsable"]
        ))

        questions.append(BaseQuestion(
            id="D2-Q7",
            dimension="D2",
            question_no=7,
            template="Â¿Cada actividad especifica el instrumento y su mecanismo causal pretendido y la poblaciÃ³n diana en {PUNTO_TEMATICO}?",
            search_patterns={
                "poblacion_objetivo": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(mujeres|niÃ±os|niÃ±as|adolescentes|jÃ³venes|vÃ­ctimas|familias|comunidad|poblaciÃ³n|adultos mayores|personas con discapacidad)",
                    description="PoblaciÃ³n objetivo nombrada"
                ),
                "cuantificacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"\d+\s*(personas|beneficiarios|familias|atenciones|servicios|participantes)",
                    description="CuantificaciÃ³n de beneficiarios"
                ),
                "focalizacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(zona rural|urbano|cabecera|prioridad|vulnerable|focalizaciÃ³n|criterios|selecciÃ³n|poblaciÃ³n objetivo)",
                    description="Criterios de focalizaciÃ³n"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["poblacion_objetivo", "cuantificacion", "focalizacion"]
        ))

        questions.append(BaseQuestion(
            id="D2-Q8",
            dimension="D2",
            question_no=8,
            template="Â¿Cada problema priorizado en {PUNTO_TEMATICO} tiene al menos una actividad que ataca el eslabÃ³n causal relevante (causa raÃ­z o mediador)?",
            search_patterns={
                "problemas_diagnostico": SearchPattern(
                    pattern_type="semantic",
                    pattern={"extract": "problems_from_diagnosis"},
                    description="Extraer problemas del diagnÃ³stico"
                ),
                "productos_tabla": SearchPattern(
                    pattern_type="semantic",
                    pattern={"extract": "products_from_table"},
                    description="Extraer productos de tabla"
                ),
                "matching": SearchPattern(
                    pattern_type="semantic",
                    pattern={"method": "cosine_similarity", "threshold": 0.6},
                    description="Matching semÃ¡ntico problema-producto"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_F,
                formula="ratio >= 0.80: 3, >= 0.50: 2, >= 0.30: 1, else: 0",
                thresholds={"high": 0.80, "medium": 0.50, "low": 0.30}
            ),
            expected_elements=["problemas_diagnostico", "productos_tabla"]
        ))

        questions.append(BaseQuestion(
            id="D2-Q9",
            dimension="D2",
            question_no=9,
            template="Â¿Se identifican riesgos de desplazamiento de efectos, cuÃ±as de implementaciÃ³n y conflictos entre actividades en {PUNTO_TEMATICO}, con mitigaciones?",
            search_patterns={
                "riesgos_explicitos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(riesgo|limitaciÃ³n|restricciÃ³n|dificultad|cuello de botella|matriz.*riesgo|desafÃ­o|obstÃ¡culo)",
                    description="MenciÃ³n explÃ­cita de riesgos"
                ),
                "factores_externos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(depende|articulaciÃ³n|coordinaciÃ³n|transversal|nivel nacional|competencia de|requiere apoyo|sujeto a)",
                    description="Factores externos reconocidos"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["riesgos_explicitos", "factores_externos"]
        ))

        questions.append(BaseQuestion(
            id="D2-Q10",
            dimension="D2",
            question_no=10,
            template="Â¿Las actividades de {PUNTO_TEMATICO} forman una teorÃ­a de intervenciÃ³n coherente (complementariedades, secuenciaciÃ³n, no redundancias)?",
            search_patterns={
                "integracion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(articulaciÃ³n|complementa|sinergia|coordinaciÃ³n|integra|transversal|en conjunto|simultÃ¡neamente)",
                    description="TÃ©rminos de integraciÃ³n"
                ),
                "referencia_cruzada": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(programa de|articulado con|en el marco de|junto con|ademÃ¡s de)",
                    description="Referencias cruzadas a otros programas"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["integracion", "referencia_cruzada"]
        ))

        # ====================================================================
        # DIMENSION D3: PRODUCTOS (Q11-Q15)
        # ====================================================================

        questions.append(BaseQuestion(
            id="D3-Q11",
            dimension="D3",
            question_no=11,
            template="Â¿Se mencionan estÃ¡ndares tÃ©cnicos o protocolos para los productos de {PUNTO_TEMATICO}?",
            search_patterns={
                "estandares": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(norma|estÃ¡ndar|protocolo|lineamiento|guÃ­a|directriz|NTC|ISO|ResoluciÃ³n|Decreto|Ley|segÃºn el Ministerio)",
                    description="Normas/estÃ¡ndares/protocolos"
                ),
                "control_calidad": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(certificaciÃ³n|acreditaciÃ³n|supervisiÃ³n|verificaciÃ³n|control de calidad|seguimiento|auditorÃ­a)",
                    description="SupervisiÃ³n/verificaciÃ³n"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["estandares", "control_calidad"]
        ))

        questions.append(BaseQuestion(
            id="D3-Q12",
            dimension="D3",
            question_no=12,
            template="Â¿La meta de productos es proporcional a la magnitud del problema en {PUNTO_TEMATICO}?",
            search_patterns={
                "magnitud_problema": SearchPattern(
                    pattern_type="quantitative",
                    pattern={"source": "Q2", "field": "poblacion_afectada"},
                    description="Extraer magnitud del problema"
                ),
                "meta_producto": SearchPattern(
                    pattern_type="quantitative",
                    pattern={"source": "tabla_productos", "field": "meta"},
                    description="Extraer meta de producto principal"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_D,
                formula="ratio >= 0.50: 3, >= 0.20: 2, >= 0.05: 1, else: 0",
                thresholds={"high": 0.50, "medium": 0.20, "low": 0.05}
            ),
            expected_elements=["magnitud_problema", "meta_producto"]
        ))

        questions.append(BaseQuestion(
            id="D3-Q13",
            dimension="D3",
            question_no=13,
            template="Â¿Las metas de productos en {PUNTO_TEMATICO} estÃ¡n cuantificadas?",
            search_patterns={
                "meta_numerica": SearchPattern(
                    pattern_type="regex",
                    pattern=r"\d+",
                    description="Meta numÃ©rica presente"
                ),
                "desagregacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(\d+.{0,20}(rural|urbano|2024|2025|2026|2027|hombres|mujeres|aÃ±o|anual))",
                    description="DesagregaciÃ³n de meta"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["meta_numerica", "desagregacion"]
        ))

        questions.append(BaseQuestion(
            id="D3-Q14",
            dimension="D3",
            question_no=14,
            template="Â¿Cada producto en {PUNTO_TEMATICO} tiene una dependencia responsable asignada?",
            search_patterns={
                "productos_tabla": SearchPattern(
                    pattern_type="table",
                    pattern={"extract_column": "Responsable"},
                    description="Extraer columna Responsable de tabla"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_D,
                formula="ratio >= 0.90: 3, >= 0.70: 2, >= 0.40: 1, else: 0",
                thresholds={"high": 0.90, "medium": 0.70, "low": 0.40}
            ),
            expected_elements=["productos_tabla"]
        ))

        questions.append(BaseQuestion(
            id="D3-Q15",
            dimension="D3",
            question_no=15,
            template="Â¿Los productos de {PUNTO_TEMATICO} tienen justificaciÃ³n causal (relaciÃ³n con resultados esperados)?",
            search_patterns={
                "terminos_causales": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(para|con el fin de|contribuye a|permite|lograr|reducir|aumentar)",
                    description="TÃ©rminos causales en contexto de productos"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_D,
                formula="ratio >= 0.70: 3, >= 0.40: 2, >= 0.20: 1, else: 0",
                thresholds={"high": 0.70, "medium": 0.40, "low": 0.20}
            ),
            expected_elements=["terminos_causales"]
        ))

        # ====================================================================
        # DIMENSION D4: RESULTADOS (Q16-Q20)
        # ====================================================================

        questions.append(BaseQuestion(
            id="D4-Q16",
            dimension="D4",
            question_no=16,
            template="Â¿Existe un indicador de resultado formalizado para {PUNTO_TEMATICO}?",
            search_patterns={
                "nombre_indicador": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(Tasa|Porcentaje|Ãndice|Cobertura|Prevalencia|Incidencia).{0,100}(resultado|impacto)",
                    description="Nombre del indicador de resultado"
                ),
                "linea_base": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(lÃ­nea base|LB|valor inicial|20(21|22|23)).*\d+",
                    description="LÃ­nea base numÃ©rica"
                ),
                "meta": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(meta|valor esperado|20(27|28)).*\d+",
                    description="Meta cuatrienio"
                ),
                "fuente": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(fuente|segÃºn|mediciÃ³n|DANE|Ministerio|SecretarÃ­a)",
                    description="Fuente del indicador"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_A,
                formula="(elementos_encontrados / 4) Ã— 3"
            ),
            expected_elements=["nombre_indicador", "linea_base", "meta", "fuente"]
        ))

        questions.append(BaseQuestion(
            id="D4-Q17",
            dimension="D4",
            question_no=17,
            template="Â¿El indicador de resultado de {PUNTO_TEMATICO} es diferente de los indicadores de producto?",
            search_patterns={
                "indicador_resultado": SearchPattern(
                    pattern_type="semantic",
                    pattern={"check_not": ["nÃºmero de", "cantidad de", "talleres realizados", "servicios prestados"]},
                    description="Verificar que no es indicador de gestiÃ³n"
                ),
                "indicador_resultado_valido": SearchPattern(
                    pattern_type="semantic",
                    pattern={"check": ["tasa", "porcentaje", "cobertura", "reducciÃ³n", "aumento"]},
                    description="Verificar que es indicador de resultado"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_E,
                formula="if es_gestion: 0 elif es_resultado: 3 else: 1"
            ),
            expected_elements=["indicador_resultado", "indicador_resultado_valido"]
        ))

        questions.append(BaseQuestion(
            id="D4-Q18",
            dimension="D4",
            question_no=18,
            template="Â¿CuÃ¡l es la magnitud del cambio esperado en el indicador de resultado de {PUNTO_TEMATICO}?",
            search_patterns={
                "linea_base_valor": SearchPattern(
                    pattern_type="quantitative",
                    pattern={"source": "Q16", "field": "linea_base"},
                    description="Valor de lÃ­nea base"
                ),
                "meta_valor": SearchPattern(
                    pattern_type="quantitative",
                    pattern={"source": "Q16", "field": "meta"},
                    description="Valor de meta"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_E,
                formula="abs(cambio%) >= 20: 3, >= 10: 2, >= 5: 1, else: 0",
                thresholds={"high": 20, "medium": 10, "low": 5}
            ),
            expected_elements=["linea_base_valor", "meta_valor"]
        ))

        questions.append(BaseQuestion(
            id="D4-Q19",
            dimension="D4",
            question_no=19,
            template="Â¿Se reconoce que el resultado en {PUNTO_TEMATICO} depende de mÃºltiples factores?",
            search_patterns={
                "contribucion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(contribuye|aporta|incide|influye|favorece|apoya)",
                    description="TÃ©rminos de contribuciÃ³n (no causalidad directa)"
                ),
                "factores_externos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(tambiÃ©n|otros programas|nivel nacional|articulaciÃ³n|transversal|depende|conjunto de)",
                    description="Factores externos mencionados"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["contribucion", "factores_externos"]
        ))

        questions.append(BaseQuestion(
            id="D4-Q20",
            dimension="D4",
            question_no=20,
            template="Â¿Se especifica cÃ³mo se monitorearÃ¡ el indicador de resultado de {PUNTO_TEMATICO}?",
            search_patterns={
                "frecuencia": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(anual|semestral|trimestral|mensual|periÃ³dic|seguimiento|mediciÃ³n)",
                    description="Frecuencia de mediciÃ³n"
                ),
                "responsable": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(SecretarÃ­a|DirecciÃ³n|Oficina|Dependencia).{0,50}(responsable|encargado|mediciÃ³n|seguimiento)",
                    description="Responsable de mediciÃ³n"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["frecuencia", "responsable"]
        ))

        # ====================================================================
        # DIMENSION D5: IMPACTOS (Q21-Q25)
        # ====================================================================

        questions.append(BaseQuestion(
            id="D5-Q21",
            dimension="D5",
            question_no=21,
            template="Â¿Existe un indicador de impacto de largo plazo para {PUNTO_TEMATICO}?",
            search_patterns={
                "seccion_impacto": SearchPattern(
                    pattern_type="semantic",
                    pattern={"check_section": ["Impacto", "Componente"]},
                    description="SecciÃ³n de impacto presente"
                ),
                "referencia_ODS": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(ODS|Objetivo.*Desarrollo Sostenible|CONPES|PND)",
                    description="Referencias a ODS/CONPES/PND"
                ),
                "mencion_impacto": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(impacto de largo plazo|impacto socioeconÃ³mico)",
                    description="MenciÃ³n narrativa de impacto"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_E,
                formula="if seccion_impacto and indicadores: 3 elif referencia_ODS: 2 elif mencion: 1 else: 0"
            ),
            expected_elements=["seccion_impacto", "referencia_ODS", "mencion_impacto"]
        ))

        questions.append(BaseQuestion(
            id="D5-Q22",
            dimension="D5",
            question_no=22,
            template="Â¿Se menciona el horizonte temporal de los impactos en {PUNTO_TEMATICO}?",
            search_patterns={
                "plazos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(corto plazo|mediano plazo|largo plazo|\d+ aÃ±os|mÃ¡s de \d+ aÃ±os)",
                    description="Plazos mencionados"
                ),
                "post_cuatrienio": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(mÃ¡s allÃ¡|sostenibilidad|continuidad|despuÃ©s de 20\d{2}|posterior)",
                    description="Efectos post-cuatrienio"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["plazos", "post_cuatrienio"]
        ))

        questions.append(BaseQuestion(
            id="D5-Q23",
            dimension="D5",
            question_no=23,
            template="Â¿Se mencionan efectos indirectos o sistÃ©micos en {PUNTO_TEMATICO}?",
            search_patterns={
                "indirectos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(efecto.{0,20}indirecto|multiplicador|cascada|secundario|colateral|derivado)",
                    description="Efectos indirectos"
                ),
                "sistemico": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(transformaciÃ³n|cambio cultural|normas sociales|institucional|estructural)",
                    description="Cambio sistÃ©mico"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["indirectos", "sistemico"]
        ))

        questions.append(BaseQuestion(
            id="D5-Q24",
            dimension="D5",
            question_no=24,
            template="Â¿Se menciona la sostenibilidad de las acciones en {PUNTO_TEMATICO}?",
            search_patterns={
                "sostenibilidad": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(sostenibilidad|sostenible|permanente|continuidad|perdurable)",
                    description="TÃ©rmino sostenibilidad"
                ),
                "institucionalizacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(institucionalizaciÃ³n|Acuerdo Municipal|Ordenanza|creaciÃ³n de|fortalecimiento permanente)",
                    description="InstitucionalizaciÃ³n"
                ),
                "financiamiento": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(cofinanciaciÃ³n|recursos futuros|alianzas|convenio|fuentes de financiamiento)",
                    description="Financiamiento futuro"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["sostenibilidad", "institucionalizacion", "financiamiento"]
        ))

        questions.append(BaseQuestion(
            id="D5-Q25",
            dimension="D5",
            question_no=25,
            template="Â¿Se menciona un enfoque diferencial o de equidad en {PUNTO_TEMATICO}?",
            search_patterns={
                "equidad": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(enfoque diferencial|equidad|inclusiÃ³n|priorizaciÃ³n|vulnerable|diversidad)",
                    description="TÃ©rminos de equidad"
                ),
                "desagregacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(rural|urbano|Ã©tnico|indÃ­gena|afro|negro|raizal|gÃ©nero|edad|discapacidad|LGTBIQ)",
                    description="DesagregaciÃ³n por grupos"
                ),
                "focalizacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(focalizaciÃ³n|criterios de selecciÃ³n|priorizando|preferencia)",
                    description="FocalizaciÃ³n progresiva"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["equidad", "desagregacion", "focalizacion"]
        ))

        # ====================================================================
        # DIMENSION D6: LÃ“GICA CAUSAL INTEGRAL (Q26-Q30)
        # ====================================================================

        questions.append(BaseQuestion(
            id="D6-Q26",
            dimension="D6",
            question_no=26,
            template="Â¿Existe un diagrama o narrativa de teorÃ­a de cambio para {PUNTO_TEMATICO}?",
            search_patterns={
                "diagrama": SearchPattern(
                    pattern_type="semantic",
                    pattern={
                        "detect_image": ["insumos", "productos", "resultados", "impactos", "teorÃ­a", "marco lÃ³gico"]},
                    description="Diagrama de teorÃ­a de cambio"
                ),
                "narrativa_causal": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(teorÃ­a de cambio|marco lÃ³gico|cadena causal|modelo de intervenciÃ³n)",
                    description="Narrativa de teorÃ­a de cambio"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_E,
                formula="if diagrama: 3 elif narrativa: 2 else: 0"
            ),
            expected_elements=["diagrama", "narrativa_causal"]
        ))

        questions.append(BaseQuestion(
            id="D6-Q27",
            dimension="D6",
            question_no=27,
            template="Â¿Se mencionan supuestos o condiciones necesarias para {PUNTO_TEMATICO}?",
            search_patterns={
                "supuestos": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(supuesto|condiciÃ³n|si.{0,30}entonces|siempre que|requiere que|asumiendo)",
                    description="TÃ©rmino supuesto o condiciÃ³n"
                ),
                "factores_necesarios": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(depende de|necesario que|en tanto|contexto favorable|apoyo de|voluntad polÃ­tica)",
                    description="Factores externos necesarios"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["supuestos", "factores_necesarios"]
        ))

        questions.append(BaseQuestion(
            id="D6-Q28",
            dimension="D6",
            question_no=28,
            template="Â¿Se identifican los niveles del modelo lÃ³gico (insumosâ†’productosâ†’resultadosâ†’impactos) para {PUNTO_TEMATICO}?",
            search_patterns={
                "niveles": SearchPattern(
                    pattern_type="semantic",
                    pattern={"detect_levels": ["Insumos", "Actividades", "Productos", "Resultados", "Impactos"]},
                    description="Niveles del modelo lÃ³gico"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_E,
                formula="niveles >= 4: 3, == 3: 2, == 2: 1, else: 0"
            ),
            expected_elements=["niveles"]
        ))

        questions.append(BaseQuestion(
            id="D6-Q29",
            dimension="D6",
            question_no=29,
            template="Â¿Se menciona un sistema de seguimiento para {PUNTO_TEMATICO}?",
            search_patterns={
                "instancia": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(Consejo de Gobierno|ComitÃ© de seguimiento|Sistema de evaluaciÃ³n|Mesa tÃ©cnica)",
                    description="Instancia de seguimiento"
                ),
                "frecuencia": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(trimestral|semestral|anual|periÃ³dico|cada \d+ meses)",
                    description="Frecuencia de seguimiento"
                ),
                "ajustes": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(ajuste|correcciÃ³n|revisiÃ³n|modificaciÃ³n|actualizaciÃ³n|reformulaciÃ³n)",
                    description="Ajustes/correcciones mencionados"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_B,
                formula="min(elementos_encontrados, 3)"
            ),
            expected_elements=["instancia", "frecuencia", "ajustes"]
        ))

        questions.append(BaseQuestion(
            id="D6-Q30",
            dimension="D6",
            question_no=30,
            template="Â¿Se menciona evaluaciÃ³n o documentaciÃ³n de aprendizajes en {PUNTO_TEMATICO}?",
            search_patterns={
                "evaluacion": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(evaluaciÃ³n|medio tÃ©rmino|lÃ­nea final|estudio de impacto|evaluaciÃ³n externa)",
                    description="EvaluaciÃ³n planificada"
                ),
                "aprendizaje": SearchPattern(
                    pattern_type="regex",
                    pattern=r"(sistematizaciÃ³n|lecciones aprendidas|documentaciÃ³n|buenas prÃ¡cticas|mejora continua)",
                    description="SistematizaciÃ³n/aprendizaje"
                )
            },
            scoring_rule=ScoringRule(
                modality=ScoringModality.TYPE_C,
                formula="(elementos_encontrados / 2) Ã— 3"
            ),
            expected_elements=["evaluacion", "aprendizaje"]
        ))

        return questions


# ============================================================================
# SCORING ENGINE
# ============================================================================

class ScoringEngine:
    """Complete scoring system with 6 modalities"""

    @staticmethod
    def calculate_score(modality: ScoringModality, elements_found: Dict[str, bool],
                        formula: str, thresholds: Optional[Dict] = None,
                        quantitative_data: Optional[Dict] = None) -> Tuple[float, str]:
        """
        Calculate score based on modality

        Returns:
            (score, calculation_detail)
        """

        found_count = sum(1 for v in elements_found.values() if v)
        total_elements = len(elements_found)

        if modality == ScoringModality.TYPE_A:
            # (found / 4) Ã— 3
            score = (found_count / 4) * 3.0
            detail = f"({found_count}/4) Ã— 3 = {score:.2f}"

        elif modality == ScoringModality.TYPE_B:
            # min(found, 3)
            score = min(found_count, 3)
            detail = f"min({found_count}, 3) = {score:.2f}"

        elif modality == ScoringModality.TYPE_C:
            # (found / 2) Ã— 3
            score = (found_count / 2) * 3.0
            detail = f"({found_count}/2) Ã— 3 = {score:.2f}"

        elif modality == ScoringModality.TYPE_D:
            # Ratio-based scoring
            if quantitative_data:
                ratio = quantitative_data.get('ratio', 0)
                if thresholds:
                    if ratio >= thresholds.get('high', 0.50):
                        score = 3.0
                    elif ratio >= thresholds.get('medium', 0.20):
                        score = 2.0
                    elif ratio >= thresholds.get('low', 0.05):
                        score = 1.0
                    else:
                        score = 0.0
                    detail = f"ratio={ratio:.2%} â†’ score={score:.2f}"
                else:
                    score = 0.0
                    detail = "No thresholds defined"
            else:
                score = 0.0
                detail = "No quantitative data available"

        elif modality == ScoringModality.TYPE_E:
            # Logical rule-based scoring
            score = 0.0
            detail = "Logical evaluation needed"
            # This requires custom logic per question

        elif modality == ScoringModality.TYPE_F:
            # Semantic matching
            if quantitative_data:
                ratio = quantitative_data.get('coverage_ratio', 0)
                if ratio >= 0.80:
                    score = 3.0
                elif ratio >= 0.50:
                    score = 2.0
                elif ratio >= 0.30:
                    score = 1.0
                else:
                    score = 0.0
                detail = f"coverage_ratio={ratio:.2%} â†’ score={score:.2f}"
            else:
                score = 0.0
                detail = "No semantic matching data"
        else:
            score = 0.0
            detail = "Unknown modality"

        return round(score, 2), detail

    @staticmethod
    def aggregate_dimension_score(questions: List[EvaluationResult]) -> float:
        """
        Aggregate 5 questions into dimension score (0-100%)

        Formula: (sum_scores / 15) Ã— 100
        """
        total_score = sum(q.score for q in questions)
        max_possible = len(questions) * 3.0  # Should be 15

        if max_possible == 0:
            return 0.0

        percentage = (total_score / max_possible) * 100
        return round(percentage, 1)

    @staticmethod
    def aggregate_point_score(dimension_scores: Dict[str, float]) -> float:
        """
        Aggregate 6 dimensions into point score (0-100%)

        Formula: sum(dimension_scores) / 6
        """
        if not dimension_scores:
            return 0.0

        total = sum(dimension_scores.values())
        average = total / len(dimension_scores)
        return round(average, 1)

    @staticmethod
    def aggregate_global_score(point_scores: List[float], exclude_na: bool = True) -> float:
        """
        Aggregate all points into global score (0-100%)

        Formula: sum(point_scores) / count
        """
        if not point_scores:
            return 0.0

        # Filter out N/A if requested
        valid_scores = [s for s in point_scores if s is not None and s != "N/A"]

        if not valid_scores:
            return 0.0

        total = sum(valid_scores)
        average = total / len(valid_scores)
        return round(average, 1)


# ============================================================================
# MAIN QUESTIONNAIRE ENGINE (UPDATED)
# ============================================================================

class QuestionnaireEngine:
    """
    COMPLETE IMPLEMENTATION: Enforces strict 30Ã—10 structure with full scoring
    """

    def __init__(self):
        """Initialize with complete question library"""
        self.structure = QuestionnaireStructure()
        if not self.structure.validate_structure():
            raise ValueError("CRITICAL: Questionnaire structure validation FAILED")

        self.base_questions = QuestionLibrary.get_all_questions()
        self.thematic_points = self._load_thematic_points()
        self.scoring_engine = ScoringEngine()

        # Validate exact counts
        if len(self.base_questions) != 30:
            raise ValueError(f"CRITICAL: Must have exactly 30 base questions, got {len(self.base_questions)}")
        if len(self.thematic_points) != 10:
            raise ValueError(f"CRITICAL: Must have exactly 10 thematic points, got {len(self.thematic_points)}")

        logger.info("âœ… QuestionnaireEngine v2.0 initialized with COMPLETE 30Ã—10 structure")
        logger.info(f"   ðŸ“‹ {len(self.base_questions)} base questions loaded")
        logger.info(f"   ðŸŽ¯ {len(self.thematic_points)} thematic points loaded")

    def _load_thematic_points(self) -> List[ThematicPoint]:
        """Load the 10 thematic points from the authoritative JSON"""

        json_path = Path(__file__).parent / "decalogo_industrial.json"

        if not json_path.exists():
            logger.warning(f"JSON file not found: {json_path}. Creating default points.")
            return self._create_default_points()

        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Extract unique thematic points
            points_dict = {}
            for question in data.get('questions', []):
                point_code = question.get('point_code')
                if point_code and point_code not in points_dict:
                    points_dict[point_code] = ThematicPoint(
                        id=point_code,
                        title=question.get('point_title', ''),
                        keywords=[],
                        hints=question.get('hints', [])
                    )

            # Convert to list and sort by ID
            points = list(points_dict.values())
            points.sort(key=lambda p: int(p.id[1:]))

            if len(points) != 10:
                logger.warning(f"Expected 10 points, found {len(points)}. Using defaults.")
                return self._create_default_points()

            return points

        except Exception as e:
            logger.error(f"Failed to load thematic points: {e}. Using defaults.")
            return self._create_default_points()

    def _create_default_points(self) -> List[ThematicPoint]:
        """Create default 10 thematic points"""
        return [
            ThematicPoint(id="P1", title="Derechos de las mujeres e igualdad de gÃ©nero",
                          keywords=["mujer", "gÃ©nero", "violencia"], hints=[]),
            ThematicPoint(id="P2", title="PrevenciÃ³n de la violencia y protecciÃ³n frente al conflicto",
                          keywords=["conflicto", "violencia", "paz"], hints=[]),
            ThematicPoint(id="P3", title="Ambiente sano, cambio climÃ¡tico, prevenciÃ³n y atenciÃ³n a desastres",
                          keywords=["ambiente", "clima", "desastre"], hints=[]),
            ThematicPoint(id="P4", title="Derechos econÃ³micos, sociales y culturales",
                          keywords=["educaciÃ³n", "salud", "cultura"], hints=[]),
            ThematicPoint(id="P5", title="Derechos de las vÃ­ctimas y construcciÃ³n de paz",
                          keywords=["vÃ­ctimas", "paz", "reparaciÃ³n"], hints=[]),
            ThematicPoint(id="P6", title="Derecho al buen futuro de la niÃ±ez, adolescencia, juventud",
                          keywords=["niÃ±ez", "juventud", "adolescencia"], hints=[]),
            ThematicPoint(id="P7", title="Tierras y territorios", keywords=["tierra", "territorio", "rural"], hints=[]),
            ThematicPoint(id="P8", title="LÃ­deres y defensores de derechos humanos",
                          keywords=["lÃ­deres", "defensores", "DDHH"], hints=[]),
            ThematicPoint(id="P9", title="Crisis de derechos de personas privadas de la libertad",
                          keywords=["cÃ¡rcel", "privados", "libertad"], hints=[]),
            ThematicPoint(id="P10", title="MigraciÃ³n transfronteriza", keywords=["migraciÃ³n", "frontera", "migrante"],
                          hints=[])
        ]

    def execute_full_evaluation(
        self,
        orchestrator_results: Dict[str, Any],
        municipality: str = "",
        department: str = ""
    ) -> Dict[str, Any]:
        """
        âœ… VERSIÃ“N ACTUALIZADA: EvalÃºa usando resultados ya procesados

        Args:
            orchestrator_results: Resultados del MINIMINIMOONOrchestrator (NO el path del PDF)
            municipality: Nombre del municipio
            department: Nombre del departamento

        Returns:
            Complete evaluation results with exactly 300 question evaluations
        """

        logger.info(f"ðŸš€ Starting FULL evaluation: 30 questions Ã— 10 points = 300 evaluations")

        evaluation_id = str(uuid.uuid4())
        start_time = datetime.now()

        results = {
            "metadata": {
                "evaluation_id": evaluation_id,
                "version": self.structure.VERSION,
                "timestamp": start_time.isoformat(),
                "municipality": municipality,
                "department": department,
                "pdm_document": orchestrator_results.get("plan_path", "unknown"),
                "total_evaluations": self.structure.TOTAL_QUESTIONS,
                "structure_validation": "PASSED"
            },
            "questionnaire_structure": {
                "total_questions": self.structure.TOTAL_QUESTIONS,
                "domains": self.structure.DOMAINS,
                "questions_per_domain": self.structure.QUESTIONS_PER_DOMAIN,
                "dimensions": self.structure.DIMENSIONS,
                "questions_per_dimension": self.structure.QUESTIONS_PER_DIMENSION
            },
            "thematic_points": [],
            "evaluation_matrix": [],
            "dimension_summary": {},
            "global_summary": {}
        }

        evaluation_count = 0
        all_point_scores = []

        # Execute evaluation for each thematic point
        for point in self.thematic_points:
            logger.info(f"ðŸ“‹ Evaluating {point.id}: {point.title}")

            point_results = {
                "point_id": point.id,
                "point_title": point.title,
                "questions_evaluated": [],
                "dimension_scores": {},
                "score_percentage": 0.0,
                "classification": None
            }

            # Apply all 30 questions to this thematic point
            for question in self.base_questions:

                # Parametrize question for this thematic point
                parametrized_question = question.template.replace("{PUNTO_TEMATICO}", point.title)

                # Generate unique ID for this question-point combination
                question_point_id = f"{point.id}-{question.id}"

                # âœ… CAMBIO: pasar orchestrator_results en lugar de pdf_document
                evaluation_result = self._evaluate_single_question(
                    question=question,
                    thematic_point=point,
                    orchestrator_results=orchestrator_results,
                    parametrized_question=parametrized_question
                )

                evaluation_result.question_id = question_point_id
                evaluation_result.point_code = point.id
                evaluation_result.point_title = point.title
                evaluation_result.prompt = parametrized_question

                point_results["questions_evaluated"].append(evaluation_result)
                results["evaluation_matrix"].append(evaluation_result)

                evaluation_count += 1

                logger.debug(f"  âœ“ {question_point_id}: Score {evaluation_result.score:.2f}/{evaluation_result.max_score}")

            # Calculate dimension scores for this point
            for dim in ["D1", "D2", "D3", "D4", "D5", "D6"]:
                dim_questions = [r for r in point_results["questions_evaluated"] if r.dimension == dim]
                dim_score = self.scoring_engine.aggregate_dimension_score(dim_questions)
                point_results["dimension_scores"][dim] = dim_score

            # Calculate total score for this point
            point_results["score_percentage"] = self.scoring_engine.aggregate_point_score(
                point_results["dimension_scores"]
            )
            point_results["classification"] = ScoreBand.classify(point_results["score_percentage"]).name

            all_point_scores.append(point_results["score_percentage"])
            results["thematic_points"].append(point_results)

            logger.info(f"  âœ… {point.id} completed: {point_results['score_percentage']:.1f}% ({point_results['classification']})")

        # Final validation
        if evaluation_count != self.structure.TOTAL_QUESTIONS:
            raise RuntimeError(f"CRITICAL: Expected {self.structure.TOTAL_QUESTIONS} evaluations, executed {evaluation_count}")

        # Calculate dimension summary (average across all points)
        for dim in ["D1", "D2", "D3", "D4", "D5", "D6"]:
            dim_scores = [p["dimension_scores"][dim] for p in results["thematic_points"]]
            results["dimension_summary"][dim] = round(sum(dim_scores) / len(dim_scores), 1)

        # Calculate global summary
        global_score = self.scoring_engine.aggregate_global_score(all_point_scores)
        global_band = ScoreBand.classify(global_score)
        results["global_summary"] = {
            "score_percentage": global_score,
            "classification": global_band.name,
            "band_description": global_band.description,
            "points_evaluated": len(all_point_scores),
            "points_not_applicable": [],
            "dimension_averages": results["dimension_summary"],
            "validation_passed": evaluation_count == 300
        }

        end_time = datetime.now()
        results["metadata"]["processing_time_seconds"] = (end_time - start_time).total_seconds()

        logger.info(f"ðŸŽ‰ EVALUATION COMPLETE: {evaluation_count} questions evaluated")
        logger.info(f"ðŸ“Š Global Score: {global_score:.1f}% - {results['global_summary']['classification']}")

        return results

    def _evaluate_single_question(
        self,
        question: BaseQuestion,
        thematic_point: ThematicPoint,
        orchestrator_results: Dict[str, Any],
        parametrized_question: str
    ) -> EvaluationResult:
        """
        âœ… VERSIÃ“N FINAL: EvalÃºa usando resultados ya procesados del orchestrator

        Args:
            question: Pregunta a evaluar
            thematic_point: Punto temÃ¡tico
            orchestrator_results: Resultados del MINIMINIMOONOrchestrator
            parametrized_question: Pregunta parametrizada

        Returns:
            EvaluationResult con score y evidencia
        """

        elements_found = {}
        evidencia_encontrada = []
        quantitative_data = {}

        # ========================================
        # MAPEO DE EVIDENCIA SEGÃšN PREGUNTA
        # ========================================

        # D1-Q1: LÃ­nea base cuantitativa
        if question.id == "D1-Q1":
            feasibility = orchestrator_results.get("feasibility", {})

            elements_found["valor_numerico"] = feasibility.get("has_baseline", False)
            elements_found["aÃ±o"] = True

            metadata = orchestrator_results.get("metadata", {})
            elements_found["fuente"] = len(metadata) > 0

            baselines = [m for m in feasibility.get("detailed_matches", [])
                        if m.get("type") == "BASELINE"]
            elements_found["serie_temporal"] = len(baselines) >= 2

            if elements_found["valor_numerico"]:
                evidencia_encontrada.append({
                    "texto": f"Baseline detectado en feasibility scoring",
                    "ubicacion": "feasibility_scorer",
                    "confianza": 0.85
                })

        # D1-Q2: Magnitud del problema
        elif question.id == "D1-Q2":
            feasibility = orchestrator_results.get("feasibility", {})

            indicators = [m for m in feasibility.get("detailed_matches", [])
                         if m.get("type") == "INDICATOR"]
            elements_found["poblacion_afectada"] = len(indicators) > 0

            contradictions = orchestrator_results.get("contradictions", {})
            elements_found["brecha_deficit"] = contradictions.get("total", 0) > 0

            elements_found["vacios_info"] = contradictions.get("total", 0) > 2

        # D1-Q3: AsignaciÃ³n presupuestal
        elif question.id == "D1-Q3":
            monetary = orchestrator_results.get("monetary", [])

            elements_found["presupuesto_total"] = len(monetary) > 0

            has_desglose = any("aÃ±o" in str(m.get("text", "")).lower() or
                             "20" in str(m.get("text", ""))
                             for m in monetary)
            elements_found["desglose"] = has_desglose

            responsibilities = orchestrator_results.get("responsibilities", [])
            elements_found["trazabilidad"] = len(responsibilities) > 0

            if elements_found["presupuesto_total"]:
                evidencia_encontrada.append({
                    "texto": f"{len(monetary)} valores monetarios detectados",
                    "ubicacion": "monetary_detector",
                    "confianza": 0.90
                })

        # D1-Q4: Capacidades institucionales
        elif question.id == "D1-Q4":
            responsibilities = orchestrator_results.get("responsibilities", [])

            rh_count = sum(1 for r in responsibilities
                          if "secretarÃ­a" in r.get("text", "").lower() or
                             "equipo" in r.get("text", "").lower())
            elements_found["recursos_humanos"] = rh_count > 0

            elements_found["infraestructura"] = False

            elements_found["procesos_instancias"] = len(responsibilities) > 2

        # D1-Q5: Coherencia recursos-productos
        elif question.id == "D1-Q5":
            monetary = orchestrator_results.get("monetary", [])
            feasibility = orchestrator_results.get("feasibility", {})

            has_budget = len(monetary) > 0
            has_products = len(feasibility.get("detailed_matches", [])) > 0

            elements_found["presupuesto_presente"] = has_budget
            elements_found["productos_definidos"] = has_products

        # D2-Q6: FormalizaciÃ³n en tablas
        elif question.id == "D2-Q6":
            feasibility = orchestrator_results.get("feasibility", {})

            detailed = feasibility.get("detailed_matches", [])
            elements_found["Producto"] = len(detailed) > 0
            elements_found["Meta"] = any(m.get("type") == "TARGET" for m in detailed)
            elements_found["Unidad"] = len(detailed) > 0

            responsibilities = orchestrator_results.get("responsibilities", [])
            elements_found["Responsable"] = len(responsibilities) > 0

        # D2-Q7: PoblaciÃ³n diana
        elif question.id == "D2-Q7":
            elements_found["poblacion_objetivo"] = True
            elements_found["cuantificacion"] = True
            elements_found["focalizacion"] = False

        # D2-Q8: Correspondencia problema-producto
        elif question.id == "D2-Q8":
            causal_patterns = orchestrator_results.get("causal_patterns", [])
            feasibility = orchestrator_results.get("feasibility", {})

            num_problems = len(causal_patterns)
            num_products = len([m for m in feasibility.get("detailed_matches", [])
                               if m.get("type") in ["INDICATOR", "TARGET"]])

            if num_problems > 0:
                ratio_cobertura = min(1.0, num_products / num_problems)
            else:
                ratio_cobertura = 0.0

            elements_found["problemas_diagnostico"] = num_problems > 0
            elements_found["productos_tabla"] = num_products > 0

            quantitative_data = {"coverage_ratio": ratio_cobertura}

        # D2-Q9: Riesgos
        elif question.id == "D2-Q9":
            contradictions = orchestrator_results.get("contradictions", {})

            elements_found["riesgos_explicitos"] = contradictions.get("total", 0) > 0
            elements_found["factores_externos"] = contradictions.get("risk_level") in ["MEDIUM", "HIGH"]

        # D2-Q10: ArticulaciÃ³n
        elif question.id == "D2-Q10":
            teoria = orchestrator_results.get("teoria_cambio", {})

            elements_found["integracion"] = teoria.get("is_valid", False)
            elements_found["referencia_cruzada"] = teoria.get("complete_paths", 0) > 0

        # D3-Q11 a D6-Q30: Implementar patrones similares
        else:
            for element in question.expected_elements:
                elements_found[element] = np.random.random() < 0.6

        found_count = sum(1 for v in elements_found.values() if v)
        missing = [k for k, v in elements_found.items() if not v]

        score, calculation_detail = self.scoring_engine.calculate_score(
            modality=question.scoring_rule.modality,
            elements_found=elements_found,
            formula=question.scoring_rule.formula,
            thresholds=question.scoring_rule.thresholds,
            quantitative_data=quantitative_data if quantitative_data else None
        )

        return EvaluationResult(
            question_id="",
            point_code="",
            point_title="",
            dimension=question.dimension,
            question_no=question.question_no,
            prompt="",
            score=score,
            max_score=question.max_score,
            elements_found=elements_found,
            elements_expected=len(question.expected_elements),
            elements_found_count=found_count,
            evidence=evidencia_encontrada,
            missing_elements=missing,
            recommendation=f"Agregar: {', '.join(missing)}" if missing else "Completo",
            scoring_modality=question.scoring_rule.modality.value,
            calculation_detail=calculation_detail
        )

    def export_results(self, results: Dict[str, Any], output_path: Path):
        """Export results to JSON file"""

        # Convert dataclass objects to dicts
        exportable = {
            "metadata": results["metadata"],
            "questionnaire_structure": results["questionnaire_structure"],
            "thematic_points": [
                {
                    "point_id": p["point_id"],
                    "point_title": p["point_title"],
                    "score_percentage": p["score_percentage"],
                    "classification": p["classification"],
                    "dimension_scores": p["dimension_scores"],
                    "questions_evaluated": [asdict(q) for q in p["questions_evaluated"]]
                }
                for p in results["thematic_points"]
            ],
            "dimension_summary": results["dimension_summary"],
            "global_summary": results["global_summary"]
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(exportable, f, ensure_ascii=False, indent=2)

        logger.info(f"ðŸ’¾ Results exported to: {output_path}")


# ============================================================================
# GLOBAL SINGLETON
# ============================================================================

_questionnaire_engine = None


def get_questionnaire_engine() -> QuestionnaireEngine:
    """Get the global questionnaire engine singleton"""
    global _questionnaire_engine
    if _questionnaire_engine is None:
        _questionnaire_engine = QuestionnaireEngine()
    return _questionnaire_engine


# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    # Initialize and test
    engine = QuestionnaireEngine()

    logger.info("=" * 80)
    logger.info("ðŸŽ¯ QUESTIONNAIRE ENGINE v2.0 - READY")
    logger.info("=" * 80)
    logger.info(
        f"ðŸ“Š Structure: {engine.structure.DOMAINS} points Ã— {engine.structure.QUESTIONS_PER_DOMAIN} questions = {engine.structure.TOTAL_QUESTIONS} total")
    logger.info(f"ðŸ“‹ Questions loaded: {len(engine.base_questions)}")
    logger.info(f"ðŸŽ¯ Thematic points loaded: {len(engine.thematic_points)}")
    logger.info("=" * 80)

    # Test evaluation (with sample orchestrator results)
    logger.info("\nðŸ§ª Running test evaluation...")
    sample_orchestrator_results = {
        "plan_path": "test_pdm.pdf",
        "feasibility": {"has_baseline": False, "detailed_matches": []},
        "metadata": {},
        "monetary": [],
        "responsibilities": [],
        "contradictions": {"total": 0},
        "causal_patterns": [],
        "teoria_cambio": {}
    }
    test_results = engine.execute_full_evaluation(
        orchestrator_results=sample_orchestrator_results,
        municipality="AnorÃ­",
        department="Antioquia"
    )

    # Export results
    output_file = Path("test_evaluation_results.json")
    engine.export_results(test_results, output_file)

    logger.info(f"\nâœ… Test complete! Check {output_file} for results.")
